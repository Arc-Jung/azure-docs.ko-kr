---
title: Data Factory 서비스에 대한 모든 항목 | Microsoft Docs
description: http://azure.microsoft.com/documentation/articles/에 있는 Data Factory라는 Azure 서비스에 대한 모든 항목, 제목 및 설명에 대한 표입니다.
services: data-factory
documentationcenter: ''
author: spelluru
manager: jhubbard
editor: MightyPen

ms.service: data-factory
ms.workload: data-factory
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 10/05/2016
ms.author: spelluru

---
# <a name="all-topics-for-azure-data-factory-service"></a>Azure Data Factory 서비스에 대한 모든 항목
이 항목에서는 **Data Factory** 서비스에 직접 적용되는 모든 항목을 나열합니다. 이 웹 페이지에서 **Ctrl+F**를 사용해 키워드를 검색하여 현재 관심 있는 항목을 찾을 수 있습니다.

## <a name="new"></a>새로 만들기
| &nbsp; | 제목 | 설명 |
| ---:|:--- |:--- |
| 1 |[Azure 데이터 팩터리를 사용하여 Amazon Redshift에서 데이터 이동](data-factory-amazon-redshift-connector.md) |Azure 데이터 팩터리를 사용하여 Amazon Redshift에서 데이터를 이동하는 방법에 대해 알아봅니다. |
| 2 |[Azure 데이터 팩터리를 사용하여 Amazon 단순 저장소 서비스에서 데이터 이동](data-factory-amazon-simple-storage-service-connector.md) |Azure 데이터 팩터리를 사용하여 Amazon 단순 저장소 서비스 (S3)에서 데이터를 이동하는 방법에 대해 알아봅니다. |
| 3 |[Azure Data Factory - 복사 마법사](data-factory-azure-copy-wizard.md) |Data Factory Azure 복사 마법사를 사용하여 지원되는 데이터 소스의 데이터를 싱크로 복사하는 방법에 대해 알아보세요. |
| 4 |[자습서: 데이터 팩터리 REST API를 사용하여 첫 번째 Azure Data Factory 빌드](data-factory-build-your-first-pipeline-using-rest-api.md) |이 자습서에서는 데이터 팩터리 REST API를 사용하여 샘플 Azure Data Factory 파이프라인을 만듭니다. |
| 5 |[자습서: .NET API를 사용하여 복사 작업이 있는 파이프라인 만들기](data-factory-copy-activity-tutorial-using-dotnet-api.md) |이 자습서에서는 .NET API를 사용하여 복사 작업이 있는 Azure Data Factory 파이프라인을 만듭니다. |
| 6 |[자습서: REST API를 사용하여 복사 작업이 있는 파이프라인 만들기](data-factory-copy-activity-tutorial-using-rest-api.md) |이 자습서에서는 REST API를 사용하여 복사 작업이 있는 Azure Data Factory 파이프라인을 만듭니다. |
| 7 |[데이터 팩터리 복사 마법사](data-factory-copy-wizard.md) |Data Factory 복사 마법사를 사용하여 지원되는 데이터 소스의 데이터를 싱크로 복사하는 방법에 대해 알아보세요. |
| 8 |[데이터 관리 게이트웨이](data-factory-data-management-gateway.md) |데이터 게이트웨이를 설정하여 온-프레미스와 클라우드 간에 데이터를 이동합니다. Azure Data Factory의 데이터 관리 게이트웨이를 사용하여 데이터를 이동합니다. |
| 9 |[Azure Data Factory를 사용하여 온-프레미스 Cassandra 데이터베이스에서 데이터 이동](data-factory-onprem-cassandra-connector.md) |Azure Data Factory를 사용하여 온-프레미스 Cassandra 데이터베이스에서 데이터를 이동하는 방법을 알아봅니다. |
| 10 |[Azure Data Factory를 사용하여 MongoDB에서 데이터 이동](data-factory-on-premises-mongodb-connector.md) |Azure Data Factory를 사용하여 MongoDB 데이터베이스에서 데이터를 이동하는 방법에 대해 알아봅니다. |
| 11 |[Azure Data Factory를 사용하여 Salesforce에서 데이터 이동](data-factory-salesforce-connector.md) |Azure Data Factory를 사용하여 Salesforce에서 데이터를 이동하는 방법에 대해 알아봅니다. |

## <a name="updated-articles,-data-factory"></a>업데이트된 문서, 데이터 팩터리
이 섹션에서는 최근에 대규모로 수행되거나 중요하게 업데이트된 문서가 목록으로 표시됩니다. 업데이트된 각 문서에 대해 추가된 markdown 텍스트의 대략적인 코드 조각이 표시됩니다. 날짜 범위 **2016-08-22**부터 **2016-10-05**까지 문서가 업데이트되었습니다.

| &nbsp; | 문서 | 업데이트된 텍스트, 코드 조각 | 업데이트 시기 |
| ---:|:--- |:--- |:--- |
| 12 |[Azure 데이터 팩터리 - .NET API 변경 로그](data-factory-api-change-log.md) |이 문서에서는 특정 버전의 Azure Data Factory SDK 변경 내용에 대해 설명합니다. 여기(https://www.nuget.org/packages/Microsoft.Azure.Management.DataFactories)에서 Azure Data Factory용 최신 NuGet 패키지를 찾을 수 있습니다. ** 버전 4.11.0** 기능 추가: / 다음에 링크된 서비스 유형이 추가 되었습니다. - OnPremisesMongoDbLinkedService (https://msdn.microsoft.com/library/mt765129.aspx) - AmazonRedshiftLinkedService (https://msdn.microsoft.com/library/mt765121.aspx) - AwsAccessKeyLinkedService (https://msdn.microsoft.com/library/mt765144.aspx) / 다음과 같은 데이터 집합이 추가 되었습니다. - MongoDbCollectionDataset (https://msdn.microsoft.com/library/mt765145.aspx) - AmazonS3Dataset (https://msdn.microsoft.com/library/mt765112.aspx) / 다음과 같은 원본 복사 형식이 추가되었습니다. - MongoDbSource (https://msdn.microsoft.com/en-US/library/mt765123.aspx) ** 버전 4.10.0** / 다음 선택적 속성이 TextFormat에 추가되었습니다. - Ski |2016-09-22 |
| 13 |[Azure Data Factory를 사용하여 Azure Blob 간 데이터 이동](data-factory-azure-blob-connector.md) |/  copyBehavior  /  원본이 BlobSource 또는 FileSystem인 경우 복사 동작을 정의합니다.  /  **PreserveHierarchy:** 대상 폴더에서 파일 계층 구조를 유지합니다. 원본 폴더에 대한 원본 파일의 상대 경로가 대상 폴더에 대한 대상 파일의 상대 경로와 동일합니다.br/..br/.**FlattenHierarchy:** 원본 폴더의 모든 파일이 대상 폴더의 첫 번째 수준에 있습니다. 대상 파일은 자동 생성된 이름을 갖습니다. .br/..br/.**MergeFiles: (기본값)** 원본 폴더의 모든 파일을 하나의 파일로 병합합니다. 파일/Blob 이름이 지정된 경우 지정된 이름이 병합된 파일 이름이 됩니다. 그렇지 않으면 자동 생성된 파일 이름이 병합된 파일 이름이 됩니다.  /  No  /  **BlobSource**에서도 이전 버전과의 호환성을 위해 이러한 두 속성을 지원합니다. / **treatEmptyAsNull**: Null 또는 빈 문자열을 null 값으로 처리할지 여부를 지정합니다. / **skipHeaderLineCount** - 건너뛰어야 하는 줄 수를 지정합니다. 입력 데이터 집합이 TextFormat을 사용하는 경우에만 해당합니다. 마찬가지로, **BlobSink**는 th를 지원합니다. |2016-09-28 |
| 14 |[Azure 기계 학습 작업을 사용하여 예측 파이프라인 만들기](data-factory-azure-ml-batch-execution-activity.md) |**웹 서비스에는 다중 입력이 필요합니다** 웹 서비스가 다중 입력을 받을 경우 **webServiceInput**을 사용하는 대신에 **webServiceInputs** 속성을 사용합니다. **webServiceInputs**에서 참조하는 데이터 집합은 또한 **입력** 작업에 포함되어야 합니다. Azure ML 실험에서 웹 서비스 입력 및 출력 포트와 전역 매개 변수에는 사용자 지정할 수 있는 기본 이름("input1", "input2")이 붙어있습니다. WebServiceInputs, webServiceOutputs 및 globalParameters 설정에 대해 사용하는 이름은 실험에서의 이름과 정확히 일치해야 합니다. 예상된 매핑을 확인하기 위해 일괄 처리 실행 도움말 페이지에서 Azure ML 끝점에 대한 샘플 요청 페이로드를 볼 수 있습니다.    {       "name": "PredictivePipeline",       "properties": {             "description": "use AzureML model",             "activities":  {                "name": "MLActivity",               "type": "AzureMLBatchExecution",                "description": "prediction analysis on batch input",                "inputs":  {                    "name": "inputDataset1"                 }, {                    "name": "inputDatase |2016-09-13 |
| 15 |[복사 작업 성능 및 조정 가이드](data-factory-copy-activity-performance.md) |1. **기초 설정**. 개발 단계 중 대표적인 샘플 데이터에 대한 복사 작업을 사용하여 파이프라인을 테스트합니다. 데이터 팩터리 조각화 모델(data-factory-scheduling-and-execution.md time-series-datasets-and-data-slices)을 사용하여 작업할 데이터의 양을 제한할 수 있습니다.  **모니터링 및 관리 앱**을 사용하여 실행 시간 및 성능 특성을 수집합니다. Data Factory 홈페이지에서 **모니터 및 관리**를 선택합니다. 트리 뷰에서 **출력 데이터 집합**을 선택합니다. **작업 기간** 목록에서 복사 작업 실행을 선택합니다. **작업 기간**에는 복사 작업 기간 및 복사되는 데이터 크기가 나열됩니다. 처리량은 **작업 창 탐색기**에 나열됩니다. 앱에 대해 자세히 알아보려면 모니터링 및 관리 앱(data-factory-monitor-manage-app.md)을 사용하여 Azure Data Factory 파이프라인 모니터링 및 관리를 참조하세요.  ! 작업 실행 세부 정보(./media/data-factory-copy-activity-performance/mmapp-activity-run-details.pn |2016-09-27 |
| 16 |[자습서: Visual Studio를 사용하여 복사 작업이 있는 파이프라인 만들기](data-factory-copy-activity-tutorial-using-visual-studio.md) |다음 사항에 유의하세요. - 데이터 집합 **형식**을 **AzureBlob**으로 설정합니다.     - **linkedServiceName**을 **AzureStorageLinkedService**로 설정합니다. 이 연결된 서비스는 2단계에서 만들었습니다.     - **folderPath**를 **adftutorial** 컨테이너로 설정합니다. 또한 **fileName** 속성을 사용하여 폴더 내의 Blob 이름을 지정할 수도 있습니다. Blob 이름을 지정하지 않으므로 컨테이너에 있는 모든 Blob의 데이터가 입력 데이터로 간주됩니다.    - 서식 **형식**을 **TextFormat**으로 설정합니다. - 텍스트 파일에는 ΓÇô **FirstName**과 **LastName** ΓÇô의 두 필드가 쉼표(**columnDelimiter**)로 구분되어 있습니다. - **가용성**은 **매시간**으로 설정되어 있습니다(**빈도**는 **시간**으로 설정되고 **간격**은 **1**로 설정됨). 따라서 데이터 팩터리는 지정한 Blob 컨테이너(**adftutorial**)의 루트 폴더에서 한 시간마다 입력 데이터를 찾습니다.  **입력** 데이터 집합의 **fileName**을 지정하지 않는 경우 입력 폴더(**folderPath**)의 모든 파일/Blob은 입력으로 간주됩니다. |2016-09-29 |
| 17 |[데이터 팩터리 .NET SDK를 사용하여 Azure Data Factory 만들기, 모니터링 및 관리](data-factory-create-data-factories-programmatically.md) |응용 프로그램 ID 및 암호(클라이언트 암호)을 기록해 두고 연습에 사용합니다. **Azure 구독 및 테넌트 ID 얻기** 컴퓨터에 Azure PowerShell의 최신 버전이 설치되어 있지 않다면, 설치하기 위해 Azure PowerShell(../powershell-install-configure.md)을 설치 및 구성하는 방법 문서의 지침을 따릅니다. 1. Azure PowerShell을 시작하고 다음 명령을 실행합니다. 2. 다음 명령을 실행하고 Azure 포털에 로그인하는 데 사용할 사용자 이름 및 암호를 입력합니다.         Login-AzureRmAccount    이 계정에 연결된 Azure 구독이 하나 뿐인 경우 다음 두 단계를 수행할 필요가 없습니다. 3. 다음 명령을 실행하여 이 계정의 모든 구독을 확인합니다.       Get-AzureRmSubscription 4. 다음 명령을 실행하여 사용하려는 구독을 선택합니다. **NameOfAzureSubscription**을 Azure 구독의 이름으로 바꿉니다.       Get-AzureRmSubscription -SubscriptionName NameOfAzureSubscription  /  Set-AzureRmCo |2016-09-14 |
| 18 |[Azure 데이터 팩터리의 파이프라인 및 활동](data-factory-create-pipelines.md) |,       "start": "2016-07-12T00:00:00Z",    "end": "2016-07-13T00:00:00Z"       }     } 다음 사항에 유의하세요. / 작업 섹션에는 **형식**이 **복사**로 설정된 작업만 있습니다. / 작업에 대한 입력을 **InputDataset**으로 설정하고 작업에 대한 출력을 **OutputDataset**으로 설정합니다. / **typeProperties** 섹션에서 **BlobSource**를 원본 유형으로 지정하고 **SqlSink**를 싱크 유형으로 지정합니다. 이 파이프라인을 만드는 전체 연습은 자습서: Blob Storage의 데이터를 SQL Database(data-factory-copy-data-from-azure-blob-storage-to-sql-database.md)에 복사를 참조하세요. **샘플 변환 파이프라인** 다음 샘플 파이프라인에는 **활동** 섹션에 **HDInsightHive** 유형의 하나의 활동이 있습니다. 이 샘플에서 HDInsight Hive 활동(data-factory-hive-activity.md)은 Azure HDInsight Hadoop 클러스터에서 Hive 스크립트 파일을 실행하여 Azure Blob 저장소에서 데이터를 변환합니다.  {     "name": "TransformPipeline",    "p |2016-09-27 |
| 19 |[Azure Data Factory의 데이터 변환](data-factory-data-transformation-activities.md) |Data Factory는 개별적 또는 다른 작업과 연계하여 파이프라인(data-factory-create-pipelines.md)에 추가할 수 있는 다음 데이터 변환 작업을 지원합니다. 에서도 SQL Server VM을 만들고 관리할 수 있습니다.  AZURE.NOTE 단계별 지침이 포함된 연습은 Hive 변환으로 파이프라인 만들기(data-factory-build-your-first-pipeline.md) 문서를 참조하세요. **HDInsight Hive 작업** Data Factory 파이프라인에서 HDInsight Hive 작업은 사용자 고유 또는 주문형 Windows/Linux 기반 HDInsight 클러스터의 Hive 쿼리를 실행합니다. 이 작업에 대한 자세한 내용은 Hive 작업(data-factory-hive-activity.md) 문서를 참조하세요. **HDInsight Pig 작업** Data Factory 파이프라인에서 HDInsight Pig 작업은 사용자 고유 또는 주문형 Windows/Linux 기반 HDInsight 클러스터의 Pig 쿼리를 실행합니다. 이 작업에 대한 자세한 내용은 Pig 작업(data-factory-pig-activity.md) 문서를 참조하세요. **HDInsight MapReduce 작업** Data Factory 파이프라인의 HDInsight MapReduce 작업은 사용자 고유 또는 주문형 Windows/Linux 기반 HDInsight 클러스터에서 MapReduce 프로그램을 실행합니다. |2016-09-26 |
| 20 |[Data Factory 예약 및 실행](data-factory-scheduling-and-execution.md) |CopyActivity2는 CopyActivity1을 성공적으로 실행하고 Dataset2를 사용할 수 있는 경우에만 실행됩니다. 샘플 파이프라인 JSON은 다음과 같습니다.  {       "name": "ChainActivities",    "properties": {           "description": "Run activities in sequence",      "activities":       {       "type": "Copy",     "typeProperties": {     "source": {     "type": "BlobSource"    },      "sink": {       "type": "BlobSink",     "copyBehavior": "PreserveHierarchy",    "writeBatchSize": 0,    "writeBatchTimeout": "00:00:00"     }       },      "inputs":       {       "name": "Dataset1"      }       ,       "outputs":      {       "name": "Dataset2"      }       ,       "policy": {     "timeout": "01:00:00"       },      "scheduler": {      "frequency": "Hour",    "interval": 1       },      "name": "CopyFromBlob1ToBlob2",     "description": "Copy data from a blob to another"       },      {       "type": "Copy",     "typeProperties": {     "source": {     "type": "BlobSource"    },      "sink": {       "type": "BlobSink",     "writeBatchSize": 0,    "writeBatchTimeout": "00:00:00"     }       },      "in |2016-09-28 |

## <a name="tutorials"></a>자습서
| &nbsp; | 제목 | 설명 |
| ---:|:--- |:--- |
| 21 |[자습서: Hadoop 클러스터를 사용하여 데이터를 처리하는 첫 번째 파이프라인 빌드](data-factory-build-your-first-pipeline.md) |이 Azure 데이터 팩터리 자습서에서는 Hadoop 클러스터에서 Hive 스크립트를 사용하여 데이터를 처리하는 데이터 팩터리를 만들고 예약하는 방법을 보여 줍니다. |
| 22 |[자습서: Azure Resource Manager 템플릿을 사용하여 첫 번째 Azure Data Factory 빌드](data-factory-build-your-first-pipeline-using-arm.md) |이 자습서에서는 Azure Resource Manager 템플릿을 사용하여 샘플 Azure Data Factory 파이프라인을 만듭니다. |
| 23 |[자습서: Azure Portal을 사용하여 첫 번째 Azure Data Factory 빌드](data-factory-build-your-first-pipeline-using-editor.md) |이 자습서에서는 Azure 포털의 데이터 팩터리 편집기를 사용하여 샘플 Azure Data Factory 파이프라인을 만듭니다. |
| 24 |[자습서: Azure PowerShell을 사용하여 첫 번째 Azure Data Factory 빌드](data-factory-build-your-first-pipeline-using-powershell.md) |이 자습서에서는 Azure PowerShell을 사용하여 샘플 Azure Data Factory 파이프라인을 만듭니다. |
| 25 |[자습서: Microsoft Visual Studio를 사용하여 첫 번째 Azure Data Factory 빌드](data-factory-build-your-first-pipeline-using-vs.md) |이 자습서에서는 Visual Studio를 사용하여 샘플 Azure Data Factory 파이프라인을 만듭니다. |
| 26 |[자습서: Azure Portal을 사용하여 복사 작업이 있는 파이프라인 만들기](data-factory-copy-activity-tutorial-using-azure-portal.md) |이 자습서에서는 Azure 포털의 데이터 팩터리 편집기를 사용하여 복사 작업이 있는 Azure Data Factory 파이프라인을 만듭니다. |
| 27 |[자습서: Azure PowerShell을 사용하여 복사 작업이 있는 파이프라인 만들기](data-factory-copy-activity-tutorial-using-powershell.md) |이 자습서에서는 Azure PowerShell을 사용하여 복사 작업이 있는 Azure Data Factory 파이프라인을 만듭니다. |
| 28 |[자습서: Visual Studio를 사용하여 복사 작업이 있는 파이프라인 만들기](data-factory-copy-activity-tutorial-using-visual-studio.md) |이 자습서에서는 Visual Studio를 사용하여 복사 작업이 있는 Azure Data Factory 파이프라인을 만듭니다. |
| 29 |[데이터 팩터리를 사용하여 Blob 저장소에서 SQL 데이터베이스로 데이터 복사](data-factory-copy-data-from-azure-blob-storage-to-sql-database.md) |이 자습서에서는 Azure Data Factory 파이프라인에서 복사 작업을 사용하여 Blob 저장소에서 SQL 데이터베이스로 데이터를 복사하는 방법을 보여 줍니다. |
| 30 |[자습서: 데이터 팩터리 복사 마법사를 사용하여 복사 작업이 있는 파이프라인 만들기](data-factory-copy-data-wizard-tutorial.md) |이 자습서에서는 데이터 팩터리가 지원하는 복사 마법사를 사용하여 복사 작업이 있는 Azure Data Factory 파이프라인을 만듭니다. |

## <a name="data-movement"></a>데이터 이동
| &nbsp; | 제목 | 설명 |
| ---:|:--- |:--- |
| 31 |[Azure Data Factory를 사용하여 Azure Blob 간 데이터 이동](data-factory-azure-blob-connector.md) |Azure Data Factory에서 Blob 데이터를 복사하는 방법을 알아봅니다. 샘플 사용: Azure Blob 저장소 및 Azure SQL 데이터베이스 간에 데이터를 복사하는 방법입니다. |
| 32 |[Azure 데이터 팩터리를 사용하여 Azure 데이터 레이크 저장소 간 데이터 이동](data-factory-azure-datalake-connector.md) |Azure 데이터 팩터리를 사용하여 Azure 데이터 레이크 저장소 간 데이터를 이동하는 방법에 대해 알아봅니다. |
| 33 |[Azure 데이터 팩터리를 사용하여 DocumentDB 간 데이터 이동](data-factory-azure-documentdb-connector.md) |Azure 데이터 팩터리를 사용하여 Azure DocumentDB 컬렉션 간 데이터를 이동하는 방법에 대해 알아봅니다. |
| 34 |[Azure 데이터 팩터리를 사용하여 Azure SQL 데이터베이스 간 데이터 이동](data-factory-azure-sql-connector.md) |Azure Data Factory를 사용하여 Azure SQL 데이터베이스 간 데이터를 이동하는 방법에 대해 알아봅니다. |
| 35 |[Azure 데이터 팩터리를 사용하여 Azure SQL 데이터 웨어하우스 간 데이터 이동](data-factory-azure-sql-data-warehouse-connector.md) |Azure 데이터 팩터리를 사용하여 Azure SQL 데이터 웨어하우스 간 데이터를 이동하는 방법에 대해 알아봅니다. |
| 36 |[Azure 데이터 팩터리를 사용하여 Azure 테이블 간 데이터 이동](data-factory-azure-table-connector.md) |Azure Data Factory를 사용하여 Azure 테이블 저장소 간 데이터를 이동하는 방법에 대해 알아봅니다. |
| 37 |[복사 작업 성능 및 조정 가이드](data-factory-copy-activity-performance.md) |복사 작업을 사용할 때 Azure Data Factory의 데이터 이동의 성능에 영향을 주는 주요 요소에 대해 알아봅니다. |
| 38 |[복사 작업을 사용하여 데이터 이동](data-factory-data-movement-activities.md) |Data Factory 파이프라인에서 데이터를 이동하는 방법(클라우드 저장소 간/온-프레미스 저장소와 클라우드 저장소 간에 데이터 마이그레이션)에 대해 알아봅니다. 또한 복사 활동을 사용하는 방법을 살펴봅니다. |
| 39 |[데이터 관리 게이트웨이에 대한 릴리스 정보](data-factory-gateway-release-notes.md) |데이터 관리 게이트웨이 릴리스 정보 |
| 40 |[Azure 데이터 팩터리를 사용하여 온-프레미스 HDFS에서 데이터 이동](data-factory-hdfs-connector.md) |Azure 데이터 팩터리를 사용하여 온-프레미스 HDFS에서 데이터를 이동하는 방법에 대해 알아봅니다. |
| 41 |[새 모니터링 및 관리 앱을 사용하여 Azure 데이터 팩터리 파이프라인 모니터링 및 관리](data-factory-monitor-manage-app.md) |모니터링 및 관리 앱을 사용하여 Azure 데이터 팩터리 및 파이프라인을 모니터링하고 관리하는 방법을 알아봅니다. |
| 42 |[온-프레미스 원본과 클라우드 간에 데이터 관리 게이트웨이로 데이터 이동](data-factory-move-data-between-onprem-and-cloud.md) |데이터 게이트웨이를 설정하여 온-프레미스와 클라우드 간에 데이터를 이동합니다. Azure Data Factory의 데이터 관리 게이트웨이를 사용하여 데이터를 이동합니다. |
| 43 |[Azure Data Factory를 사용하여 OData 소스에서 데이터 이동](data-factory-odata-connector.md) |Azure 데이터 팩터리를 사용하여 OData 소스에서 데이터를 이동하는 방법에 대해 알아봅니다. |
| 44 |[Azure 데이터 팩터리를 사용하여 ODBC 데이터 저장소에서 데이터 이동](data-factory-odbc-connector.md) |Azure 데이터 팩터리를 사용하여 ODBC 데이터 저장소에서 데이터를 이동하는 방법에 대해 알아봅니다. |
| 45 |[Azure 데이터 팩터리를 사용하여 DB2에서 데이터 이동](data-factory-onprem-db2-connector.md) |Azure 데이터 팩터리를 사용하여 DB2 데이터베이스에서 데이터를 이동하는 방법에 대해 알아봅니다. |
| 46 |[Azure Data Factory를 통한 온-프레미스 파일 시스템에서의 데이터 이동](data-factory-onprem-file-system-connector.md) |Azure Data Factory를 사용하여 온-프레미스 파일 시스템에서 데이터를 이동하는 방법에 대해 알아봅니다. |
| 47 |[Azure 데이터 팩터리를 사용하여 MySQL에서 데이터 이동](data-factory-onprem-mysql-connector.md) |Azure 데이터 팩터리를 사용하여 MySQL 데이터베이스에서 데이터를 이동하는 방법에 대해 알아봅니다. |
| 48 |[Azure 데이터 팩터리를 사용하여 온-프레미스 Oracle 간 데이터 이동](data-factory-onprem-oracle-connector.md) |Azure 데이터 팩터리를 사용하여 온-프레미스 Oracle 데이터베이스 간 데이터를 이동하는 방법에 대해 알아봅니다. |
| 49 |[Azure 데이터 팩터리를 사용하여 PostgreSQL에서 데이터 이동](data-factory-onprem-postgresql-connector.md) |Azure 데이터 팩터리를 사용하여 PostgreSQL 데이터베이스에서 데이터를 이동하는 방법에 대해 알아봅니다. |
| 50 |[Azure 데이터 팩터리를 사용하여 Sybase에서 데이터 이동](data-factory-onprem-sybase-connector.md) |Azure 데이터 팩터리를 사용하여 Sybase 데이터베이스에서 데이터를 이동하는 방법에 대해 알아봅니다. |
| 51 |[Azure 데이터 팩터리를 사용하여 Teradata에서 데이터 이동](data-factory-onprem-teradata-connector.md) |Teradata 데이터베이스에서 데이터를 이동시킬 수 있는 데이터 팩터리 서비스용 Teradata 커넥터에 대해 알아봅니다. |
| 52 |[Azure 데이터 팩터리를 사용하여 온-프레미스 또는 IaaS(Azure VM) SQL Server 간 데이터 이동](data-factory-sqlserver-connector.md) |Azure 데이터 팩터리를 사용하여 온-프레미스 또는 Azure VM에 있는 SQL Server 데이터베이스 간에 데이터를 이동하는 방법에 대해 알아봅니다. |
| 53 |[Azure Data Factory를 사용하여 웹 테이블 원본에서 데이터 이동](data-factory-web-table-connector.md) |Azure Data Factory를 사용하여 온-프레미스 웹 페이지의 테이블로 데이터를 이동하는 방법에 대해 알아봅니다. |

## <a name="data-transformation"></a>데이터 변환
| &nbsp; | 제목 | 설명 |
| ---:|:--- |:--- |
| 54 |[Azure 기계 학습 작업을 사용하여 예측 파이프라인 만들기](data-factory-azure-ml-batch-execution-activity.md) |Azure Data Factory 및 Azure 기계 학습을 사용하여 예측 파이프라인을 만드는 방법을 설명합니다. |
| 55 |[연결된 서비스 계산](data-factory-compute-linked-services.md) |데이터의 변환/처리를 위해 Azure 데이터 팩터리 파이프라인에서 사용할 수 있는 계산 환경을 알아봅니다. |
| 56 |[데이터 팩터리 및 배치를 사용하여 대규모 데이터 집합 처리](data-factory-data-processing-using-batch.md) |Azure 배치의 병렬 처리 기능을 사용하여 Azure Data Factory 파이프라인에서 대용량 데이터를 처리하는 방법을 설명합니다. |
| 57 |[Azure Data Factory의 데이터 변환](data-factory-data-transformation-activities.md) |Hadoop, Machine Learning 또는 Azure Data Lake Analytics를 사용하여 Azure Data Factory에서 데이터를 변환 또는 처리하는 방법에 대해 알아봅니다. |
| 58 |[Hadoop 스트리밍 작업](data-factory-hadoop-streaming-activity.md) |Azure Data Factory에서 Hadoop 스트리밍 작업을 사용하여 주문형/사용자 고유의 HDInsight 클러스터에서 Hadoop 스트리밍 프로그램을 실행하는 방법에 대해 알아봅니다. |
| 59 |[Hive 작업](data-factory-hive-activity.md) |Azure 데이터 공장에서 Hive 활동을 사용하여 필요 시/사용자 고유의 HDInsight 클러스터에서 Hive 쿼리를 실행하는 방법을 알아봅니다. |
| 60 |[데이터 팩터리에서 MapReduce 프로그램 호출](data-factory-map-reduce.md) |Azure HDInsight 클러스터에서 Azure 데이터 팩터리의 MapReduce 프로그램을 실행하여 데이터를 처리하는 방법을 알아봅니다. |
| 61 |[Pig 작업](data-factory-pig-activity.md) |Azure Data Factory에서 Pig 작업을 사용하여 주문형/사용자 고유의 HDInsight 클러스터에서 Pig 스크립트를 실행하는 방법을 알아봅니다. |
| 62 |[Data Factory에서 Spark 프로그램 호출](data-factory-spark.md) |MapReduce 작업을 사용하여 Azure Data Factory에서 Spark 프로그램을 호출하는 방법에 대해 알아봅니다. |
| 63 |[SQL Server 저장 프로시저 작업](data-factory-stored-proc-activity.md) |SQL Server 저장 프로시저 작업을 사용하여 데이터 팩터리 파이프라인으로 Azure SQL 데이터베이스 또는 Azure SQL 데이터 웨어하우스에서 저장 프로시저를 호출하는 방법을 알아봅니다. |
| 64 |[Azure Data Factory 파이프라인에서 사용자 지정 작업 사용](data-factory-use-custom-activities.md) |사용자 지정 작업을 만들고 Azure Data Factory 파이프라인에서 사용하는 방법에 대해 알아봅니다. |
| 65 |[Azure Data Factory의 Azure 데이터 레이크 분석에서 U-SQL 스크립트 실행](data-factory-usql-activity.md) |Azure Data Lake Analytics 계산 서비스에서 U-SQL 스크립트를 실행하여 데이터를 처리하는 방법에 대해 알아봅니다. |

## <a name="samples"></a>샘플
| &nbsp; | 제목 | 설명 |
| ---:|:--- |:--- |
| 66 |[Azure 데이터 팩터리 - 샘플](data-factory-samples.md) |Azure Data Factory 서비스와 함께 제공 되는 샘플에 대 한 세부 정보를 제공 합니다. |

## <a name="use-cases"></a>사용 사례
| &nbsp; | 제목 | 설명 |
| ---:|:--- |:--- |
| 67 |[고객 사례 연구](data-factory-customer-case-studies.md) |고객들이 Azure 데이터 팩터리를 어떻게 사용하고 있는지 알아봅니다. |
| 68 |[사용 사례 - 고객 프로파일링](data-factory-customer-profiling-usecase.md) |Azure 데이터 팩터리로 데이터 기반 워크플로를(파이프라인) 만들어 게임 고객을 프로파일링하는 방법을 알아봅니다. |
| 69 |[사용 사례 - 제품 추천](data-factory-product-reco-usecase.md) |Azure Data Factory 및 기타 서비스로 구현한 사용 사례에 대해 알아봅니다. |

## <a name="monitor-and-manage"></a>모니터링 및 관리
| &nbsp; | 제목 | 설명 |
| ---:|:--- |:--- |
| 70 |[Azure Data Factory 파이프라인 모니터링 및 관리](data-factory-monitor-manage-pipelines.md) |Azure 포털과 Azure PowerShell을 사용하여 사용자가 만든 Azure Data Factory와 파이프라인을 모니터링하고 관리하는 방법에 대해 알아봅니다. |

## <a name="sdk"></a>SDK)
| &nbsp; | 제목 | 설명 |
| ---:|:--- |:--- |
| 71 |[Azure 데이터 팩터리 - .NET API 변경 로그](data-factory-api-change-log.md) |Azure Data Factory에 대한 특정 .NET API 버전의 주요 변경 내용, 기능 추가 사항, 버그 수정 등을 설명합니다. |
| 72 |[데이터 팩터리 .NET SDK를 사용하여 Azure Data Factory 만들기, 모니터링 및 관리](data-factory-create-data-factories-programmatically.md) |데이터 팩터리 SDK를 사용하여 프로그래밍 방식으로 Azure Data Factory를 만들고, 모니터링하고, 관리하는 방법을 알아봅니다. |
| 73 |[Azure 데이터 팩터리 개발자 참조](data-factory-sdks.md) |Azure 데이터 팩터리를 만들고 모니터링하고 관리하는 다양한 방법을 알아봅니다. |

## <a name="miscellaneous"></a>기타
| &nbsp; | 제목 | 설명 |
| ---:|:--- |:--- |
| 74 |[Azure 데이터 팩터리 - 질문과 대답](data-factory-faq.md) |Azure 데이터 팩터리에 대한 질문과 대답입니다. |
| 75 |[Azure 데이터 팩터리 - 함수 및 시스템 변수](data-factory-functions-variables.md) |Azure 데이터 팩터리 함수 및 시스템 변수 목록을 제공합니다. |
| 76 |[Azure 데이터 팩터리 - 이름 지정 규칙](data-factory-naming-rules.md) |데이터 팩터리 엔터티에 대한 이름 지정 규칙을 설명합니다. |
| 77 |[데이터 팩터리 문제 해결](data-factory-troubleshoot.md) |Azure 데이터 팩터리 사용과 관련된 문제를 해결하는 방법에 대해 알아봅니다. |

<!--HONumber=Oct16_HO2-->


