---
title: Personalizer 작동 방식 - Personalizer
titleSuffix: Azure Cognitive Services
description: Personalizer는 기계 학습을 사용하여 컨텍스트에서 사용할 작업을 검색합니다. 각 학습 루프에는 Rank(순위) 및 Reward(보상) 호출을 통해 보낸 데이터에 대해 단독으로 학습되는 모델이 있습니다. 모든 학습 루프는 서로 완전히 독립적입니다.
author: edjez
manager: nitinme
ms.service: cognitive-services
ms.subservice: personalizer
ms.topic: conceptual
ms.date: 06/07/2019
ms.author: edjez
ms.openlocfilehash: 38480d3cc32d53084b79af627e4f7e6ae7dcc03d
ms.sourcegitcommit: dad277fbcfe0ed532b555298c9d6bc01fcaa94e2
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 07/10/2019
ms.locfileid: "67722366"
---
# <a name="how-personalizer-works"></a>Personalizer 작동 방식

Personalizer는 기계 학습을 사용하여 컨텍스트에서 사용할 작업을 검색합니다. 각 학습 루프에는 **Rank**(순위) 및 **Reward**(보상) 호출을 통해 보낸 데이터에 대해 단독으로 학습되는 모델이 있습니다. 모든 학습 루프는 서로 완전히 독립적입니다. 개인에 맞게 설정하려는 애플리케이션의 각 부분 또는 동작에 대한 학습 루프를 만듭니다.

다음 목록을 사용하여 각 루프에 대해 현재 컨텍스트에 따라 **Rank API를 호출**합니다.

* 가능한 작업 목록: 최상위 작업을 선택할 콘텐츠 항목입니다.
* [콘텍스트 기능 목록](concepts-features.md): 사용자, 콘텐츠 및 컨텍스트와 같은 컨텍스트 관련 데이터입니다.

**Rank** API는 다음 중 하나를 사용하도록 결정합니다.

* _활용_: 과거 데이터에 따라 최상의 작업을 결정하는 현재 모델입니다.
* _검색_: 상위 작업 대신 다른 작업을 선택합니다.

**Reward** API는 다음과 같습니다.

* 각 순위 호출의 기능 및 보상 점수를 기록하여 모델을 학습시키기 위한 데이터를 수집합니다.
* 이 데이터를 사용하여 _학습 정책_에 지정된 설정에 따라 모델을 업데이트합니다.

## <a name="architecture"></a>아키텍처

다음 이미지에서는 Rank 호출 및 Reward 호출의 아키텍처 흐름을 보여 줍니다.

![alt text](./media/how-personalizer-works/personalization-how-it-works.png "맞춤 설정 작동 방식")

1. Personalizer는 내부 AI 모델을 사용하여 작업의 순위를 결정합니다.
1. 이 서비스에서 현재 모델을 활용할지, 아니면 모델에 대한 새 선택 항목을 검색할지를 결정합니다.  
1. 순위 결과가 EventHub로 전송됩니다.
1. Personalizer에서 보상을 받으면 해당 보상을 EventHub에 보냅니다. 
1. 순위와 보상이 상호 관련됩니다.
1. AI 모델이 상관 관계 결과에 따라 업데이트됩니다.
1. 유추 엔진이 새 모델로 업데이트됩니다. 

## <a name="research-behind-personalizer"></a>Personalizer에 대한 연구

Personalizer는 Microsoft Research의 백서, 연구 활동 및 진행 중인 탐구 영역을 포함하여 [보충 학습](concepts-reinforcement-learning.md) 분야의 최첨단 과학 및 연구를 기반으로 합니다.

## <a name="terminology"></a>용어

* **학습 루프**: 맞춤 설정을 활용할 수 있는 애플리케이션의 모든 부분에 대한 학습 루프를 만들 수 있습니다. 개인에 맞게 설정하려는 환경이 둘 이상인 경우 각각에 대한 반복을 만듭니다. 

* **작업**: 작업은 제품 또는 프로모션과 같이 선택할 수 있는 콘텐츠 항목입니다. Personalizer는 Rank API를 통해 사용자에게 표시할 상위 작업(_Reward 작업_이라고 함)을 선택합니다. 각 작업에는 Rank 요청과 함께 제출되는 기능이 있을 수 있습니다.

* **Context**: 더 정확한 순위를 제공하려면 컨텍스트에 대한 정보를 제공합니다. 예를 들어 다음과 같습니다.
    * 사용자
    * 사용하는 디바이스 
    * 현재 시간입니다.
    * 현재 상황에 대한 다른 데이터
    * 사용자 또는 컨텍스트에 대한 기록 데이터

    특정 애플리케이션에 다른 컨텍스트 정보가 있을 수 있습니다. 

* **[기능](concepts-features.md)** : 콘텐츠 항목 또는 사용자 컨텍스트에 대한 정보 단위입니다.

* **보상**: 사용자가 Rank API에서 반환된 작업에 응답한 정도를 0과 1 사이의 점수로 나타낸 측정값입니다. 0~1 값은 선택이 맞춤 설정의 비즈니스 목표를 달성하는 데 도움이 되는 정도에 기반한 비즈니스 논리에 따라 설정됩니다. 

* **검색**: Personalizer 서비스는 최상의 작업을 반환하는 대신 사용자를 위해 다른 작업을 선택하는 시점을 검색합니다. Personalizer 서비스는 드리프트와 정체를 방지하고 검색을 통해 진행 중인 사용자 동작에 적응할 수 있습니다. 

* **실험 기간**: Personalizer 서비스에서 해당 이벤트에 대한 Rank 호출이 발생한 순간부터 보상을 기다리는 시간의 양입니다.

* **비활성 이벤트**: 비활성 이벤트는 Rank를 호출한 이벤트이지만, 클라이언트 애플리케이션 결정으로 인해 사용자가 결과를 볼 수 있는지가 확실하지 않습니다. 비활성 이벤트를 사용하면 맞춤 설정 결과를 만들고 저장한 다음, 나중에 기계 학습 모델에 영향을 주지 않고 삭제하도록 결정할 수 있습니다.

* **모델**: Personalizer 모델은 사용자 동작에 대해 학습된 모든 데이터를 캡처하고, Rank 및 Reward 호출에 보내는 인수와 학습 정책에 따라 결정된 학습 동작을 조합하여 학습 데이터를 가져옵니다. 

## <a name="example-use-cases-for-personalizer"></a>Personalizer 사용 사례

* 의도 확인 및 명확성: 사용자의 의도가 명확하지 않은 경우 각 사용자에게 맞춤형 옵션을 제공하여 사용자 환경을 개선합니다.
* 메뉴 및 옵션에 대한 기본 제안: 개인적이지 않은 메뉴 또는 대안 목록을 제공하는 대신, 첫 번째 단계로 봇이 맞춤형 방식으로 가장 가능성 높은 항목을 제안하게 합니다.
* 봇 특성 및 어조: 어조, 자세한 정도 및 작성 스타일이 달리질 수 있는 봇의 경우 맞춤형 방식으로 이러한 특성을 바꾸는 방법을 고려합니다.
* 알림 및 경고 콘텐츠: 사용자의 참여를 높이려면 경고에 어떤 텍스트를 사용해야 하는지 결정합니다.
* 알림 및 경고 타이밍: 사용자의 참여를 높이려면 사용자에게 알림을 보내야 하는 시기에 대한 맞춤형 학습을 제공합니다.

## <a name="checklist-for-applying-personalizer"></a>Personalizer를 적용하기 위한 검사 목록

Personalizer를 적용할 수 있는 경우는 다음과 같습니다.

* 애플리케이션에 대한 비즈니스 또는 유용성 목표가 있습니다.
* 사용자에게 표시할 항목을 상황에 맞게 결정하여 해당 목표를 향상시킬 수 있는 위치가 애플리케이션에 있습니다.
* 최상의 선택은 전체 사용자 동작과 총 보상 점수에서 학습할 수 있고 학습해야 합니다.
* 기계 학습을 개인 설정에 사용하려면 [사용 책임 지침](ethics-responsible-use.md) 및 팀의 선택 내용을 따릅니다.
* 결정된 내용을 최상의 옵션(제한된 선택 항목의 [작업](concepts-features.md#actions-represent-a-list-of-options)로 표현할 수 있습니다.
* 해당 선택을 비즈니스 논리에서 얼마나 잘 컴퓨팅할 수 있는지 나타내기 위해 사용자 동작을 여러 관점에서 측정하고 -1~1 사이의 숫자로 나타낼 수 있습니다.
* 보상 점수는 교란 요인 또는 외부 요인을 많이 반영하지 않으며, 특히 실험 기간은 보상 점수를 컴퓨팅하면서도 관련성을 유지할 수 있을 만큼 짧습니다.
* 순위에 대한 컨텍스트는 올바른 선택을 하는 데 도움이 될 것으로 보이고 개인 식별 정보가 포함되지 않은 5개 이상의 기능 사전으로 표현할 수 있습니다.
* 각 작업에 대한 정보는 Personalizer에서 올바른 선택을 하는 데 도움이 될 것으로 보이는 5개 이상의 특성 또는 기능 사전으로 제공됩니다.
* 100,000개 이상의 상호 작용 기록을 누적할 수 있을 만큼 충분히 오랫동안 데이터를 보존할 수 있습니다.

## <a name="machine-learning-considerations-for-applying-personalizer"></a>Personalizer를 적용하기 위한 기계 학습 고려 사항

Personalizer는 개발자가 제공하는 피드백을 통해 학습되는 기계 학습 방법인 보충 학습을 기반으로 합니다. 

Personalizer에서 가장 효율적으로 학습할 수 있는 상황은 다음과 같습니다.
* 시간이 지남에 따라 문제가 발생하면 최적의 맞춤 설정(예: 뉴스 또는 패션의 기본 설정)에 따라 유지할 수 있는 충분한 이벤트가 있습니다. Personalizer는 현실 세계의 지속적인 변화에 적응할 수 있지만, 새로운 패턴을 검색하고 해결하는 데 충분한 이벤트와 데이터가 없으면 최적의 결과가 되지 않습니다. 자주 발생하는 사용 사례를 선택해야 합니다. 하루에 500회 이상 발생하는 사용 사례를 확인하는 것이 좋습니다.
* 컨텍스트 및 작업에는 학습을 용이하게 하는 기능이 충분히 있습니다.
* 호출당 순위를 지정하는 50개 미만의 작업이 있습니다.
* 데이터 보존 설정을 통해 Personalizer는 오프라인 평가 및 정책 최적화를 수행하는 데 충분한 데이터를 수집할 수 있습니다. 이는 일반적으로 50,000개 이상의 데이터 요소입니다.

## <a name="how-to-use-personalizer-in-a-web-application"></a>웹 애플리케이션에서 Personalizer를 사용하는 방법

웹 애플리케이션에 루프를 추가하는 방법은 다음과 같습니다.

* 개인 설정할 환경, 현재 갖고 있는 작업 및 기능, 사용할 컨텍스트 기능, 설정할 보상을 결정합니다.
* 애플리케이션에서 개인 설정 SDK에 대한 참조를 추가합니다.
* 개인 설정 준비가 완료되면 순위 API를 호출합니다.
* eventId를 저장합니다. 나중에 보상 API를 사용하여 보상을 보냅니다.
1. 사용자가 맞춤형 페이지를 본 것이 확실하면 이벤트에 대한 활성화를 호출합니다.
1. 사용자가 순위 콘텐츠를 선택할 때까지 기다립니다. 
1. 보상 API를 호출하여 순위 API의 출력이 얼마나 효과적인지 나타냅니다.

## <a name="how-to-use-personalizer-with-a-chat-bot"></a>챗봇에 Personalizer를 사용하는 방법

이 예제에서는 사용자에게 매번 일련의 메뉴 또는 선택 항목을 보내는 대신, 개인 설정을 사용하여 기본 제안을 작성하는 방법을 보여줍니다.

* 이 샘플의 [코드](https://github.com/Azure-Samples/cognitive-services-personalizer-samples/tree/master/samples/ChatbotExample)를 받습니다.
* 봇 솔루션을 설정합니다. LUIS 애플리케이션을 게시합니다. 
* 봇의 순위 및 보상 API 호출을 관리합니다.
    * LUIS 의도 처리를 관리하는 코드를 추가합니다. 상위 의도로 **없음**이 반환되거나 상위 의도 점수가 비즈니스 논리 임계값보다 낮으면 의도를 Personalizer에 보내서 의도의 순위를 지정합니다.
    * 의도 목록을 사용자가 선택할 수 있는 링크로 제공하되, 첫 번째 의도를 순위 API 응답의 최상위 의도로 배치합니다.
    * 사용자의 선택을 캡처하여 보상 API 호출로 보냅니다. 

### <a name="recommended-bot-patterns"></a>권장하는 봇 패턴

* 각 사용자에 대한 결과를 캐싱하는 것과는 반대로, 명확성이 필요한 때마다 Personalizer 순위 API를 호출합니다. 한 사람의 의도를 명확히 한 결과는 시간이 지남에 따라 변할 수 있으며, 순위 API가 달라진 결과를 검색하도록 허용하면 전체적인 학습 속도가 빨라집니다.
* 개인 설정에 필요한 데이터를 충분히 확보할 수 있도록 여러 사용자에게 공통으로 적용되는 상호 작용을 선택합니다. 예를 들어 소수의 사용자만 도달하는 대화 그래프의 심층 영역에서 이루어지는 소규모 의도 확인보다는 기초적인 질문이 더 좋을 수 있습니다.
* 순위 API 호출을 사용하여 "첫 번째 제안이 적합한" 대화를 만듭니다. 이때 사용자는 "X를 원하십니까?" 또는 "X를 찾으셨습니까?"라는 질문을 받게 되고, 사용자에게 메뉴에서 선택해야 하는 옵션을 제공하는 대신 간단하게 사용자가 확인하기만 하면 됩니다. 예를 들어 사용자가 "커피를 주문하고 싶습니다"라고 말하면 봇은 "더블 에스프레소를 원하십니까?"라고 말할 수 있습니다. 이 방식에서는 보상 신호가 한 가지 제안과 직접적인 관련이 있으므로 보상 신호 역시 강력합니다.

## <a name="how-to-use-personalizer-with-a-recommendation-solution"></a>추천 솔루션에 Personalizer를 사용하는 방법

추천 엔진을 사용하여 큰 카탈로그를 몇 개 항목으로 필터링합니다. 그런 다음, 해당 항목을 실행 가능한 30개 작업으로 순위 API에 전송할 수 있습니다.

추천 엔진을 Personalizer에 사용하여 다음과 같은 일을 할 수 있습니다.

* [추천 솔루션](https://github.com/Microsoft/Recommenders/)을 설정합니다. 
* 페이지를 표시할 때 추천 모델을 호출하여 간단한 추천 목록을 가져옵니다.
* 개인 설정을 호출하여 추천 솔루션의 출력 순위를 지정합니다.
* 보상 API를 호출하여 사용자 작업에 대한 피드백을 보냅니다.


## <a name="pitfalls-to-avoid"></a>문제 방지

* 맞춤형 동작을 모든 사용자가 검색할 수 없고 특정 사용자만 기억해야 하거나 사용자별 대안 목록의 일부로 제공되는 Personalizer는 사용하지 마세요. 예를 들어 Personalizer를 사용하여 20가지 메뉴 항목 목록 중에서 첫 번째 피자 주문을 추천하는 것은 유용하지만, 아이 돌보기와 관련하여 도움이 필요할 때 사용자의 연락처 목록의 전화 번호(예: "할머니")로 연락을 하는 것은 사용자 기반에서 개인 설정이 불가능합니다.


## <a name="adding-content-safeguards-to-your-application"></a>애플리케이션에 콘텐츠 세이프가드 추가

애플리케이션에서 사용자에게 표시하는 콘텐츠를 다양하게 변형할 수 있고 그 콘텐츠 중 일부가 안전하지 않거나 일부 사용자에게 적절하지 않은 경우 사용자가 허용되지 않는 콘텐츠를 볼 수 없게 적절한 세이프가드를 작동할 계획을 미리 세워야 합니다. 세이프가드를 구현하는 가장 좋은 패턴은 다음과 같습니다. 세이프가드를 구현하는 가장 좋은 패턴은 다음과 같습니다.
    * 순위를 지정할 작업 목록을 획득합니다.
    * 대상 그룹에 대해 실현할 수 없는 작업을 필터링합니다.
    * 실행 가능한 작업만 순위를 지정합니다.
    * 상위 작업을 사용자에게 표시합니다.

일부 아키텍처에서는 위의 시퀀스를 구현하기 어려울 수 있습니다. 이 경우 순위 지정 후 세이프가드를 구현하는 대안이 있지만, 세이프가드 범위를 벗어나는 작업이 Personalizer 모델 학습에 사용되지 않도록 프로비저닝해야 합니다.

* 학습이 비활성화된 순위를 지정할 작업 목록을 가져옵니다.
* 작업의 순위를 지정합니다.
* 최상위 작업을 실현할 수 있는지 확인합니다.
    * 최상위 작업을 실현할 수 있는 경우 이 순위에 대한 학습을 활성화하고 사용자에게 표시합니다.
    * 최상위 작업을 실현할 수 없는 경우 이 순위에 대한 학습을 활성화하지 말고, 자체 논리 또는 대안을 확인하여 사용자에게 무엇을 표시할 것인지 결정합니다. 두 번째 순위 옵션을 사용하는 경우에도 이 순위에 대한 학습을 활성화하지 마세요.

## <a name="verifying-adequate-effectiveness-of-personalizer"></a>Personalizer의 효율성 확인

[오프라인 평가](how-to-offline-evaluation.md)를 수행하여 Personalizer의 효율성을 정기적으로 모니터링할 수 있습니다.

## <a name="next-steps"></a>다음 단계

[Personalizer를 사용할 수 있는 경우](where-can-you-use-personalizer.md)를 이해합니다.