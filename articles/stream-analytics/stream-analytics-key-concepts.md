<properties 
	pageTitle="스트림 분석 주요 개념에 대해 알아보기 | Microsoft Azure" 
	description="Azure 스트림 분석의 주요 개념에 대해 알아보기: 지원되는 입력과 출력, 작업 구성 및 메트릭을 포함하여 스트림 분석 작업의 구성 요소입니다." 
	keywords="event processing,data stream,key concepts,serialization"	
	services="stream-analytics" 
	documentationCenter="" 
	authors="jeffstokes72" 
	manager="paulettm" 
	editor="cgronlun" />

<tags 
	ms.service="stream-analytics" 
	ms.devlang="na" 
	ms.topic="article" 
	ms.tgt_pltfrm="na" 
	ms.workload="data-services" 
	ms.date="04/28/2015" 
	ms.author="jeffstok" />


# 스트림 분석 핵심 개념: 스트림 분석 작업의 기본 사항 가이드 

Azure 스트림 분석은 완전히 관리되는 서비스로, 클라우드의 데이터 스트리밍에 대해 대기 시간이 짧고 확장성이 뛰어난 고가용성의 복합 이벤트 처리 기능을 제공합니다. 스트림 분석 기능은 고객이 데이터 스트림을 분석하도록 스트리밍 작업을 설정하고 거의 실시간으로 분석할 수 있도록 해 줍니다. 이 문서에서는 스트림 분석 작업의 주요 개념을 설명합니다.

## 스트림 분석에서 무엇을 수행할 수 있나요?
스트림 분석을 사용 하면 다음과 같은 작업을 수행할 수 있습니다.

- 고용량 및 고속 데이터 스트림에 대한 복합 이벤트 처리를 수행합니다.   
- 커넥티드 카 또는 유틸리티 그리드와 같은 전역으로 분산된 자산이나 장비에서 이벤트 데이터를 수집합니다. 
- 거의 실시간으로 모니터링하고 진단하도록 원격 분석 데이터를 처리합니다. 
- 향후 처리를 위해 실시간 이벤트 캡처 및 보관

자세한 내용은 [Azure 스트림 분석 소개](stream.analytics.introduction)를 참조하세요.

스트림 분석 작업은 * 하나 이상의 입력 소스 * 들어오는 데이터 스트림을 통해 쿼리 * 출력 대상 모두를 포함합니다.


## 입력

### 데이터 스트림

각 스트림 분석 작업 정의는 작업에서 사용되고 변환될 하나 이상의 데이터 스트림 입력 소스를 포함해야 합니다. [Azure Blob 저장소](azure.blob.storage) 및 [Azure 이벤트 허브](azure.event.hubs)는 데이터 스트림 입력된 소스로 지원됩니다. 이벤트 허브 입력 소스는 여러 다른 장치 및 서비스에서 이벤트 스트림을 수집하는 데 사용되며, Blob 저장소는 많은 양의 데이터를 수집하기 위한 입력 소스로 사용될 수 있습니다. Blob은 데이터를 스트리밍하지 않으므로 Blob의 레코드에 타임스탬프가 포함되지 않은 한, Blob을 통한 스트림 분석 작업은 본질적으로 일시적이지 않습니다.

### 참조 데이터
스트림 분석 기능은 또 다른 유형의 입력 소스인 참조 데이터도 지원하니다. 이 데이터는 상관 관계를 지정하고 조회를 수행하는 데 사용되는 보조 데이터로, 일반적으로 정적이며 자주 변경되지 않습니다. [Azure Blob 저장소](azure.blob.storage)는 유일하게 지원되는 참조 데이터용 입력 소스입니다. 참조 데이터 소스 Blob은 크기가 50MB로 제한됩니다.

참조 데이터를 새로 고칠 수 있도록 설정하려면 사용자가 경로 패턴 내에서 {날짜} 및 {시간} 토큰을 사용하여 입력 구성에 Blob 목록을 지정해야 합니다. 작업은 UTC 표준 시간대를 사용하여 Blob 이름으로 인코딩된 날짜 및 시간에 기반한 해당 Blob을 로드합니다.

예를들어, 해당 작업에 날짜 형식이 “YYYY-MM-DD”이며 시간 형식이 “HH:mm”인 /sample/{date}/{time}/products.csv와 같은 경로 패턴이 있는 포털에서 구성된 참조 입력이 있는 경우, 작업은 UTC 표준 시간대로 2015년 4월 16일 오후 5시 30분에(PST 표준 시간대 사용 시 2015년 4월 16일 오전 10시 30분에 해당) /sample/2015-04-16/17:30/products.csv라는 이름의 파일을 선택합니다.


### 직렬화
쿼리가 올바르게 작동하려면 스트림 분석 기능에서 들어오는 데이터 스트림에 사용되는 serialization 형식을 인식해야 합니다. 현재 지원되는 형식은 데이터 스트리밍의 경우 JSON, CSV 및 Avro이고, 참조 데이터의 경우 CSV 또는 JSON입니다.

### 생성된 속성
작업에 사용되는 입력 형식에 따라 일부 추가 필드가 이벤트 메타데이터와 함께 생성됩니다. 다른 입력 열과 마찬가지로 이러한 필드를 쿼리할 수 있습니다. 기존 이벤트에 아래의 속성 중 하나와 이름이 같은 필드가 있으면 입력 메타데이터로 덮어쓰입니다.

<table border="1">
	<tr>
		<th></th>
		<th>속성</th>
		<th>설명</th>
	</tr>
	<tr>
		<td rowspan="4" valign="top"><strong>Blob</strong></td>
		<td>BlobName</td>
		<td>이 이벤트가 발생한 입력 Blob의 이름입니다.</td>
	</tr>
	<tr>
		<td>EventProcessedUtcTime</td>
		<td>Blob 레코드가 처리된 날짜 및 시간입니다.</td>
	</tr>
	<tr>
		<td>BlobLastModifiedUtcTime</td>
		<td>Blob이 마지막으로 수정된 날짜 및 시간입니다.</td>
	</tr>
	<tr>
		<td>PartitionId</td>
		<td>입력 어댑터의 0부터 시작하는 파티션 ID입니다.</td>
	</tr>
	<tr>
		<td rowspan="3" valign="top"><b>이벤트 허브</b></td>
		<td>EventProcessedUtcTime</td>
		<td>이벤트가 처리되는 날짜 및 시간입니다.</td>
	</tr>
	<tr>
		<td>EventEnqueuedUtcTime</td>
		<td>이벤트 허브에서 이벤트를 받은 날짜 및 시간입니다.</td>
	</tr>
	<tr>
		<td>PartitionId</td>
		<td>입력 어댑터의 0부터 시작하는 파티션 ID입니다.</td>
	</tr>
</table>

###느리거나 입력 데이터가 없는 파티션
여러 파티션이 있는 입력 소스 및 뒤에 하나 이상의 파티션 지연이 있거나 데이터가 없는 입력 소스를 읽을 때 스트리밍 작업은 시스템을 통해 흐르는 이벤트를 유지하기 위해 이 상황을 어떻게 처리할지 결정합니다. 입력 설정 ‘최대로 허용되는 도착 지연’은 해당 동작을 제어하며 기본적으로 데이터를 무기한으로 기다리도록 설정되어 있습니다. 이는 이벤트의 타임스탬프가 대체되지 않지만 해당 이벤트는 가장 느린 입력 파티션에 기반하여 흐르며 하나 이상의 입력 파티션에 데이터가 없는 경우 흐름을 중단함을 의미합니다. 이는 데이터가 입력 파티션에 균등하게 분산되어 있고 이벤트 사이의 시간 일관성이 중요한 경우에 유용합니다.

사용자는 제한된 시간 동안만 기다리도록 결정할 수도 있으며, ‘최대로 허용되는 도착 지연’은 뒤에 지연되는 입력 파티션을 유지하며, ‘지연된 이벤트에 대한 동작’에 따라 이벤트에 대해 동작하고, 데이터가 나중에 도착하는 경우 해당 이벤트를 중단하거나 해당 이벤트의 타임스탬프를 조절하여 작업을 진행하도록 결정한 이후에 지연을 결정합니다. 이는 대기 시간이 중요하며 타임스탬프 이동이 허용되지만 입력이 균등하게 분배되지 않을 수 있는 경우에 유용합니다.

###순서 비지정 이벤트 파티션
스트리밍 작업 쿼리가 TIMESTAMP BY 키워드를 사용할 때, 입력을 위해 도착할 이벤트에서 순서에 대한 보장이 없습니다. 동일한 입력 파티션에서 일부 이벤트는 지연될 수 있고, 이벤트를 중단하거나 이벤트의 타임스탬프를 조절하는 ‘지연된 이벤트에 대한 동작’ 설정에 따라 매개 변수 ‘입력 내에서 최대로 허용된 순서 비지정’는 스트리밍 작업이 순서 비지정이 허용되는 이벤트에 대한 동작하도록 합니다.

### 추가 리소스
입력 소스를 만드는 방법에 대한 자세한 내용은 [Azure 이벤트 허브 개발자 가이드](azure.event.hubs.developer.guide) 및 [Azure Blob 저장소 사용](azure.blob.storage.use)을 참조하세요.



## 쿼리
들어오는 데이터를 필터링, 조작 및 처리하기 위한 논리는 스트림 분석 쿼리 작업에 정의됩니다. 쿼리는 임시 쿼리를 위한 일부 특정 확장이 있는 표준 Transact-SQL 구문에 주로 속하는 SQL 유사 언어인 스트림 분석 쿼리 언어를 사용하여 작성됩니다.

### 기간 이동
기간 이동 확장을 통해 특정 기간에 속하는 이벤트 하위 집합을 집계하고 계산할 수 있습니다. 기간 이동 함수는 **GROUP BY** 문을 사용하여 호출됩니다. 예를 들어 다음 쿼리는 초당 받은 이벤트 개수를 계산합니다.

	SELECT Count(*) 
	FROM Input1 
	GROUP BY TumblingWindow(second, 1) 

### 실행 단계
좀 더 복잡한 쿼리의 경우 표준 SQL 절 **WITH**를 사용하여 임시로 명명된 결과 집합을 지정할 수 있습니다. 예를 들어 다음 쿼리는 **WITH**를 사용하여 두 실행 단계를 통해 변환합니다.
 
	WITH step1 AS ( 
		SELECT Avg(Reading) as avr 
		FROM temperatureInput1 
		GROUP BY Building, TumblingWindow(hour, 1) 
	) 

	SELECT Avg(avr) AS campus_Avg 
	FROM step1 
	GROUP BY TumblingWindow (day, 1) 

쿼리 언어에 대한 자세한 내용은 [Azure 스트림 분석 쿼리 언어 참조](stream.analytics.query.language.reference)를 참조하세요.

## 출력
출력 대상은 스트림 분석 작업의 결과를 쓸 위치입니다. 결과는 작업이 입력 이벤트를 처리할 때 출력 대상에 계속 기록됩니다. 다음 출력 대상이 지원됩니다.

- Azure 이벤트 허브 - 장치로 다시 명령을 실행하는 경우처럼 여러 스트리밍 파이프라인을 함께 구성해야 하는 시나리오에 대한 출력 대상으로 이벤트 허브를 선택합니다.
- Azure Blob 저장소 - 출력을 장기간 보관하거나 나중에 처리하기 위해 데이터를 저장하는 데 Blob 저장소를 사용합니다.
- Azure 테이블 저장소 - Azure 테이블 저장소는 스키마에 제약 조건이 더 적은 구조화된 데이터 저장소입니다. 다른 스키마 및 다른 유형의 엔터티는 동일한 Azure 테이블에 저장될 수 있습니다. Azure 테이블 저장소는 지속적이고 효율적인 검색을 위해 데이터를 저장하는 데 사용할 수 있습니다. 자세한 내용은 [Azure 저장소 소개](../storage.introduction.md) 및 [Azure 테이블 저장소에 대한 확장 가능한 분할 전략 설계](https://msdn.microsoft.com/library/azure/hh508997.aspx)를 참조하세요.
- Azure SQL 데이터베이스 - 이 출력 대상은 원래 관계형인 데이터나 데이터베이스에 호스트되고 있는 콘텐츠를 필요로 하는 응용 프로그램에 적합합니다.


## 작업 규모 지정

스트림 분석 작업은 작업이 수신하는 데이터 처리 능력의 크기를 정의하는 스트리밍 단위를 구성하여 확장할 수 있습니다. 각 스트리밍 단위는 대략 1MB/초의 처리량에 해당합니다. 각 구독에서 해당 지역의 작업에는 지역당 12개 스트리밍 단위 할당량이 배정되어야 합니다.

자세한 내용은 [Azure 스트림 분석 작업 규모 지정](stream.analytics.scale.jobs)을 참조하세요.


## 작업 모니터링 및 문제 해결

### 지역별 모니터링 저장소 계정

작업 모니터링을 사용하려면 스트림 분석 작업을 포함하는 각 지역의 데이터를 모니터링을 위한 Azure 저장소 계정을 지정해야 합니다. 이 계정은 작업을 만들 때 구성됩니다.

### 메트릭
다음 메트릭은 스트림 분석 작업의 사용 현황 및 성능을 모니터링하는 데 사용할 수 있습니다.

- 오류 - 스트림 분석 작업에 의해 발생한 오류 메시지의 수입니다.
- 입력 이벤트 - 이벤트 개수를 기준으로 스트림 분석 작업이 받은 데이터의  
- 양입니다.
- 출력 이벤트 - 이벤트 개수를 기준으로 스트림 분석 작업이 출력 대상에 보낸 데이터의 양입니다.
- 순서 비지정 이벤트 - 순서 비지정 관련 정책에 기반하여 조정된 타임스탬프를 받거나 삭제된 순서가 정해지지 않은 수신 이벤트의 수입니다.
- 데이터 변환 오류 - 스트림 분석 작업에 의해 발생하는 데이터 변환 오류 수입니다.

### 작업 로그
스트림 분석 작업을 디버깅하거나 문제를 해결하는 가장 좋은 방법은 Azure 작업 로그를 사용하는 것입니다. 작업 로그는 포털의 **관리 서비스** 섹션에서 액세스할 수 있습니다. 작업 로그를 검사하려면 **서비스 유형**을 **스트림 분석**으로, **서비스 이름**을 작업의 이름으로 설정합니다.


## 작업 관리 

### 작업 시작 및 중지
작업을 시작할 때 **시작 출력** 값을 지정하라는 메시지가 표시됩니다. 이는 이 작업이 결과 출력을 생성할 때를 결정합니다. 연결된 쿼리에 기간이 포함된 경우 작업은 지정된 시간에 첫 번째 출력 이벤트를 생성하기 위해 필요한 기간이 시작될 때 입력 소스에서 입력을 받기 시작합니다. **작업 시작 시간**, **사용자 지정** 및 **마지막 중지 시간**의 세 가지 선택 사항이 있습니다. 기본 설정은 **작업 시작 시간**입니다. 작업이 임시로 중단되는 경우, 가장 좋은 방법은 마지막 출력 시간부터 작업을 다시 시작하고 데이터 손실을 피하기 위해 **시작 출력** 값으로 **마지막으로 중단된 시간**을 선택하는 것입니다. **사용자 지정** 선택 사항의 경우 날짜와 시간을 지정해야 합니다. 이 설정은 입력 소스에 사용할 기록 데이터 양을 지정하거나 특정 시간부터 데이터 수집을 시작하려는 경우에 유용합니다.

### 작업 구성
스트림 분석 작업에 대해 다음과 같은 최상위 수준 설정을 조정할 수 있습니다.

- **출력 시작** - 이 설정을 사용하여 이 작업이 결과 출력 생성을 시작하는 시기를 지정합니다. 연결된 쿼리에 기간이 포함된 경우 작업은 지정된 시간에 첫 번째 출력 이벤트를 생성하기 위해 필요한 기간이 시작될 때 입력 소스에서 입력을 받기 시작합니다. **작업 시작 시간**과 **사용자 지정**의 두 가지 옵션을 사용할 수 있습니다. 기본 설정은 **작업 시작 시간**입니다. **사용자 지정** 옵션의 경우 날짜와 시간을 지정해야 합니다. 이 설정은 입력 소스에서 사용할 기록 데이터 양을 지정하거나 작업이 마지막으로 중지되었을 때와 같은 특정 시간부터 데이터 수집을 시작하려는 경우에 유용합니다. 
- **순서 비지정 관련 정책** - 순차적으로 스트림 분석 작업에 도착하지 않는 이벤트 처리를 위한 설정입니다. 시간 임계값을 지정하여 허용 오차 기간에 포함된 이벤트의 순서를 다시 지정하고 이 기간을 벗어나는 이벤트에 대해 **삭제** 또는 **조정** 중 수행할 조치를 결정할 수 있습니다. **삭제** 순서 없이 수신된 모든 이벤트를 삭제하며 **조정**은 시스템을 변경합니다. 가장 최근에 수신한 순서가 지정된 이벤트의 타임스탬프의 순서가 이벤트의 타임 스탬프입니다. 
- **지연된 도착 관련 정책** - 여러 파티션이 있는 입력 소스 및 뒤에 하나 이상의 파티션 지연이 있거나 데이터가 없는 입력 소스를 읽을 때 스트리밍 작업은 시스템을 통해 흐르는 이벤트를 유지하기 위해 이 상황을 어떻게 처리할지 결정합니다. 입력 설정 ‘최대로 허용되는 도착 지연’은 해당 동작을 제어하며 기본적으로 데이터를 무기한으로 기다리도록 설정되어 있습니다. 이는 이벤트의 타임스탬프가 대체되지 않지만 해당 이벤트는 가장 느린 입력 파티션에 기반하여 흐르며 하나 이상의 입력 파티션에 데이터가 없는 경우 흐름을 중단함을 의미합니다. 이는 데이터가 입력 파티션에 균등하게 분산되어 있고 이벤트 사이의 시간 일관성이 중요한 경우에 유용합니다. 사용자는 제한된 시간 동안만 기다리도록 결정할 수도 있으며, ‘최대로 허용되는 도착 지연’은 뒤에 지연되는 입력 파티션을 유지하며, ‘지연된 이벤트에 대한 동작’에 따라 이벤트에 대해 동작하고, 데이터가 나중에 도착하는 경우 해당 이벤트를 중단하거나 해당 이벤트의 타임스탬프를 조절하여 작업을 진행하도록 결정한 이후에 지연을 결정합니다. 이는 대기 시간이 중요하며 타임스탬프 이동이 허용되지만 입력이 균등하게 분배되지 않을 수 있는 경우에 유용합니다.
- **로캘** - 이 설정을 사용하여 스트림 분석 작업에 대한 국제화 기본 설정을 지정할 수 있습니다. 데이터의 타임스탬프는 로캘과 관련이 없지만 여기에서 설정한 내용은 작업이 데이터를 구문 분석하고, 비교하고, 정렬하는 방식에 영향을 줍니다. 미리 보기 릴리스의 경우 **en-US**만 지원됩니다.

### 상태

Azure 포털에서 스트림 분석 작업의 상태를 검사할 수 있습니다. 실행 중인 작업은 **유휴 상태**, **처리 중** 또는 **저하됨**의 세 가지 상태 중 하나일 수 있습니다. 이러한 각 상태에 대한 정의는 아래와 같습니다.

- **유휴 상태** - 작업을 만든 이후 또는 마지막 2분 내에 보이는 입력 바이트가 없습니다. 작업이 오랫동안 **유휴 상태**인 경우 입력이 존재할 가능성이 있지만 처리할 원시 바이트가 없습ㄴ디ㅏ.
- **처리 중** - 필터링된 입력된 이벤트의 0이 아닌 양을 스트림 분석 작업을 통해 성공적으로 사용했습니다. 작업이 출력을 생성하지 않고 **처리 중** 상태인 경우, 데이터 처리 시간 창이 크거나 쿼리 논리가 복잡할 가능성이 있습니다.
- **저하됨** - 이 상태는 스트림 분석 작업에 입력/출력 통신 오류, 쿼리 오류 또는 다시 시도 가능한 런타임 오류 중 하나가 발생함을 나타냅니다. 작업에 발생한 오류 유형을 구분하려면 작업 로그를 확인합니다.


## 지원 받기
추가 지원이 필요한 경우 [Azure 스트림 분석 포럼](https://social.msdn.microsoft.com/Forums/en-US/home?forum=AzureStreamAnalytics)을 참조하세요.


## 다음 단계

이제 스트림 분석의 주요 개념에 익숙할 것입니다.

- [Azure 스트림 분석 소개](stream-analytics-introduction.md)
- [Azure 스트림 분석 사용 시작](stream-analytics-get-started.md)
- [Azure 스트림 분석 작업 규모 지정](stream-analytics-scale-jobs.md)
- [Azure 스트림 분석 쿼리 언어 참조](https://msdn.microsoft.com/library/azure/dn834998.aspx)
- [Azure 스트림 분석 관리 REST API 참조](https://msdn.microsoft.com/library/azure/dn835031.aspx)





<!--Link references-->
[azure.blob.storage]: http://azure.microsoft.com/documentation/services/storage/
[azure.blob.storage.use]: http://azure.microsoft.com/documentation/articles/storage-dotnet-how-to-use-blobs/

[azure.event.hubs]: http://azure.microsoft.com/services/event-hubs/
[azure.event.hubs.developer.guide]: http://msdn.microsoft.com/library/azure/dn789972.aspx

[stream.analytics.query.language.reference]: http://go.microsoft.com/fwlink/?LinkID=513299
[stream.analytics.forum]: http://go.microsoft.com/fwlink/?LinkId=512151

[stream.analytics.introduction]: stream-analytics-introduction.md
[stream.analytics.get.started]: stream-analytics-get-started.md
[stream.analytics.developer.guide]: ../stream-analytics-developer-guide.md
[stream.analytics.scale.jobs]: stream-analytics-scale-jobs.md
[stream.analytics.limitations]: ../stream-analytics-limitations.md
[stream.analytics.query.language.reference]: http://go.microsoft.com/fwlink/?LinkID=513299
[stream.analytics.rest.api.reference]: http://go.microsoft.com/fwlink/?LinkId=517301

<!---HONumber=58--> 