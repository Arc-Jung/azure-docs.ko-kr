<properties
   pageTitle="Azure의 Elasticsearch에서 복원력 및 복구 구성"
   description="Elasticsearch에 대한 복원력 및 복구와 관련된 고려 사항."
   services=""
   documentationCenter="na"
   authors="mabsimms"
   manager="marksou"
   editor=""
   tags=""/>

<tags
   ms.service="guidance"
   ms.devlang="na"
   ms.topic="article"
   ms.tgt_pltfrm="na"
   ms.workload="na"
   ms.date="02/18/2016"
   ms.author="masimms"/>
   
# Azure의 Elasticsearch에서 복원력 및 복구 구성

이 문서는 [시리즈의 일부](guidance-elasticsearch.md)입니다.

Elasticsearch의 주요 기능은 노드 오류 및/또는 네트워크 파티션 이벤트 시 복원 기능을 제공하는 지원입니다. 복제는 Elasticsearch를 통해 하나의 노드가 액세스할 수 없게 되는 경우에 다른 노드에 모든 데이터 항목의 복사본 하나를 사용할 수 있도록 하는 모든 클러스터의 복원력을 향상시킬 수 있는 가장 확실한 방법입니다. 노드를 일시적으로 사용할 수 없는 경우 누락된 노드에서 데이터의 복제본을 포함하는 다른 노드는 문제가 해결될 때까지 누락된 데이터를 제공할 수 있습니다. 장기적인 문제 발생 시 누락된 노드를 새 노드로 바꿀 수 있으며 Elasticsearch는 복제본에서 새 노드에 데이터에 복원할 수 있습니다.

여기에서 Azure에서 호스트되는 경우 Elasticsearch와 함께 사용할 수 있는 복원 기능 및 복구 옵션을 요약하고 데이터 손실의 가능성 및 확장된 데이터 복구 시간을 최소화하기 위해 고려해야 하는 Elasticsearch 클러스터의 몇 가지 중요한 측면을 설명합니다.

또한 이 문서는 Elasticsearch 클러스터에 다양한 유형의 오류가 미치는 영향을 보여 주도록 수행되는 일부 샘플 테스트 및 복구하는대로 시스템이 반응하는 방법을 보여 줍니다.

Elasticsearch 클러스터는 복제본을 사용하여 가용성을 유지 관리하고 읽기 성능을 향상시킵니다. 복제본은 복제하는 주 분할된 데이터베이스의 다른 VM에 저장되어야 합니다. 데이터 노드를 호스팅하는 VM이 실패하거나 사용할 수 없게 되는 경우 시스템이 복제본을 보유하는 VM을 사용하여 계속해서 작동할 수 있도록 합니다.

## 전용 마스터 노드 사용

Elasticsearch 클러스터의 노드 중 하나는 마스터 노드로 선택됩니다. 이 노드의 목적은 다음과 같은 클러스터 관리 작업을 수행하는 것입니다.

- 실패한 노드 검색 및 복제본으로 전환,

- 노드 워크로드를 분산하도록 분할된 데이터베이스 재배치 및

- 노드가 다시 온라인 상태가 될 때 분할된 데이터베이스 복구.

중요한 클러스터에서 전용 마스터 노드를 사용해야 하며 유일한 역할이 마스터가 되도록 3개의 전용 노드가 있는지 확인해야 합니다. 이 구성은 이러한 노드가 수행해야 하는 리소스 집약적인 작업의 양을 낮추고(데이터를 저장하거나 쿼리를 처리하지 않음) 클러스터 안정성을 향상시킬 수 있습니다. 이러한 노드 중 하나가 선택되지만 다른 노드는 시스템 상태의 복사본을 포함하고 마스터 노드 실패 시 넘겨 받을 수 있습니다.

## Azure를 사용하여 고가용성 제어 - 업데이트 도메인 및 장애 도메인 

다른 VM은 동일한 실제 하드웨어를 공유할 수 있습니다. Azure 데이터 센터에서 단일 랙은 많은 VM을 호스팅할 수 있고 이러한 모든 VM은 공통 전원 및 네트워크 스위치를 공유합니다 따라서 단일 랙 수준의 오류는 VM의 수에 영향을 줄 수 있습니다. Azure는 장애 도메인(FD)의 개념을 사용하여 이러한 위험을 시도하고 분산합니다. FD는 동일한 랙을 공유하는 VM 그룹에 해당합니다. 랙 수준의 오류가 노드 및 모든 해당 복제 데이터베이스를 동시에 보유하는 노드와 충돌하지 않도록 하려면 VM이 FD에 분산되어 있는지 확인해야 합니다.

마찬가지로, VM은 [Azure 패브릭 컨트롤러](https://azure.microsoft.com/documentation/videos/fabric-controller-internals-building-and-updating-high-availability-apps/)로 수행되어 계획된 유지 관리 및 운영 체제 업그레이드를 수행할 수 있습니다. Azure는 업데이트 도메인(UD)에 VM을 할당합니다. 계획된 유지 관리 이벤트가 발생하는 경우 단일 UD의 VM은 언제든지 영향을 받으며 다른 UD의 VM은 업데이트되는 UD의 VM이 다시 온라인 상태로 전환될 때까지 실행 상태로 남아 있습니다. 따라서 노드를 호스팅하는 VM 및 다른 UD에 속하는 해당 복제본이 언제든지 가능한지 확인해야 합니다.

> [AZURE.NOTE] FD 및 UD에 대한 자세한 내용은 [가상 컴퓨터의 가용성 관리][]를 참조하세요.

특정 UD 및 FD에 VM을 명시적으로 할당할 수 없습니다. 이 할당은 VM이 만들어질 때 Azure에 의해 제어됩니다. 자세한 내용은 문서 [가상 컴퓨터의 가용성 관리][]를 참조하세요. 그러나 VM을 가용성 집합(AS)의 일부로 만들어야 하는지 지정할 수 있습니다. 동일한 AS의 VM은 UD 및 FD에 걸쳐 분산됩니다. VM을 수동으로 만드는 경우 Azure는 두 개의 FD 및 다섯 개의 UD로 각 AS를 연결하고 컴퓨터는 이러한 FD 및 UD에 할당되고 추가 VM으로 순환 주기는 다음과 같이 프로비전됩니다.

- AS에 프로비전된 첫 번째 VM은 첫 번째 FD(FD 0) 및 첫 번째 UD(UD 0)에 배치됩니다.
- AS에 프로비전된 두 번째 VM은 FD 1 및 UD 1에 배치됩니다.
- AS에 프로비전된 세 번째 VM은 FD 0 및 UD 2에 배치됩니다. AS에 프로비전된 네 번째 VM은 FD 1 및 UD 3에 배치됩니다.
- AS에 프로비전된 다섯 번째 VM은 FD 0 및 UD 4에 배치됩니다.
- AS에 프로비전된 여섯 번째 VM은 FD 1 및 UD 0에 배치됩니다.
- AS에 프로비전된 일곱 번째 VM은 FD 0 및 UD 1에 배치됩니다.

> [AZURE.IMPORTANT] ARM(Azure Resource Manager)을 사용하여 VM을 만드는 경우 각 가용성 집합은 최대 3개의 FD와 20개의 UD에 할당될 수 있습니다. 이것이 ARM을 사용하는 특별한 이유입니다.

일반적으로 동일한 가용성 집합에 동일한 목적을 제공하는 모든 VM을 배치하지만 다른 기능을 수행하는 VM에 대한 다른 가용성 집합을 만듭니다. 즉 Elasticsearch를 사용하여 다음에 대한 별도 가용성 집합을 만드는 것을 고려해야 함을 의미합니다.

- 데이터 노드를 호스팅하는 VM.
- 클라이언트 노드를 호스팅하는 VM(사용하는 경우).
- 마스터 노드를 호스팅하는 VM.

또한 클러스터의 각 노드가 속한 장애 도메인 및 업데이트 도메인인지 확인해야 합니다. 이 정보는 동시에 수행되는 분할된 데이터베이스 및 해당 복제본에 대한 범위를 최소화하도록 Elasticsearch가 동일한 오류 및 업데이트 도메인에 분할된 데이터베이스 및 해당 복제본을 만들지 않는 것을 확인할 수 있습니다. [분할된 데이터베이스 할당 인식](https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-awareness.html#allocation-awareness)을 구성하여 클러스터의 하드웨어 배포를 미러링하도록 Elasticsearch 노드를 구성할 수 있습니다 합니다. 예를 들어 다음과 같이 elasticsearch.yml 파일에서 *faultDomain* 및 *updateDomain*이라는 한 쌍의 사용자 지정 노드 특성을 정의할 수 있습니다.

```yaml
node.faultDomain: \${FAULTDOMAIN}
node.updateDomain: \${UPDATEDOMAIN}
```

이 경우 특성은 Elasticsearch를 시작할 때 *\\${FAULTDOMAIN}* 및 *\\${UPDATEDOMAIN}* 환경 변수에 저장된 값을 사용하여 설정됩니다. 또한 *faultDomain* 및 *updateDomain*이 인식 특성에 할당됐음을 나타내도록 다음 항목을 Elasticsearch.yml 파일에 추가하고 이러한 특성에 대해 허용되는 값 집합을 지정해야 합니다.

```yaml
cluster.routing.allocation.awareness.force.updateDomain.values: 0,1,2,3,4
cluster.routing.allocation.awareness.force.faultDomain.values: 0,1
cluster.routing.allocation.awareness.attributes: updateDomain, faultDomain
```

[분할된 데이터베이스 할당 필터링](https://www.elastic.co/guide/en/elasticsearch/reference/2.0/shard-allocation-filtering.html#shard-allocation-filtering)과 함께 분할된 데이터베이스 할당 인식을 사용하여 명시적으로 어떤 노드가 지정된 인덱스에 대해 분할된 데이터베이스를 호스팅할 수 있는지 지정할 수 있습니다.

AS에서 FD 및 UD 수를 초과하여 확장해야 할 경우 추가 AS에서 VM을 만들 수 있습니다. 그러나 다른 AS의 노드가 유지 관리를 위해 동시에 수행될 수 있다는 것을 이해해야 합니다. 각 분할된 데이터베이스 및 해당 복제본 중 하나 이상이 동일한 AS 내에 포함되어 있는지 확인합니다.

> [AZURE.NOTE] 현재 작업은 AS당 100개의 VM으로 제한됩니다. 자세한 내용은 [Azure 구독 및 서비스 제한, 할당량 및 제약 조건](../azure-subscription-service-limits.md)을 참조하세요.

### 백업 및 복원

복제본 사용은 치명적인 오류(예: 실수로 전체 클러스터 삭제)로부터 완벽한 보호를 제공하지 않습니다. 클러스터의 데이터를 정기적으로 백업하고 이러한 백업에서 시스템을 복원하기 위한 전략을 시도하고 테스트해야 합니다.

Elasticsearch [스냅숏 및 복원 API](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html)를 사용하여 인덱스를 백업 및 복원합니다. 스냅숏은 공유 파일 시스템에 저장할 수 있습니다. 또는 HDFS([HDFS 플러그 인](https://github.com/elasticsearch/elasticsearch-hadoop/tree/master/repository-hdfs)) 또는 Azure 저장소([Azure 클라우드 플러그 인](https://github.com/elasticsearch/elasticsearch-cloud-azure#azure-repository))에 스냅숏을 작성할 수 있는 플러그 인을 사용할 수 있습니다.

스냅숏 저장소 메커니즘을 선택하는 경우 다음 사항을 고려해야 합니다.

- [Azure 파일 저장소](https://azure.microsoft.com/services/storage/files/)를 사용하여 모든 노드에서 액세스할 수 있는 공유 파일 시스템을 구현할 수 있습니다.

- Hadoop와 함께 Elasticsearch를 실행하는 경우 HDFS 플러그 인을 사용합니다.

- HDFS 플러그 인은 JVM의 Elasticsearch 인스턴스 내에서 실행하는 Java 보안 관리자를 사용하지 않도록 설정해야 합니다.

- HDFS 플러그 인은 올바른 Hadoop 구성이 Elasticsearch에서 사용되는지 제공되는 모든 HDFS 호환 파일 시스템을 지원합니다.

  
## 노드 간 일시적인 연결 처리

일시적인 네트워크 결함, VM은 데이터 센터에서 정기 유지 관리 후 다시 부팅하고 기타 비슷한 이벤트는 노드를 일시적으로 액세스할 수 없게 될 수 있습니다. 이벤트의 수명이 짧을 수 있는 이러한 상황에서 분할된 데이터베이스 균형 조정의 오버헤드는 빠르게 연속적으로 두 번 발생하고(오류가 감지된 경우 한 번, 노드가 마스터에 표시될 때 다시) 성능에 영향을 주는 중요한 오버헤드가 될 수 있습니다. 인덱스 또는 모든 인덱스에 대한 *delayed\_timeout* 속성을 설정하여 마스터가 클러스터를 균형 조정하도록 임시 노드 액세스 비가능성을 방지할 수 있습니다. 아래 예제에서는 지연 시간을 5분으로 설정합니다.

```http
PUT /_all/settings
{
	"settings": {
    "index.unassigned.node_left.delayed_timeout": "5m"
	}
}
```

자세한 내용은 [노드가 벗어날 때 할당 지연](https://www.elastic.co/guide/en/elasticsearch/reference/current/delayed-allocation.html)을 참조하세요.

중단에 취약한 네트워크에서 다른 노드에 더 이상 액세스할 수 없는 시기를 감지하도록 마스터를 구성하는 매개 변수를 수정할 수도 있습니다. 이러한 매개 변수는 Elasticsearch와 함께 제공된 [Zen Discovery](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html#modules-discovery-zen) 모듈의 일부이며 Elasticsearch.yml 파일에서 설정할 수 있습니다. 예를 들어 *discovery.zen.fd.ping.retries* 매개 변수는 실패한 것을 결정하기 전에 마스터 노드가 클러스터의 다른 노드로 ping을 실행하려고 하는 횟수를 지정합니다. 이 매개 변수의 기본값은 3이지만 다음과 같이 수정할 수 있습니다.

```yaml
discovery.zen.fd.ping_retries: 6
```

## 복구 제어

오류가 발생한 후 노드에 대한 연결이 복원되면 해당 노드의 모든 분할된 데이터베이스는 최신으로 전환되도록 복구되어야 합니다. 기본적으로 Elasticsearch는 다음 순서대로 분할된 데이터베이스를 복구합니다.

- 역방향으로 인덱스를 만든 날짜입니다. 새 인덱스는 이전 인덱스 전에 복구됩니다.

- 역방향으로 인덱스 이름입니다. 다른 항목보다 사전순으로 큰 이름을 가진 인덱스는 먼저 복원됩니다.

다른 항목보다 더 중요하지만 이러한 기준과 일치하지 않는 일부 인덱스의 경우 *index.priority* 속성을 설정하여 인덱스의 우선 순위를 재정의할 수 있습니다. 이 속성에 대해 높은 값을 가진 인덱스는 낮은 값을 가진 인덱스 전에 복구됩니다.

```http
PUT low_priority_index
{
	"settings": {
		"index.priority": 1
	}
}

PUT high_priority_index
{
	"settings": {
		"index.priority": 10
	}
}
```

자세한 내용은 [인덱스 복구 우선 순위 지정](https://www.elastic.co/guide/en/elasticsearch/reference/2.0/recovery-prioritization.html#recovery-prioritization)을 참조하세요.

*\_recovery* API를 사용하여 하나 이상의 인덱스에 대한 복구 프로세스를 모니터링할 수 있습니다.

```http
GET /high_priority_index/_recovery?pretty=true
```

자세한 내용은 [인덱스 복구](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-recovery.html#indices-recovery)를 참조하세요.

> [AZURE.NOTE] 복구를 필요로 하는 분할된 데이터베이스가 있는 클러스터는 모든 분할된 데이터베이스를 현재 사용할 수 없음을 나타내는 *노란색*의 상태가 됩니다. 모든 분할된 데이터베이스를 사용할 수 있는 경우 클러스터 상태는 *녹색*으로 전환되어야 합니다. *빨간색*의 상태인 클러스터는 하나 이상의 분할된 데이터베이스가 물리적으로 누락되었음을 나타냅니다. 백업에서 데이터를 복원해야 할 수 있습니다.

## 분할 브레인 방지 

노드 간 연결에 실패하는 경우 분할 브레인이 발생할 수 있습니다. 마스터 노드를 클러스터의 부분에 연결할 수 없는 경우 연결 가능 상태로 남아 있는 네트워크 세그먼트에서 선택이 발생하고 다른 노드는 마스터가 됩니다. 잘못 구성된 클러스터에서 클러스터의 각 부분은 데이터 불일치 또는 손상이 발생한 다른 마스터를 가질 수 있습니다. 이러한 현상을 *분할 브레인*이라고 합니다.

elasticsearch.yml 파일에서 검색 모듈의 *minimum\_master\_nodes* 속성을 구성하여 분할 브레인의 가능성을 줄일 수 있습니다. 이 속성은 마스터의 선택을 활성화하는 데 사용할 수 있어야 하는 노드의 수를 지정합니다. 다음 예에서는 2로 이 속성의 값을 설정합니다.

```yaml
discovery.zen.minimum_master_nodes: 2
```

이 값은 마스터 역할을 수행할 수 있는 노드 수의 가장 낮은 대부분으로 설정되어야 합니다. 예를 들어 클러스터에 3개의 마스터 노드가 있는 경우 *minimum\_master\_nodes*는 2로 설정되어야 하며, 5개의 마스터 노드가 있는 경우 *minimum\_master\_nodes*는 3으로 설정되어야 합니다. 이상적으로 홀수의 마스터 노드가 있어야 합니다.

**참고:** 동일한 클러스터에 여러 마스터 노드가 동시에 시작된 경우 분할 브레인이 발생할 수 있습니다. 이 경우는 드물지만 각 사이에 짧은 지연 시간(5초)으로 노드를 순차적으로 시작하여 방지할 수 있습니다.

## 롤링 업데이트 처리

노드에 소프트웨어 업그레이드를 직접 수행하는 경우(예: 최신 버전으로 마이그레이션 또는 패치 수행) 클러스터의 나머지 부분을 사용 가능하도록 유지하면서 오프라인 상태로 전환해야 하는 개별 노드에서 작업해야 합니다. 이 경우 다음 프로세스를 구현하는 것이 좋습니다.

선택된 마스터를 클러스터의 나머지 부분에서 누락된 노드를 통해 분할된 데이터베이스를 균형 조정하지 않도록 분할된 데이터베이스 재할당이 충분히 지연되어야 합니다. 기본적으로 분할된 데이터베이스 재할당은 1분 동안 지연되지만 노드를 더 오랫동안 사용할 수 없게 될 경우 기간을 늘릴 수 있습니다. 다음 예제에서는 5분으로 지연 시간을 늘립니다.

```http
PUT /_all/_settings
{
	"settings": {
		"index.unassigned.node_left.delayed_timeout": "5m"
	}
}
```

> [AZURE.IMPORTANT] 클러스터의 *cluster.routing.allocation.enable*을 *none*으로 설정하여 분할된 데이터베이스 재할당을 완전히 비활성화할 수도 있습니다. 그러나 인덱스 할당이 빨간색 상태의 클러스터로 실패할 수 있으므로 노드가 오프라인일 때 새 인덱스가 만들어질 경우 이 방법을 사용하면 안 됩니다.

노드에서 Elasticsearch가 유지 관리되도록 중지합니다. Elasticsearch가 서비스로 실행되는 경우 운영 체제 명령을 사용하여 제어된 방식으로 프로세스를 중단할 수 있습니다. 다음 예제에서는 Ubuntu에서 실행되는 단일 노드에서 Elasticsearch 서비스를 중지하는 방법을 보여 줍니다.

```bash
service elasticsearch stop
```

또는 노드에서 직접 Shutdown API를 사용할 수 있습니다.

```http
POST /_cluster/nodes/_local/_shutdown
```

노드에서 필요한 유지 관리를 수행한 다음 노드를 다시 시작하고 클러스터에 연결될 때까지 기다립니다.

분할된 데이터베이스 할당 재활성화:

```http
PUT /_cluster/settings
{
	"transient": {
		"cluster.routing.allocation.enable": "all"
	}
}
```

> [AZURE.NOTE] 둘 이상의 노드를 유지해야 하는 경우 분할된 데이터베이스 할당을 재활성화하기 전에 각 노드에 대해 2, 3 및 4단계를 반복합니다.

가능하면 이 프로세스 동안 새 데이터의 인덱싱을 중지합니다. 노드가 다시 온라인 상태로 전환되고 클러스터를 다시 연결할 때 복구 시간을 최소화하는 데 도움이 됩니다.

특히 Windows에서 Elasticsearch를 실행하는 경우 JVM과 같은 항목에 대한 자동화된 업데이트에 주의하십시오(이상적으로 이러한 항목에 대해 자동 업데이트 비활성화). Java 업데이트 에이전트는 Java의 최신 버전을 자동으로 다운로드할 수 있지만 업데이트를 적용하려면 Elasticsearch를 다시 시작해야 할 수 있습니다. Java 업데이트 에이전트가 구성된 방법에 따라 조정되지 않은 노드의 임시 손실이 발생할 수 있습니다. 호환성 문제를 일으킬 수 있는 다른 버전의 JVM을 실행하는 동일한 클러스터에 다른 Elasticsearch의 인스턴스를 유발할 수도 있습니다.

## Elasticsearch 복원력 및 복구 테스트 및 분석

이 섹션은 3개의 데이터 노드 및 3개의 마스터 노드를 구성하는 Elasticsearch 클러스터의 복원력 및 복구를 평가하도록 수행된 일련의 테스트를 설명합니다.

네 개의 시나리오가 테스트되었습니다.

1.  노드 오류 및 데이터 손실 없이 다시 시작합니다. 데이터 노드가 중지되고 5분 후 다시 시작됩니다. 추가 I/O가 분할된 데이터베이스 이동에 발생되지 않도록 Elasticsearch는 이 간격에 누락된 분할된 데이터베이스를 재할당하지 않도록 구성되었습니다. 노드가 다시 시작되면 복구 프로세스는 해당 노드의 분할된 데이터베이스를 최신으로 제공합니다.

2.  치명적인 데이터 손실로 노드 오류. 데이터 노드가 중지되고 치명적인 디스크 오류를 시뮬레이션하도록 이를 보유하고 있는 데이터가 지워집니다. 노드는 효과적으로 원래 노드의 대체로 역할을 하도록 다시 시작됩니다(5분 후). 복구 프로세스는 이 노드에 대한 누락된 데이터를 다시 빌드해야 하고 다른 노드에서 보유하는 분할된 데이터베이스를 재배치해야 합니다.

3.  노드 오류 및 데이터 손실이 없지만 분할된 데이터베이스 재할당으로 다시 시작합니다. 데이터 노드가 중지되고 보유하는 분할된 데이터베이스는 다른 노드로 재할당됩니다. 그런 다음 노드가 다시 시작되고 클러스터를 균형 조정하도록 더 많은 재할당이 발생합니다.

4.  롤링 업데이트. 클러스터의 각 노드는 중지되고 잠시 후에 다시 시작되어 소프트웨어 업데이트 후 다시 부팅되는 컴퓨터를 시뮬레이션합니다. 한 번에 하나의 노드만 중지됩니다. 노드가 중지된 동안 분할된 데이터베이스는 재할당되지 않습니다.

각 시나리오는 노드가 오프라인에서 수행되고 복구되는 동안 다양한 데이터 수집 작업, 집계 및 쿼리 필터링으로 구성된 동일한 워크로드가 적용되었습니다. 워크로드의 대량 삽입 작업은 각각 1000개의 문서를 저장했고 한 개의 인덱스에 대해 수행된 반면 집계 및 쿼리 필터는 수백 만 개의 문서를 포함하는 별도 인덱스를 사용했습니다. 이를 통해 쿼리의 성능이 대량 삽입에서 개별적으로 평가될 수 있도록 합니다. 각 인덱스는 다섯 개의 분할된 데이터베이스와 하나의 복제본으로 구성되어 있습니다.

다음 섹션은 노드가 오프라인 상태이거나 복구되는 동안 성능 저하를 알리는 이러한 테스트의 결과 및 보고된 오류를 요약합니다. 결과는 하나 이상의 노드가 누락된 지점을 강조 표시하고 시스템이 완벽하게 복구되고 오프라인에서 수행되는 노드 전에 있었던 성능의 비슷한 수준을 달성하는 데 걸리는 시간을 예측하는 그래픽으로 표시됩니다.

> [AZURE.NOTE] 이러한 테스트를 수행하는 데 사용된 테스트 장비는 온라인으로 제공됩니다. 이러한 도구를 조정 및 사용하여 사용자 고유의 클러스터 구성의 복원력 및 복구 가능성을 확인할 수 있습니다. 자세한 내용은 [자동화된 Elasticsearch 복원 기능 테스트 실행][]을 참조하세요.

## 노드 오류 및 데이터 손실 없이 다시 시작: 결과

<!-- TODO; reformat this pdf for display inline -->

이 테스트의 결과는 파일 [ElasticsearchRecoveryScenario1.pdf](https://github.com/mspnp/azure-guidance/blob/master/figures/Elasticsearch/ElasticsearchRecoveryScenario1.pdf)에 나와 있습니다. 그래프는 워크로드의 성능 프로필 및 클러스터의 각 노드에 대한 실제 리소스를 표시합니다. 그래프의 시작 부분은 일반적으로 약 20분 동안 실행하는 시스템을 보여 줍니다. 이 때 노드 0은 다시 시작되기 전에 5분 동안 종료됩니다. 추가 20분에 대한 통계가 설명됩니다. 시스템은 복구 및 안정화하는 데 약 10분이 걸립니다. 이는 다른 워크로드에 대한 트랜잭션 속도 및 응답 시간으로 설명됩니다.

다음 사항에 유의하세요.

- 테스트하는 동안 오류가 보고되지 않았습니다. 데이터 손실이 없으며 모든 작업이 성공적으로 완료되었습니다.

- 노드 0이 오프라인 상태였을 동안 세 가지 유형의 모든 작업(대량 삽입, 쿼리 집계 및 쿼리 필터)에 대한 트랜잭션 속도가 떨어졌고 평균 응답 시간은 증가했습니다.

- 복구 기간 동안 쿼리 집계 및 쿼리 필터 작업에 대한 트랜잭션 속도 및 응답 시간은 점진적으로 복원되었습니다. 대량 삽입에 대한 성능은 축소 전에 잠깐 동안 복구됐습니다. 그러나 대량 삽입에서 사용되는 인덱스를 증가하도록 유발하는 데이터 양 때문일 가능성이 있으며 이 작업에 대한 트랜잭션 속도는 노드 0이 오프라인에서 수행되기 전에도 느려지는 것을 볼 수 있습니다.

- 노드 0에 대한 CPU 사용률 그래프는 복구 단계 동안 축소된 작업을 보여 줍니다. 이는 복구 메커니즘으로 인해 발생한 증가된 디스크 및 네트워크 작업 때문입니다. 노드는 오프라인 상태 동안 누락한 모든 데이터를 불러오고 포함하는 분할된 데이터베이스를 업데이트해야 합니다.

- 인덱스에 대한 분할된 데이터베이스는 모든 노드에서 정확하게 동일하게 분산되지 않습니다. 총 20개의 분할된 데이터베이스를 만드는 각각 5개의 분할된 데이터베이스 및 하나의 복제본으로 구성된 두 개의 인덱스가 있습니다. 따라서 두 노드는 6개의 분할된 데이터베이스를 포함하는 반면 다른 둘은 각각 7개를 보유합니다. 이것은 CPU 사용률 그래프에서 초기 20분 동안 명백합니다. 노드 0은 다른 둘보다 덜 사용 중입니다. 복구가 완료된 후 노드 2가 부하가 더 적은 노드가 될 것이므로 일부 전환이 발생하는 것처럼 보입니다.

    
## 치명적인 데이터 손실로 노드 오류: 결과

<!-- TODO; reformat this pdf for display inline -->

이 테스트의 결과는 파일 [ElasticsearchRecoveryScenario2.pdf](https://github.com/mspnp/azure-guidance/blob/master/figures/Elasticsearch/ElasticsearchRecoveryScenario2.pdf)에 나와 있습니다. 첫 번째 테스트와 같이 그래프의 시작 부분은 일반적으로 약 20분 동안 실행하는 시스템을 보여 줍니다. 이 때 노드 0은 5분 동안 종료됩니다. 이 간격 동안 다시 시작되기 전에 치명적인 데이터 손실을 시뮬레이션하는 이 노드의 Elasticsearch 데이터가 제거됩니다. 테스트가 복원되기 전에 표시되는 성능 수준 전에 전체 복구는 12~15분이 걸리는 것처럼 표시됩니다.

다음 사항에 유의하세요.

- 테스트하는 동안 오류가 보고되지 않았습니다. 데이터 손실이 없으며 모든 작업이 성공적으로 완료되었습니다.

- 노드 0이 오프라인 상태였을 동안 세 가지 유형의 모든 작업(대량 삽입, 쿼리 집계 및 쿼리 필터)에 대한 트랜잭션 속도가 떨어졌고 평균 응답 시간은 증가했습니다. 이 시점에서 테스트의 성능 프로필은 첫 번째 시나리오와 비슷합니다. 시나리오는 동일하므로 이 시점에서 놀랍지 않습니다.

- 복구 기간 중 그림에 더 많은 변동성이 있지만 트랜잭션 속도 및 응답 시간은 복원됐습니다. 누락된 분할된 데이터베이스를 복원하도록 데이터를 제공하는 클러스터의 노드가 수행되는 추가 작업 때문일 가능성이 가장 큽니다. 이 추가 작업은 CPU 사용률, 디스크 작업 및 네트워크 작업 그래프에서 명백합니다.

- 노드 0 및 1에 대한 CPU 사용률 그래프는 복구 단계 중에 감소된 작업을 보여 줍니다. 이는 복구 프로세스에 의해 발생한 증가된 디스크 및 네트워크 작업 때문입니다. 첫 번째 시나리오에서 복구되는 중인 노드만이 이 동작을 표시하지만 이 시나리오에서 노드 0에 대한 대부분의 누락된 데이터가 노드 1에서 복원되는 중으로 보입니다.

- 노드 0에 대한 I/O 작업은 첫 번째 시나리오에 비해 실제로 줄어듭니다. 이는 기존 분할된 데이터베이스를 최신으로 전환하는 데 필요한 일련의 더 작은 I/O 요청이 아닌 전체 분할된 데이터베이스에 대한 단순한 데이터 복사의 I/O 효율성 때문일 수 있습니다.

- 세 개의 모든 노드에 대한 네트워크 작업은 데이터가 노드 간 전송되고 수신되므로 작업의 급증을 나타냅니다. 시나리오 1에서 노드 0만이 많은 네트워크 작업으로 표시했지만 이 작업은 장기간 동안 유지되는 것으로 보였습니다. 마찬가지로 이 차이는 분할된 데이터베이스를 복구할 때 수신한 일련의 더 작은 요청이 아닌 단일 요청으로 분할된 데이터베이스에 대한 전체 데이터 전송의 효율성 때문일 수 있습니다.

## 노드 오류 및 분할된 데이터베이스 재할당으로 다시 시작: 결과

<!-- TODO; reformat this pdf for display inline -->

파일 [ElasticsearchRecoveryScenario3.pdf](https://github.com/mspnp/azure-guidance/blob/master/figures/Elasticsearch/ElasticsearchRecoveryScenario3.pdf)는 이 테스트의 결과를 보여 줍니다. 첫 번째 테스트와 같이 그래프의 시작 부분은 일반적으로 약 20분 동안 실행하는 시스템을 보여 줍니다. 이 때 노드 0은 5분 동안 종료됩니다. 이 시점에서 Elasticsearch 클러스터는 누락된 분할된 데이터베이스를 다시 만들고 나머지 노드에서 분할된 데이터베이스의 균형을 다시 조정합니다. 5분 후 노드 0은 다시 온라인 상태가 되고 클러스터는 다시 한 번 분할된 데이터베이스의 균형을 다시 조정해야 합니다. 성능은 12-15분 후에 복원됩니다.

다음 사항에 유의하세요.

- 테스트하는 동안 오류가 보고되지 않았습니다. 데이터 손실이 없으며 모든 작업이 성공적으로 완료되었습니다.

- 이전 두 테스트에 비해 노드 0이 오프라인 상태였을 동안 세 가지 유형의 모든 작업(대량 삽입, 쿼리 집계 및 쿼리 필터)에 대한 트랜잭션 속도가 떨어졌고 평균 응답 시간은 크게 증가했습니다. 이는 누락된 분할된 데이터베이스를 다시 만드는 증가된 클러스터 작업 및 디스크에 대해 발생된 수치로 증명되는 클러스터의 균형 재조정 및 이 기간의 노드 1및 2에 대한 네트워크 작업 때문입니다.

- 노드 0이 다시 온라인 상태가 된 후 해당 기간 동안 트랜잭션 속도 및 응답 시간은 불안하게 유지됩니다.

- 노드 0에 대한 CPU 사용률 및 디스크 작업 그래프는 복구 단계 동안 매우 감소된 초기 작업을 보여 줍니다. 이 시점에서 이는 노드 0이 데이터를 제공하지 않기 때문입니다. 약 5분의 기간 후 노드는 네트워크, 디스크 및 CPU 작업에서 급격한 증가에 의해 표시된 작업으로 급증합니다. 클러스터의 노드 간에 분할된 데이터베이스 재배포로 인해 발생했을 가능성이 높습니다. 그런 다음 노드 0은 일반 작업을 표시합니다.
  
## 롤링 업데이트: 결과

<!-- TODO; reformat this pdf for display inline -->

파일 [ElasticsearchRecoveryScenario4.pdf](https://github.com/mspnp/azure-guidance/blob/master/figures/Elasticsearch/ElasticsearchRecoveryScenario4.pdf)에서 이 테스트의 결과는 각 노드가 오프라인으로 전환된 다음 연속적으로 다시 전환되는 방법을 보여 줍니다. 다음 노드가 순서대로 중지되는 시점에 다시 시작되기 전에 각 노드는 5분 동안 종료됩니다.

다음 사항에 유의하세요.

- 각 노드가 순환되는 동안 처리량과 응답 시간 측면에서 성능은 동일하게 유지됩니다.

- 디스크 작업은 온라인 상태로 전환되므로 짧은 시간 동안 증가하는 각 노드에 대해 증가합니다. 이는 노드가 다운되는 동안 발생한 모든 변경 내용을 롤포워드하는 복구 프로세스 때문일 가능성이 가장 큽니다.

- 노드가 오프라인 상태가 되면 네트워크 작업의 급증이 나머지 노드에서 발생합니다. 급증은 또한 노드가 다시 시작될 때 발생합니다.

- 마지막 노드가 재순환된 후 시스템은 상당한 변동성 기간으로 들어갑니다. 이는 모든 노드 간에 변경 내용을 동기화하고 모든 복제본 및 해당 분할된 데이터베이스가 일관되는지 확인해야 하는 복구 프로세스에 의해 발생할 가능성이 가장 큽니다. 한 지점에서 이 노력으로 인해 연속된 대량 삽입 작업은 제한 시간을 초과하고 실패할 수 있습니다. 각 경우를 보고한 오류는 다음과 같습니다.

```
Failure -- BulkDataInsertTest17(org.apache.jmeter.protocol.java.sampler.JUnitSampler$AnnotatedTestCase): java.lang.AssertionError: failure in bulk execution:
[1]: index [systwo], type [logs], id [AVEg0JwjRKxX_sVoNrte], message [UnavailableShardsException[[systwo][2] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@787cc3cd]]

```

후속 실험은 각 노드 순환 간 몇 분의 지연 도입이 이 오류를 제거하는 것을 보여 주었으므로 이는 여러 노드를 동시에 복원하려는 복구 프로세스와 수 천 개의 새 문서를 저장하려는 대량 삽입 작업 간의 경합으로 인한 것일 가능성이 컸습니다.


## 요약

수행된 테스트는 다음을 나타냈습니다.

- Elasticsearch는 클러스터에서 발생할 가능성이 있는 가장 일반적인 실패 모드에 대해 높은 복원력이 있습니다.

- Elasticsearch는 잘 디자인된 클러스터가 노드에서 치명적인 데이터 손실이 발생할 경우 신속하게 복구할 수 있습니다. 사용 후 삭제 저장소에 데이터를 저장하도록 Elasticsearch를 구성한 경우 발생할 수 있으며 다시 시작한 후에 노드는 이어서 다시 프로비전됩니다. 이러한 결과는 이 경우에도 사용 후 삭제 저장소 사용이 이 저장소의 클래스가 제공하는 성능 이점이 더 많다는 것을 보여 줍니다.

- 처음 세 개의 시나리오에서 오류가 오프라인으로 전환되고 복구되는 동안 동시 대량 삽입, 집계 및 쿼리 필터 작업에서 오류가 발생하지 않았습니다.

- 시나리오 4만이 잠재적 데이터 손실을 나타냈고 이 손실은 추가되는 새 데이터에만 영향을 줬습니다. 보고된 오류의 유형은 일시적일 가능성이 크므로 실패한 삽입 작업을 재시도하여 이 가능성을 완화하도록 데이터 수집을 수행하는 응용 프로그램에서 바람직합니다.

- 또한 테스트 4의 결과는 클러스터에서 계획된 노드의 유지 관리를 수행하는 경우 한 노드와 다음 노드 순환 간에 몇 분 정도를 허용하는 경우 성능이 향상되는 것을 보여 줍니다. 계획되지 않은 경우(예: 운영 체제 업데이트 수행 후 데이터센터에서 노드 재순환)에서 노드가 종료되고 다시 시작되는 방법 및 시기를 제어하기가 더 어렵습니다. 연속적인 노드 작동 중단 후 Elasticsearch가 클러스터의 상태를 복구하려고 할 때 발생하는 경합으로 시간 제한과 오류가 발생할 수 있습니다.

[가상 컴퓨터의 가용성 관리]: ../articles/virtual-machines/virtual-machines-manage-availability.md
[자동화된 Elasticsearch 복원 기능 테스트 실행]: guidance-elasticsearch-running-automated-resilience-tests.md

<!---HONumber=AcomDC_0224_2016-->