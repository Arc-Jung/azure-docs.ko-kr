<properties
   pageTitle="Azure에서 Elasticsearch 실행 | Microsoft Azure"
   description="Azure에서 Elasticsearch의 설치, 구성 및 실행 방법"
   services=""
   documentationCenter="na"
   authors="mabsimms"
   manager="marksou"
   editor=""
   tags=""/>

<tags
   ms.service="guidance"
   ms.devlang="na"
   ms.topic="article"
   ms.tgt_pltfrm="na"
   ms.workload="na"
   ms.date="02/18/2016"
   ms.author="masimms"/>

# Azure에서 Elasticsearch 실행

이 문서는 [시리즈의 일부](guidance-elasticsearch.md)입니다.

## 개요

Elasticsearch은 확장성이 뛰어난 오픈 소스 검색 엔진 및 데이터베이스로서, 빅데이터 집합에 저장된 정보를 신속하게 분석 및 검색해야 할 때 유용합니다. 일반적인 시나리오는 다음과 같습니다.

- **대규모 무료 텍스트 검색**, 여기에서 검색 용어의 조합과 일치하는 문서를 신속하게 배치 및 검색할 수 있습니다.

- **이벤트 로깅**, 여기에서 정보는 다양한 소스에서 도착할 수 있습니다. 이벤트 체인이 특정 결론을 얻은 방법을 식별하기 위해 데이터를 분석해야 합니다.

- **원격 장치 및 기타 소스에서 캡처한 데이터 저장**. 데이터는 다양한 정보를 포함할 수 있지만 일반적인 요구 사항은 연산자가 전체 시스템의 상태를 이해하도록 일련의 대시보드에 이 정보를 제공하는 것입니다. 응용 프로그램은 정보를 사용하여 결과적으로 수행해야 하는 데이터 및 비즈니스 작업의 흐름에 대한 신속한 결정을 내릴 수도 있습니다.

- **재고 관리**, 여기에서 상품이 판매되면 재고 변동이 기록됩니다. 비즈니스 시스템은 이 정보를 사용하여 제품 수준이 낮은 경우 사용자에게 재고 수준을 보고하고 재고를 다시 정렬할 수 있습니다. 분석가는 트렌드에 대한 데이터를 검사하여 어떤 제품이 어떤 상황에서 잘 판매되는지 확인할 수 있습니다.

- **재무** 분석, 여기에서 시장 정보가 거의 실시간으로 도착합니다. 매입/매도 의사 결정을 도와주는 데 사용할 수 있는 다양한 금융 상품의 최신 성능을 나타내는 대시보드를 생성할 수 있습니다.

이 문서는 Elasticsearch의 일반적인 구조에 대해 간략히 소개하고 Azure를 사용하여 Elasticsearch 클러스터를 구현하는 방법을 설명합니다. 문서는 Elasticsearch 클러스터를 배포하고 다양한 기능 성능 및 시스템 관리 요구 사항에 집중하고 요구 사항이 선택한 구성 및 토폴로지를 구동하는 방법을 고려하기 위한 모범 사례에 중점을 둡니다.

> [AZURE.NOTE] 이 참고 자료에서는 [Elasticsearch][]의 기본 사용에 어느 정도 익숙하다고 가정합니다.

## Elasticsearch의 구조

Elasticsearch는 검색 엔진으로 작동하도록 고도로 최적화된 문서 데이터베이스입니다. 문서는 JSON 형식으로 직렬화됩니다. 세부 정보는 보기에서 추상화되더라도 데이터는 [Apache Lucene][]을 사용하여 구현된 인덱스에 보관되며 Elasticsearch를 사용하기 위해 Lucene을 완전히 이해할 필요는 없습니다.

### 클러스터, 노드, 인덱스 및 분할

Elasticsearch는 여러 노드에 데이터를 분산하는 분할 및 고가용성을 제공하는 복제를 사용하는 클러스터된 아키텍처를 구현합니다.

문서는 인덱스에 저장됩니다. 인덱스 안에서 고유하게 문서를 식별하는 데 사용할 문서 안의 필드를 사용자가 지정하거나, 시스템이 자동으로 키 및 값을 생성할 수 있습니다. 인덱스는 물리적으로 문서를 구성하는 데 사용되며 문서를 찾기 위한 기본 방법입니다. 또한 Elasticsearch는 빠른 검색을 구현하고 컬렉션 내에서 분석을 수행하기 위해, 나머지 필드에서 반전된 인덱스 역할을 수행하는 추가적인 구조 집합을 자동으로 만듭니다.

인덱스는 분할된 데이터베이스 집합을 구성합니다. 문서는 인덱스 키 값과 인덱스 내의 분할된 데이터베이스 수를 기초로 해싱 메커니즘을 통해 분할된 데이터베이스 간에 균등하게 분산됩니다. 즉 문서가 분할된 데이터베이스에 할당되면 인덱스 키가 변경되지 않는 한 해당 분할된 데이터베이스에서 이동하지 않습니다. Elasticsearch는 클러스터 내의 모든 사용 가능한 데이터 노드 간에 분할된 데이터베이스를 배포합니다. 단일 노드가 최초에는 동일한 인덱스에 속한 하나 이상의 분할된 데이터베이스를 담을 수 있지만, 새 노드가 클러스터에 추가되면 Elasticsearch가 시스템에서의 부하 분산을 위해 분할된 데이터베이스를 재할당합니다. 노드가 제거될 때도 동일한 재분산이 적용됩니다.

인덱스는 복제할 수 있습니다. 이 경우 해당 인덱스에서 각각의 분할된 데이터베이스가 복사됩니다. Elasticsearch는 인덱스에 대해 원래의 분할된 데이터베이스("기본 분할된 데이터베이스"라고 함)와 그 복제본이 항상 서로 다른 노드에 위치하도록 합니다.

> [AZURE.NOTE] 인덱스를 만든 후에는 복제본은 추가할 수 있지만 인덱스의 분할된 데이터베이스 수는 쉽게 변경할 수 없습니다.

문서를 추가하거나 수정하는 경우 모든 쓰기 작업이 기본 분할된 데이터베이스에서 수행된 다음 각각의 복제본에서 수행됩니다. 기본적으로 이 프로세스는 일관성 유지를 위해 동기로 수행됩니다. Elasticsearch는 데이터를 쓸 때 버전 관리에 낙관적 동시성을 적용합니다. 읽기 작업은 기본 공유된 데이터베이스나 그 복제본 중 하나를 통해 처리될 수 있습니다.

아래 그림은 3개 노드로 구성된 Elasticsearch 클러스터의 핵심 측면을 보여줍니다. 각각의 분할된 데이터베이스마다 2개의 기본 분할된 데이터베이스로 구성된(모두 6개의 분할된 데이터베이스) 인덱스를 만들었습니다.

![](media/guidance-elasticsearch/general-cluster1.png)

*기본 노드 2개와 복제본의 2개 집합으로 구성된 간단한 Elasticsearch 클러스터*

이 클러스터에서 기본 분할된 데이터베이스 1과 기본 분할된 데이터베이스 2는 서로 간의 부하 분산을 위해 별도의 노드에 위치합니다. 복제본도 마찬가지로 배포됩니다. 한 노드가 실패할 경우에 시스템의 지속적인 작동을 위해 필요한 충분한 정보가 나머지 노드에 있습니다. 필요 시 해당 기본 분할된 데이터베이스가 사용할 수 없게 될 경우 Elasticsearch가 분할된 데이터베이스 복제본을 기본 분할된 데이터베이스로 승격시킵니다. 노드가 실행을 시작하면 새 클러스터를 시작하거나(클러스터 내 최초 노드인 경우) 기존 클러스터에 연결할 수 있습니다. 노드가 속해 있는 클러스터는 `cluster.name` elasticsearch.yml 파일의 설정에 따라 결정됩니다.

### 노드 역할

Elasticsearch 클러스터의 노드는 다음과 같은 역할을 수행할 수 있습니다.

- 인덱스 데이터를 포함하는 하나 이상의 분할된 데이터베이스를 포함할 수 있는 **데이터 노드**.

- 인덱스 데이터를 보유하지 않지만 클라이언트 응용 프로그램이 적합한 데이터 노드로 보내는 요청을 처리하는 **클라이언트 노드**.
 
- 인덱스 데이터를 보유하지 않지만 유지 관리, 클러스터 관련 라우팅 정보 배포(어떤 노드가 어떤 분할된 데이터베이스를 포함하는지에 대한 목록), 사용 가능한 노드 결정, 노드가 나타나고 사라질 때 분할된 데이터베이스 재할당, 노드 실패 후 복구 조정 등의 클러스터 관리 작업을 수행하는 **마스터 노드**. 여러 노드를 마스터로 구성할 수 있지만 실제로는 하나의 노드만 마스터 기능을 수행하도록 선택됩니다. 이 노드가 실패하면 다른 선택이 수행되며 다른 적격 마스터 노드 중 하나가 선택되어 임무를 이어 받습니다.

기본적으로 Elasticsearch 노드는 (개발 및 개념 증명 목적으로 단일 컴퓨터에서 완전한 작업 클러스터를 구축할 수 있도록 하기 위해) 3가지 역할 모두를 수행하지만, 다음과 같이 *elasticsearch.yml* 파일에서 *node.data* 및 *node.master* 설정을 통해 그 작업을 변경할 수 있습니다.

```yaml
# Configuration for a data node
node.data: true
node.master: false
```

```yaml
# Configuration for a client node
node.data: false
node.master: false
```

```yaml
# Configuration for a master node
node.data: false
node.master: true
```

> [AZURE.NOTE] 선택된 마스터 노드는 클러스터의 상태 유지를 위해 중요합니다. 다른 노드는 해당 노드가 계속 사용 가능한지 확인하기 위해 정기적으로 Ping을 수행합니다. 선택 된 마스터 노드가 데이터 노드 역할도 하는 경우 이 노드가 사용 중이어서 해당 Ping에 답하지 못할 수 있습니다. 이 상황에서는 마스터가 실패 상태로 간주되며 다른 마스터 노드 중 하나가 그 대신 선택됩니다.
> 
> 원래의 마스터가 실제로는 계속 사용 가능할 경우 마스터 노드가 2개 선택된 클러스터를 초래할 수 있어, 데이터 손상과 기타 문제로 이어지는 "분할 브레인" 문제가 발생합니다.
> 
> [Azure의 Elasticsearch에서 복원력 및 복구 구성][] 문서에서는 이런 상황이 발생할 가능성을 줄이는 클러스터 구성 방법에 대해 설명합니다. 그러나, 궁극적으로 대규모 클러스터에는 데이터 관리를 담당하지 않는 전담 마스터 노드를 사용하는 것이 좋습니다.

클러스터의 노드는 클러스터 내 타 노드와([가십][]을 통해) 자신이 포함하는 분할된 데이터베이스에 대한 정보를 공유합니다. 데이터를 저장하고 가져오는 클라이언트 응용 프로그램은 클러스터 내 아무 노드에나 연결할 수 있고 요청은 정확한 노드로 투명하게 라우팅됩니다. 클라이언트 응용 프로그램이 클러스터로부터 데이터를 요청하면 이 요청을 최초로 수신한 노드가 작업을 전달하고, 데이터를 가져오기 위해 각각의 관련 노드와 통신한 다음 클라이언트 응용 프로그램으로 반환하기 전에 결과를 집계할 책임이 있습니다.

클라이언트 노드를 사용하여 요청을 처리하면 데이터 노드의 이런 분산/수집 작업 부담을 덜어주어 데이터 서비스 작업에 시간을 할애할 수 있도록 합니다. 데이터 노드에 대해 HTTP 전송을 비활성화하면 클라이언트 응용 프로그램이 실수로 데이터 노드와 통신하는(그 결과 클라이언트 노드 역할을 하게 함) 것을 방지할 수 있습니다.

```yaml
http.enabled: false
```

데이터 노드는 Elasticsearch 전송 모듈(TCP 소켓을 사용하여 노드 간을 직접 연결)을 사용하여 동일한 네트워크의 다른 데이터 노드, 클라이언트 노드 및 전담 마스터 노드와 계속 통신할 수 있지만 클라이언트 응용 프로그램은 HTTP를 통해서만 클라이언트 노드에 연결할 수 있습니다. 아래 그림은 Elasticsearch 클러스터에서 전담 마스터, 클라이언트 및 데이터 노드가 혼합 구성된 토폴로지를 보여줍니다.

![](media/guidance-elasticsearch/general-cluster2.png)

*다양한 유형의 노드를 표시하는 Elasticsearch 클러스터*

### 클라이언트 노드 사용의 비용과 이점

응용 프로그램이 Elasticsearch 클러스터에 쿼리를 제출하면 응용 프로그램이 연결한 노드가 쿼리 프로세스의 전달을 담당합니다. 해당 노드가 요청을 각 데이터 노드에 전달하고 결과를 취합하여 누적된 정보를 응용 프로그램에 반환합니다. 쿼리에 집계 및 기타 연산이 관련된 경우, 응용 프로그램이 연결하는 노드가 각각의 다른 노드에서 데이터를 검색한 후 필요한 연산을 수행합니다. 이 분산/수집 프로세스는 상당한 처리 및 메모리 리소스를 사용할 수 있습니다.

전담 클라이언트 노드를 사용하여 이러한 작업을 수행하게 하면 데이터 노드가 데이터 관리 및 보관에 주력할 수 있습니다. 이 때문에 복잡한 쿼리와 집계가 필요한 여러 상황에서는 전담 클라이언트 노드를 사용하는 것이 큰 장점을 가져올 수 있습니다. 그러나 전담 클라이언트 노드의 영향은 시나리오, 워크로드, 클러스터 크기에 따라 다릅니다.

예를 들어 데이터를 저장할 때 추가 네트워크 "홉"으로 인해 클라이언트 노드를 사용하면 데이터 수집 워크로드가 덜 효율적일 수 있습니다. 분할된 데이터베이스가 6개인 3노드 클러스터에서 모든 환경 인수와 노드 부하가 동일할 때 시스템에 전담 클라이언트 노드가 구성되지 않은 경우, 데이터를 저장하거나 수정하는 응용 프로그램이 가장 적합한 분할된 데이터베이스에 연결할 가능성은 1/3이 되며 1/3의 경우에서 추가적인 네트워크 점프를 수행할 필요가 없어집니다.

반면에 단일 노드가 이러한 작업이 수행하는 각각의 분산/집계 작업 세트를 담당하므로 복잡한 집계를 수행하는 워크로드에서는 전담 클라이언트를 사용하는 것이 유익할 수 있습니다. 혼합 워크로드 환경에서는 특정 워크로드에 대한 클라이언트 노드 사용의 영향을 평가하기 위해 성능 테스트를 실행할 대비가 필요합니다.

> [AZURE.NOTE] 이 튜닝 프로세스에 대한 자세한 내용은 [Azure에서 Elasticsearch에 대한 데이터 집계 및 쿼리 성능 튜닝][] 문서를 참조합니다.

### 클러스터에 연결

Elasticsearch는 클라이언트 응용 프로그램를 구축하고 클러스터에 요청을 보내기 위한 여러 REST API를 공개하고 있습니다. .NET Framework를 사용하여 응용 프로그램을 개발하는 경우 [Elasticsearch.Net & NEST][] 등 두 가지 고급 API를 사용할 수 있습니다.

Java를 사용하여 클라이언트 응용 프로그램을 구축하는 경우 [노드 클라이언트 API][]를 사용하여 클라이언트 노드를 동적으로 만들어 클러스터에 추가할 수 있습니다. 시스템이 상대적으로 적은 수의 장기 연결을 사용할 경우 클라이언트 노드를 동적으로 만드는 것이 편리합니다. 노드 API를 사용하여 만든 클라이언트 노드는 클러스터 라우팅 맵과 함께 마스터 노드에서 제공합니다(어떤 노드가 어떤 분할된 데이터베이스를 포함하는지에 대한 상세 정보). 이 정보를 통해 Java 응용 프로그램이 데이터를 인덱싱 또는 쿼리할 때 적합한 노드에 직접 연결하여 타 API 사용 시에 필요할 수 있는 홉 수를 줄일 수 있습니다.

이 방법에서는 클라이언트 노드를 클러스터 안에 등록하는 데 따른 오버헤드 발생이라는 단점이 있습니다. 대규모 클라이언트 노드가 신속하게 나타났다 사라질 경우 클러스터 라우팅 맵의 유지 관리 및 배포가 미치는 영향이 막대할 수 있습니다.

**연결 부하 분산**

Elasticsearch에서는 연결 부하 분산의 구현에 몇 가지 메커니즘을 사용합니다. 다음 목록에서는 몇 가지 일반적인 접근 방식을 요약합니다.

**클라이언트 기반 부하 분산**: Elasticsearch.Net 또는 NEST API를 사용하여 클라이언트 응용 프로그램을 구축하는 경우 연결 풀을 사용하여 노드 간에 연결 요청을 라운드 로빈 방식으로 처리하면 외부 부하 분산 장치 없이 요청의 부하를 분산할 수 있습니다. 다음 코드 스니펫은 3개 노드의 주소로 구성된 *ElasticsearchClient* 개체를 만드는 방법을 보여줍니다. 클라이언트 응용 프로그램의 요청은 이 노드 간에 배포됩니다.

```csharp
// C#
var node1 = new Uri("http://node1.example.com:9200");
var node2 = new Uri("http://node2.example.com:9200");
var node3 = new Uri("http://node3.example.com:9200");

var connectionPool = new SniffingConnectionPool(new[] {node1, node2, node3});
var config = new ConnectionConfiguration(connectionPool);
var client = new ElasticsearchClient(config);
```

> [AZURE.NOTE] 유사한 기능을 [전송 클라이언트 API][]를 통해 Java 응용 프로그램에 사용할 수 있습니다.

**서버 기반 부하 분산**: 별도의 부하 분산 장치를 사용하여 노드에 요청을 배포할 수 있습니다. 이 방법은 주소 투명성이라는 장점이 있습니다. 즉 클라이언트 응용 프로그램을 각 노드의 상세 정보로 구성할 필요가 없으므로 클라이언트 노드 수정 없이 더 간단하게 노드를 추가, 제거 또는 재할당할 수 있습니다. 아래 그림은 클라이언트 노드를 사용하지 않을 경우 동일한 방법으로 데이터 노드에 직접 연결할 수는 있지만 부하 분산 장치를 사용하여 클라이언트 노드 집합에 요청을 전달하는 구성을 보여줍니다.

![](media/guidance-elasticsearch/general-clientappinstances.png)

*Azure Load Balancer를 통해 Elasticsearch 클러스터에 연결하는 클라이언트 응용 프로그램 인스턴스*

> [AZURE.NOTE] [Azure Load Balancer][]를 사용하여 클러스터를 공용 인터넷에 공개하거나, 클라이언트 응용 프로그램과 클러스터가 전적으로 동일한 사설 가상 네트워크(VNET)에 포함된 경우 [내부 부하 분산 장치][]를 사용할 수 있습니다.

**사용자 지정 부하 분산**: Azure 부하 분산 장치 대신 [nginx][]를 역방향 프록시 서버로 사용할 수 있습니다. Nginx는 클라이언트의 IP 주소에 따라 라운드 로빈, 최소 연결(현재 연결 수가 가장 적은 대상에 요청 전달), 해시 등 몇 가지 부하 분산 방법을 제공합니다.

> [AZURE.NOTE] Azure VM으로 nginx 서버를 배포할 수 있습니다. 가용성을 유지하려면 동일한 Azure 가용성 집합에 두 개 이상의 nginx 서버를 만들어야 합니다.

부하 분산을 사용할지 여부와 사용할 구현 방법을 선택할 때는 다음 사항을 고려해야 합니다.

- 모든 응용 프로그램 인스턴스에 대한 모든 요청을 처리하기 위해 동일한 노드에 연결하면 해당 노드가 병목이 될 수 있습니다. 사용 가능한 스레드 수가 소진되므로 요청이 대기 상태가 되고, 큐 길이가 과도하면 요청을 거부할 수 있습니다(많은 사용자에게 배포될 수 있는 응용 프로그램 코드에서 단일 노드의 연결 상세 정보를 하드 코딩하지 않음).

- Elasticsearch.Net, NEST 및 전송 클라이언트 API의 라운드 로빈 메커니즘은 연결 풀에서 다음으로 사용 가능한 노드에 대해 연결을 재시도함으로써 실패한 연결 요청을 처리합니다. 풀에서 응답하지 않는 노드에 대한 연결은 일시 *데드* 상태로 표시될 수 있습니다. 나중에 다시 활성 상태가 될 수 있고, 풀이 이 노드에 Ping을 수행하여 활성화 여부를 판단할 수 있습니다.

- Azure 부하 분산 장치는 여러 요인(클라이언트 IP 주소, 클라이언트 포트, 클라이언트 포트, 대상 IP 주소, 대상 포트, 프로토콜 유형)을 기초로 노드에 요청을 투명하게 리디렉션할 수 있습니다. 이 전략에서는 특정 컴퓨터에서 실행되는 클라이언트 응용 프로그램의 인스턴스가 동일한 Elasticsearch 노드로 전달될 가능성이 높습니다. 부하 분산 장치의 프로브 구성에 따라 Elasticsearch 서비스가 이 노드에서는 실패했지만 VM 자체는 계속 실행되는 경우, 타 클라이언트 인스턴스에 의한 타 노드 연결은 성공할 수 있지만 이 노드에 대한 모든 연결은 시간 초과 상태가 됩니다.

- Azure 부하 분산 장치는 부하 분산 장치가 수행한 상태 프로브 요청에 적절하게 답할 수 없는 경우 순환 순서에서 노드를 제외하도록 구성할 수 있습니다.

### 노드 검색

Elasticsearch는 피어-투-피어 통신을 기반으로 하므로 클러스터 내 타 노드 검색이 노드 수명 주기에서 중요한 부분입니다. 노드 검색을 통해 새 데이터 노드가 동적으로 클러스터에 추가되므로 클러스터의 투명한 규모 확장이 가능합니다. 또한 데이터 노드가 타 노드로부터의 통신 요청에 응답하지 못할 경우 마스터 노드가 이 데이터 노드가 실패했다고 판단하고, 필요한 조치를 통해 다른 운영 데이터 노드를 담았던 분할된 데이터베이스를 재할당할 수 있습니다.

Elasticsearch 노드 검색은 검색 모듈을 사용하여 처리됩니다. 검색 모듈에는 다른 검색 메커니즘을 사용하도록 전환할 수 있는 플러그인이 있습니다. 기본 검색 모듈([Zen][])에서는 노드가 Ping 요청을 실행하여 동일한 네트워크의 타 노드를 검색합니다. 다른 노드가 응답하면 가십을 통해 정보를 교환합니다. 그런 다음 마스터 노드가 분할된 데이터베이스를 새 노드에 배포하고(데이터 노드인 경우) 클러스터를 다시 분산합니다.

Zen 검색 모듈도 노드 실패 탐지를 위한 마스터 선택 프로세스와 프로토콜을 처리합니다.

Elasticsearch 버전 2.0 이전에 Zen 검색 모듈은 멀티캐스트 통신을 통해 노드가 서로 연결할 수 있도록 했습니다. 이 방법에서는 새 노드를 클러스터에 도입하기가 매우 간편했으나, 동일한 네트워크에 다른 Elasticsearch 설치가 우연히 동일한 클러스터 이름을 사용할 경우 새 설치를 동일한 클러스터의 일부로 간주하고 분할된 데이터베이스가 이 설치의 노드로 전달될 수 있어 보안 문제의 우려가 있었습니다.

또한 Azure 가상 컴퓨터(VM) 형태로 Elasticsearch 노드를 실행할 경우 멀티캐스트 메시지가 지원되지 않습니다. 따라서 Zen 검색이 유니캐스트 메시지를 사용하도록 구성하고 elasticsearch.yml 구성 파일에서 유효한 연결 노드 목록을 제공해야 합니다.

> [AZURE.NOTE] Elasticsearch 2.0 이상에서는 멀티캐스트를 기본 검색 메커니즘으로 사용하지 않습니다.

Azure VNET 내에 Elasticsearch 클러스터를 호스팅하는 경우, 클러스터에서 각 VM에 부여한 사설 DHCP 할당 IP 주소가 할당된 상태를 유지하도록(고정) 지정할 수 있습니다. 이 고정 IP 주소를 사용하도록 Zen 검색 유니캐스트 메시징을 구성할 수 있습니다. 동적 IP 주소가 있는 VM을 사용할 경우 VM이 중지했다 다시 시작하면 새로운 IP 주소가 할당되어 검색이 어려울 수 있는 점에 유의해야 합니다. 이 시나리오를 처리하기 위해 Zen 검색 모듈을 [Azure 클라우드 플러그인][]과 교체할 수 있습니다. 이 플러그인은 Azure 구독 정보를 기초로 하는 Azure API를 통해 검색 메커니즘을 구현합니다.

> [AZURE.NOTE] 현재 버전의 Azure Cloud Plugin에서는 Elasticsearch 노드의 Java 키 저장소에 Azure 구독에 대한 관리 인증서를 설치하고, elasticsearch.yml 파일에서 이 키 저장소 액세스를 위한 위치와 자격 증명을 제공해야 합니다. 이 파일은 일반 텍스트 형태이므로 Elasticsearch 서비스를 실행하는 계정만 액세스할 수 있도록 하는 것이 매우 중요합니다.
> 
> 또한 이 방법은 Azure 리소스 관리자(ARM) 배포와 호환되지 않을 수 있습니다. 따라서 마스터 노드에 고정 IP 주소를 사용하고, 이 노드를 사용하여 클러스터 전체에서 Zen 검색 유니캐스트 메시징을 구현하는 것이 좋습니다. 다음 구성(동일한 데이터 노드에 대한 elasticsearch.yml 파일의 내용)에서 호스트 IP 주소는 클러스터의 마스터 노드를 참조합니다.

```yaml
discovery.zen.ping.multicast.enabled: false  
discovery.zen.ping.unicast.hosts: ["10.0.0.10","10.0.0.11","10.0.0.12"]
```

## 일반 시스템 지침

Elasticsearch는 단일 랩톱에서 최고급 사양의 서버에 이르기까지 다양한 컴퓨터에서 실행할 수 있습니다. 그러나 사용 가능한 메모리, 연산 능력, 고속 디스크 등의 리소스가 클수록 성능이 향상됩니다. 다음 섹션에서는 Elasticsearch를 실행하기 위한 기본 하드웨어 및 소프트웨어 요구 사항을 요약합니다.

### 메모리 요구 사항

Elasticsearch는 속도를 위해 데이터를 메모리 내에 저장하려 합니다. Azure에서 일반적인 엔터프라이즈 또는 중간 크기의 상용 배포를 호스팅하는 프로덕션 서버는 14GB ~ 28GB의 RAM(D3 또는 D4 VM)이 필요합니다. **메모리가 더 큰 노드를 만들기 보다는 더 많은 노드 간에 부하를 분산합니다**(실험에서 메모리가 더 많은 더 큰 노드를 사용할 경우 장애 시 복구 시간이 늘어날 수 있었음). 그러나 작은 노드가 매우 많은 클러스터를 만들면 가용성과 처리량은 늘어나지만 시스템 관리 및 유지 관리를 위한 업무도 늘어납니다.

**Elasticsearch 힙에 서버에서 사용 가능한 메모리의 50%를 할당합니다**. Linux를 사용할 경우 Elasticsearch 실행에 앞서 ES\_HEAP\_SIZE 환경 변수를 설정합니다. 또는 Windows 또는 Linux를 사용 하는 경우 Elasticseach를 시작할 때 `Xmx` 및 `Xms` 매개 변수에서 메모리 크기를 지정할 수 있습니다. 두 매개 변수를 모두 동일한 값으로 설정하면 런타임에서 JVM(Java Virtual Machine)이 힙 크기를 재조정하지 않게 됩니다. 그러나 **30GB 이상은 할당하지 않아야 합니다**. 운영 체제 파일 캐시에 나머지 메모리를 사용합니다.

> [AZURE.NOTE] Elasticsearch은 Lucene 라이브러리를 사용하여 인덱스를 만들고 관리합니다. Lucene 구조는 디스크 기반 형식을 사용하며 파일 시스템 캐시 안에 해당 구조를 캐시하여 성능을 크게 높입니다.

64비트 컴퓨터에서 Java의 최적 최대 힙 크기는 30GB보다 약간 큰 수준입니다. 이보다 크면 Java가 힙의 개체를 참조하기 위해 확장 메커니즘을 사용하도록 전환되므로 각 개체에 대한 메모리 요구 사항이 커지고 성능이 줄어듭니다.

힙 크기가 30GB 이상이면 기본 Java 가비지 수집기(동시 표시 및 스윕)도 차선의 성능을 제공할 수 있습니다. Elasticsearch와 Lucene은 기본값에 대해서만 테스트되었으므로 현재 다른 가비지 수집기로의 전환은 권장되지 않습니다.

주 메모리를 디스크로 스왑하면 성능에 심각한 영향을 미치므로 메모리를 과도하게 커밋하지 마십시오. 가능한 경우 스왑을 완전히 비활성화합니다(세부 정보는 운영 체제마다 다름). 이것이 불가능하다면 Elasticsearch 구성 파일 (elasticsearch.yml)에서 *mlockall* 설정을 다음과 같이 활성화합니다.

```yaml
bootstrap.mlockall: true
```

이 구성 설정은 JVM이 메모리를 잠그게 하며, 운영 체제에 의한 스왑을 방지합니다.

### 디스크 및 파일 시스템 요구 사항

분할된 데이터베이스를 저장하기 위해 프리미엄 저장소에서 지원하는 데이터 디스크를 사용합니다. 나중에 디스크를 더 추가할 수는 있지만 분할된 데이터베이스에서 예상되는 최대 데이터 크기를 담을 수 있게 디스크 크기를 지정해야 합니다. 한 노드에서 여러 디스크에 분할된 데이터베이스를 확장할 수 있습니다.

> [AZURE.NOTE] Elasticsearch는 LZ4 알고리즘을 사용하여 저장된 필드에 대한 데이터를 압축하며 Elasticsearch 2.0 이후에는 압축 형식을 변경할 수 있습니다. 압축 알고리즘을 *zip* 및 *gzip* 유틸리티에서 사용하는 수축으로 전환할 수 있습니다. 이 압축 기술은 리소스를 더 많이 사용하지만 아카이브된 로그 데이터와 같은 콜드 인덱스에는 사용을 고려해야 합니다. 이 방식은 인덱스 크기를 줄이는 데 도움이 됩니다.

클러스터의 모든 노드에서 디스크 레이아웃과 용량이 동일한 필요는 없습니다. 그러나 클러스터의 다른 노드에 비해 디스크 용량이 매우 큰 노드는 데이터를 더 많이 담게 되기 때문에 이 데이터를 처리하기 위해 더 높은 처리 능력이 필요합니다. 따라서 노드가 타 노드보다 "사용이 많아지며" 결과적으로 성능에 영향을 미칠 수 있습니다.

가능한 경우 RAID 0(스트라이핑)을 사용합니다. Elasticsearch가 자체 HA 솔루션을 복제본 형태로 제공하므로 패리티 및 미러링을 구현하는 다른 형태의 RAID는 필요하지 않습니다.

> [AZURE.NOTE] Elasticsearch 2.0.0 이전에도 *path.data* 구성 설정에서 여러 디렉터리를 지정하여 소프트웨어 수준에서의 스트라이핑 구현이 가능했습니다. Elasticsearch 2.0.0에서는 이러한 형태의 스트라이핑이 더 이상 지원되지 않습니다. 대신 여러 분할된 데이터베이스를 서로 다른 경로에 할당하며 단일 분할된 데이터베이스의 모든 파일은 동일한 경로에 기록됩니다. **스트라이핑이 필요할 경우 운영 체제나 하드웨어 수준에서 데이터를 스트라이핑해야 합니다**.

저장소 처리량을 최대화하려면 각 **VM은 전용 프리미엄 저장소 계정이 있어야 합니다**.

Lucene 라이브러리는 인덱스 데이터 저장에 많은 수의 파일을 사용할 수 있으며, Elasticsearch는 노드 간 및 클라이언트와의 통신을 위해 상당한 수의 소켓을 열 수 있습니다. 운영 체제가 적합한 수의 열린 파일 설명자를 지원하도록 구성되어야 합니다(충분한 메모리를 사용할 수 있는 경우 최대 64000). 많은 Linux 배포에서 기본 구성은 열린 파일 설명자 수를 1024로 제한하는데, 이는 지나치게 적습니다.

Elasticsearch는 메모리 매핑(mmap) IO와 Java New IO(NIO) I/O를 사용하여 데이터 파일 및 인덱스에 대한 동시 액세스를 최적화합니다. Linux를 사용할 경우 256K 메모리 맵 영역에 대한 공간에서 사용 가능한 가상 메모리가 충분하도록 운영 체제를 구성해야 합니다.

> [AZURE.NOTE] 많은 Linux 배포에서는 기본적으로 디스크에 데이터 쓰기를 정렬할 때 CFQ(Completely Fair Queuing) 스케줄러를 사용합니다. 이 스케줄러는 SSD에 최적화되어 있지 않습니다. SSD에 더 효과적인 NOOP 스케줄러 또는 Deadline 스케줄러를 사용하도록 운영 체제를 다시 구성하는 것이 좋습니다.

### CPU 요구 사항

Azure VM은 1~32개의 코어를 지원하는 다양한 CPU 구성에서 사용할 수 있습니다. 데이터 노드에서는 먼저 표준 DS 시리즈 VM에서 출발하여 DS3(4코어) 또는 D4(8코어) SKU를 선택하는 것이 좋습니다. DS3도 14GB RAM을 제공하며 DS4는 28GB를 포함합니다.

GS 시리즈(프리미엄 저장소용) 및 G 시리즈(표준 저장소용)는 대규모 집계 등 연산 집약적 워크로드에 더 유용할 수 있는 제온 E5 V3 프로세서를 사용합니다. 최신 정보는 [가상 컴퓨터 크기][]를 참조하세요.

### 네트워크 요구 사항

Elasticsearch에는 구현하는 클러스터의 크기와 변동성에 따라 1~10Gbps의 네트워크 대역폭이 필요합니다. Elasticsearch는 더 많은 노드가 클러스터에 추가되면 노드 간에 분할된 데이터베이스를 마이그레이션합니다. Elasticsearch는 모든 노드 간의 통신 시간이 대략적으로 비슷하다고 간주하며 해당 노드에서 보유한 분할된 데이터베이스의 상대적 위치는 고려하지 않습니다. 또한 복제는 분할된 데이터베이스 간에 상당한 네트워크 I/O를 초래할 수 있습니다. 따라서 **다른 지역에 있는 노드에 클러스터를 만들지 않는 것이 좋습니다**.

### 소프트웨어 요구 사항

Windows 또는 Linux에서 Elasticsearch를 실행할 수 있습니다. Elasticsearch 서비스는 Java jar 라이브러리로 배포되며 Elasticsearch 패키지에 포함된 다른 Java 라이브러리와 의존성이 있습니다. Elasticsearch를 실행하려면 Java 7(업데이트 55 이상) 또는 Java 8(업데이트 20 이상) JVM을 설치해야 합니다.

> [AZURE.NOTE] *Xmx* 및 *Xms* 이외의 메모리 매개 변수(Elasticsearch 엔진에 대한 명령줄 옵션 형태로 지정, [메모리 요구 사항][] 참조)는 기본 JVM 구성 설정을 수정하지 않습니다. Elasticsearch는 기본값을 사용하도록 설계되었으므로 이를 변경하면 Elasticsearch 성능 저하를 초래할 수 있습니다.

### Azure에서 Elasticsearch 배포

Elasticsearch의 단일 인스턴스 배포는 어렵지 않지만, 많은 노드를 만들고 각각 Elasticsearch를 설치 및 구성하는 작업은 상당한 시간이 소요되며 오류가 발생하기 쉽습니다. Azure VM에서 Elasticsearch 실행을 고려할 때는 두 가지 옵션으로 오류 가능성을 낮출 수 있습니다.

- [ARM(Azure Resource Manager) 템플릿](http://azure.microsoft.com/documentation/templates/elasticsearch/)을 사용하여 클러스터를 만듭니다. 이 템플릿은 완전히 매개 변수화되어 사용자가 노드를 구현하는 VM의 크기와 성능 계층, 사용할 디스크 수, 기타 공통 요소를 지정할 수 있습니다. 템플릿은 Windows Server 2012 또는 Ubuntu Linux 14.0.4 기반인 클러스터를 만들 수 있습니다.

- 자동화되거나 무인 모드로 실행될 수 있는 스크립트를 사용합니다. Elasticsearch 클러스터를 만들고 배포할 수 있는 스크립트는 [Azure 빠른 시작 템플릿][] 사이트에서 제공합니다.

## 클러스터 및 노드 크기 조정과 확장성 

Elasticsearch에서는 서로 다른 요구 사항과 규모 수준을 지원하도록 설계된 다양한 배포 토폴로지를 구현합니다. 이 섹션에서는 몇 가지 일반적인 토폴로지에 대해 논의하고 이 토폴로지를 기반으로 하는 클러스터 구현의 고려 사항에 대해 설명합니다.

### Elasticsearch 토폴로지

아래 그림은 Azure에 대한 Elasticsearch 토폴로지를 디자인하는 출발점을 설명합니다.

![](media/guidance-elasticsearch/general-startingpoint.png)

*Azure에서 Elasticsearch 클러스터를 만들기 위한 권장 출발점*

이 토폴로지는 클라이언트 노드 3개, 마스터 노드 3개와 함께 6개의 데이터 노드로 구성됩니다(하나의 마스터 노드만 선택하며 다른 두 노드는 마스터 노드 실패 시 선택 가능). 각 노드는 별도 VM으로 구현됩니다. Azure 웹 응용 프로그램은 부하 분산 장치를 통해 클라이언트 노드로 전달됩니다.

이 예제에서는 모든 노드와 웹 응용 프로그램이 동일한 Azure VNET에 상주하여 외부로부터 효과적으로 분리됩니다. 클러스터가 외부에서 사용할 수 있어야 할 경우(온-프레미스 클라이언트를 통합하는 하이브리드 솔루션의 일부 등) Azure Load Balancer를 사용하여 공용 IP 주소를 사용할 수 있지만 클러스터에 대한 무단 액세스를 방지하기 위해 추가적인 보안 예방 조치가 필요합니다.

선택적 "Jump Box"는 관리자에게만 제공되는 VM입니다. 이 VM은 Azure VNET과의 네트워크 연결뿐 아니라, 외부 네트워크로부터의 관리자 로그온을 허용하는 외부 접촉 네트워크 연결이 있습니다(이 로그온은 강력한 암호나 인증서를 사용하여 보호되어야 함). 관리자는 Jump Box에 로그온한 다음 클러스터 내 아무 노드에나 직접 연결할 수 있습니다.

또는 조직과 VNET 간의 사이트 간 VPN이나 [Express 경로][] 회로를 사용하여 VNET에 연결합니다. 이 메커니즘에서는 클러스터를 공용 인터넷에 노출시키지 않고도 클러스터에 대한 관리자 액세스가 가능합니다.

VM 가용성을 유지하기 위해 데이터 노드를 동일한 Azure 가용성 집합으로 그룹화합니다. 마찬가지로 클라이언트 노드는 다른 가용성 집합에 유지되며 마스터 노드는 제3의 가용성 집합에 저장됩니다.

이 토폴로지는 비교적 쉽게 확장됩니다. 즉 간단하게 적합한 유형의 노드를 더 추가하고 elasticsearch.yml 파일에서 동일한 클러스터 이름으로 구성하면 됩니다. 클라이언트 노드도 Azure 부하 분산 장치에 대한 백엔드 풀에 추가해야 합니다.

**클러스터 지리적 위치**

**여러 지역에 걸친 클러스터에 노드를 배포하면 안 됩니다. 노드 내 통신 성능에 영향을 미칠 수 있습니다**([네트워크 요구 사항][] 참조). 서로 다른 지역의 사용자에게 지리적으로 가깝게 데이터를 배치하려면 여러 클러스터를 만들어야 합니다. 이 상황에서 클러스터 동기화 방법(또는 동기화 여부)을 고려해야 합니다. 가능한 솔루션은 다음과 같습니다.

[내 노드][]는 여러 Elasticsearch 클러스터에 참여할 수 있고 모두를 하나의 대형 클러스터로 간주한다는 점을 제외하고는 클라이언트 노드와 유사합니다. 데이터는 계속 각 클러스터에서 로컬로 관리되지만(업데이트가 클러스터 경계를 넘어 전파되지 않음) 모든 데이터가 표시됩니다. 내 노드는 어느 클러스터에서나 문서를 쿼리, 생성 및 관리할 수 있습니다.

가장 큰 제약은 내 노드는 새 인덱스를 만드는 데 사용할 수 없고 인덱스 이름이 모든 클러스터에서 고유해야 한다는 점입니다. 따라서 내 노드에서 액세스 가능한 클러스터를 설계할 때는 인덱스의 이름 지정 방법을 고려하는 것이 중요합니다.

이 메커니즘을 사용하면 각 클러스터가 로컬 클라이언트 응용 프로그램에서 가장 액세스할 가능성이 높은 데이터를 포함하지만, 이러한 클라이언트가 여전히 원격 데이터 작업에액세스하여 수정할 수 있습니다. 대기 시간은 늘어날 수 있습니다. 아래 그림은 이 토폴로지의 예를 보여줍니다. 클러스터 1의 내 노드가 강조 표시되어 있습니다. 다이어그램에는 표시되지 않았지만 다른 클러스터도 내 노드가 있을 수 있습니다.

![](media/guidance-elasticsearch/general-tribenode.png)

*내 노드를 통해 여러 클러스터에 액세스하는 클라이언트 응용 프로그램*

이 예에서 클라이언트 응용 프로그램은 클러스터 1의 내 노드에 연결하지만(동일한 지역에 함께 배치) 이 노드는 다른 지역에 있을 수 있는 클러스터 2 및 클러스터 3에 액세스할 수 있도록 구성할 수 있습니다. 클라이언트 응용 프로그램은 아무 클러스터에서나 데이터를 검색 또는 수정하는 요청을 보낼 수 있습니다.

> [AZURE.NOTE] 내 노드에는 클러스터 연결을 위해 멀티캐스트 검색이 필요한데, 이로 인한 보안 우려가 있습니다. 자세한 내용은 [노드 검색][]을 참조하세요.

- 클러스터 간의 지리적 복제 구현. 이 접근 방식에서는 각 클러스터에서 수행한 변경이 거의 실시간으로 다른 데이터 센터의 클러스터로 전파됩니다. 이 기능을 지원하기 위해 [PubNub Changes 플러그인][] 등의 타사 플러그인을 Elasticsearch에 사용할 수 있습니다.

- [Elasticsearch 스냅숏 및 복원 모듈][] 사용. 데이터가 매우 느리게 움직이며 단일 클러스터에 의해서만 수정될 경우 스냅숏을 사용하여 데이터의 주기적 사본을 만들고 이러한 스냅숏을 다른 클러스터에 복원할 수 있습니다([Azure 클라우드 플러그인][]을 설치한 경우 Azure Blob 저장소에 스냅숏을 복원할 수 있음). 그러나 이 솔루션은 데이터가 급속히 변하거나 여러 클러스터에서 데이터가 변경되는 경우 적합하지 않습니다.

**소규모 토폴로지**

전담 마스터, 클라이언트 및 데이터 노드의 클러스터로 구성된 대규모 토폴로지가 모든 시나리오에 적합하지는 않습니다. 소규모 프로덕션이나 개발 시스템을 구축하는 경우 아래 그림에서 보여준 3개 노드 클러스터를 고려합니다.

클라이언트 응용 프로그램은 클러스터에서 사용 가능한 아무 데이터 노드에나 연결합니다. 클러스터는 이름이 P1-P3인 분할된 데이터베이스 3개와 이름이 R1-R3인 복제본을 포함합니다. 3개 노드를 사용하면 Elasticsearch가 분할된 데이터베이스와 복제본을 배포하여 단일 노드가 실패해도 데이터가 손실되지 않다록 합니다.

![](media/guidance-elasticsearch/general-threenodecluster.png)

*분할된 데이터베이스 3개와 복제본이 있는 3노드 클러스터*

독립 실행형 컴퓨터에서 개발 설치를 실행할 경우 마스터, 클라이언트 및 데이터 저장소 역할을 수행하는 단일 노드의 클러스터를 구성할 수 있습니다. 또는 Elasticsearch의 여러 인스턴스를 시작하여 동일한 컴퓨터에서 클러스터로 실행되는 여러 노드를 시작할 수 있습니다. 아래 그림은 예제를 보여줍니다.

![](media/guidance-elasticsearch/general-developmentconfiguration.png)

*동일한 컴퓨터에서 여러 Elasticsearch 노드를 실행하는 개발 구성*

개발 컴퓨터에 메모리가 매우 넉넉하고 여러 대의 빠른 디스크가 있지 않은 한 경합을 초래할 수 있으므로 프로덕션 환경에는 이 독립 실행형 구성이 권장되지 않습니다. 또한 고가용성을 보장할 수 없으므로 컴퓨터가 실패하면 모든 노드가 손실됩니다.

### 클러스터 및 데이터 노드 확장

Elasticsearch에서는 수직적 확장(더 크고 강력한 컴퓨터 사용) 및 수평적 확장(컴퓨터 간에 부하 분산) 등 2가지 확장이 가능합니다.

**Elasticsearch 데이터 노드의 수직 확장**

Azure VM을 사용하여 Elasticsearch 클러스터를 호스팅할 경우 각 노드가 VM에 해당합니다. 노드에 대한 수직 확장 제한은 상당 부분 VM의 SKU와, 개별 저장소 계정 및 Azure 구독에 적용되는 전반적인 제한 사항에 따라 좌우됩니다.

[Azure 구독 및 서비스 한도, 할당량 및 제약 조건](azure-subscription-service-limits/) 페이지에서는 이러한 제한을 상세히 설명하지만, Elasticsearch 클러스터 구축에 관한 한 다음 목록의 항목이 가장 크게 관련되어 있습니다.

- 각 저장소 계정은 20,000 IOPS로 제한됩니다. 클러스터의 각 VM은 전용(가급적 프리미엄) 저장소 계정을 활용해야 합니다.

- VNET의 데이터 노드 수입니다. Azure 리소스 관리자(ARM)를 사용하지 않는 경우 VNET당 최대 2048 VM 인스턴스로 제한됩니다. 대부분의 경우 이 값이 충분하지만 노드가 수천 개인 초대형 구성에서는 제약이 있을 수 있습니다.

- 지역별 구독당 저장소 계정 수. 각 지역에서 Azure 구독당 최대 100개의 저장소 계정을 만들 수 있습니다. 저장소 계정은 가상 디스크를 저장하는 데 사용되며 각 저장소 계정의 공간 제약은 500TB입니다.

- 구독당 코어 수. 기본 제한은 구독당 20개이지만 지원 티켓을 통해 제한 증가를 요청하여 10,000개 코어까지 늘릴 수 있습니다.

- VM 크기당 메모리 양. VM 크기가 작으면 사용 가능한 메모리 크기가 제한됩니다(D1 컴퓨터 3.5GB, D2 컴퓨터 7GB). 이러한 컴퓨터는 성능을 높이기 위해 Elasticsearch가 상당한 크기의 데이터를 캐시해야 하는 시나리오에는 적합하지 않을 수 있습니다(데이터 집계, 데이터 수집 중 대규모 문서 분석 등).

- VM 크기당 최대 디스크 수. 이 제한은 클러스터의 성능과 크기를 제한할 수 있습니다. 디스크 수가 더 적으면 데이터를 더 적게 저장하며 스트라이핑에 사용 가능한 디스크 수가 줄어 성능이 낮아질 수 있습니다.

- 가용성 집합당 업데이트 도메인/장애 도메인 수. ARM을 사용하여 VM을 만들 때 각 가용성 집합은 최대 3개의 장애 도메인과 20개의 업데이트 도메인에 할당할 수 있습니다. 이 제한은 업데이트 배포가 잦은 대규모 클러스터의 복원력에 영향을 미칠 수 있습니다.

또한 64GB 메모리 이상의 VM을 사용하도록 고려하지 않아야 합니다. [메모리 요구 사항][] 섹션에 설명된 대로 JVM에 대해 각 VM에서 30GB 이상의 RAM을 할당하지 않아야 하며 운영 체제가 I/O 버퍼링에 남은 메모리를 활용할 수 있도록 해야 합니다.

여러 저장소 계정에 걸친 클러스터의 VM에 가상 디스크를 배포할 때는 I/O 제한 가능성을 줄일 수 있게 항상 이러한 제한 사항을 염두에 두어야 합니다. 매우 큰 클러스터에서는 논리적 인프라를 다시 설계하고 별도의 기능적 파트션으로 분할해야 할 수 있습니다. 예를 들어, VNET 연결이 필요하여 복잡성이 커질 수는 있지만 구독 간에 클러스터를 분할해야 할 수 있습니다.

**Elasticsearch 클러스터의 수평 확장**

Elasticsearch 내에서 수평 확장 제한은 각 인덱스에 대해 정의한 분할된 데이터베이스 수에 따라 결정됩니다. 처음에는 많은 분할된 데이터베이스를 클러스터의 동일한 노드에 할당할 수 있지만 데이터 크기 증대에 따라 다른 노드를 추가하고 이 노드 간에 분할된 데이터베이스를 배포할 수 있습니다. 이론적으로 노드 수가 분할된 데이터베이스 수에 도달할 때만 시스템이 수평 확장을 중지합니다.

수직 확장과 마찬가지로 수평 확장에서도 구현 계획에서 다음과 같은 몇 가지 문제를 고려해야 합니다.

- Azure VNET에서 연결할 수 있는 최대 VM 수. 매우 큰 클러스터의 경우 수평 확장성을 제한할 수 있습니다. 이 제한을 회피하기 위해 여러 VNET에 걸친 노드의 클러스터를 만들 수 있지만 피어와 각 노드의 근접성이 떨어져 성능이 저하될 수 있습니다.

- VM 크기당 디스크의 수. 서로 다른 시리즈 및 SKU는 각기 다른 연결 디스크 수를 지원합니다. 또한 복원력과 복구 문제는 고려해야 하지만 VM에 포함된 임시 저장소를 사용하여 제한된 크기의 더 빠른 데이터 저장소를 제공할 수 있습니다(자세한 내용은 Elasticsearch 복원 및 복구 구성, 테스트 및 분석 문서 참조). D 시리즈, DS 시리즈, Dv2 시리즈 및 GS 시리즈 VM은 SSD를 임시 저장소로 사용합니다.

필요에 따라 Azure 규모 집합을 사용하여 VM을 시작 및 중지할 수 있습니다(자세한 내용은 [가상 컴퓨터 규모 집합에서 자동으로 컴퓨터 규모 조정][] 참조). 그러나 이 방법은 다음과 같은 이유로 Elasticsearch 클러스터에 적합하지 않을 수 있습니다.

- 이 방법은 상태 비저장 VM에 가장 적합합니다. Elasticsearch 클러스터에서 노드를 추가 또는 제거할 때마다 부하 분산을 위해 분할된 데이터베이스가 다시 할당되며, 이 프로세스에서 상당한 크기의 네트워크 트래픽과 디스크 I/O가 발생하고 데이터 수집 속도에 크게 영향을 미칠 수 있습니다. 더 많은 VM을 동적으로 시작하여 사용 가능하게 되는 추가적인 처리 및 메모리 리소스가 이 오버헤드를 감당할 만큼 가치가 있는지 평가해야 합니다.

- VM 시작은 즉시 발생하지 않으며 추가 VM이 사용 가능하거나 종료되기 몇 분 전에 발생할 수 있습니다. 이런 식의 확장은 지속적인 수요 변화를 처리하는 데만 사용되어야 합니다.

- 확장 후, 축소의 실질적인 필요 여부 Elasticsearch 클러스터에서 VM을 제거하는 것은 Elasticsearch가 해당 VM에 위치한 분할된 데이터베이스와 복제본을 복구하고 하나 이상의 나머지 노드에서 다시 만들어야 하기 때문에 리소스가 많이 필요한 프로세스가 될 수 잇습니다. 동시에 여러 VM을 제거하면 클러스터의 무결성이 손상되어 복구가 어려워질 수 있습니다. 또한 시간이 지나면서 많은 Elasticsearch 구현이 증대할 수 있습니다. 데이터는 규모가 줄지 않는 속성이 있습니다. 문서를 수동으로 삭제하고 시간이 만료되면 제거되도록 문서에 TTL(시간 제한)을 구성할 수도 있지만 대부분의 경우 이전에 할당한 공간을 새 문서나 수정된 문서가 신속히 재사용하게 됩니다. 문서가 제거 또는 변경되면 인덱스 내 조각호가 발생할 수 잇습니다. 이 경우 Elasticsearch HTTP [최적화][] API(Elasticsearch 2.0.0 이상) 또는 [강제 병합][] API (Elasticsearch 2.1.0 이상)를 사용하여 조각 모음을 수행할 수 있습니다.

### 인덱스에 대해 분할된 데이터베이스 수 결정

클러스터의 노드 수는 시간에 따라 달라질 수 있지만 인덱스의 분할된 데이터베이스 수는 인덱스를 만든 이후로 고정됩니다. 분할된 데이터베이스를 추가 또는 제거하려면 데이터를 다시 인덱싱하는 작업이 필요합니다. 즉 필요한 수의 분할된 데이터베이스로 새 인덱스를 만들고 기존 인덱스의 데이터를 새 인덱스로 복사하는 프로세스입니다(별칭을 사용하여 데이터가 다시 인덱싱되었다는 사실로부터 사용자를 격리할 수 있음, 자세한 내용은 [Azure에서 Elasticsearch에 대한 데이터 집계 및 쿼리 성능 튜닝][] 문서 참조). 따라서 클러스터에서 최초의 인덱스를 만들기 전에 필요할 것 같은 분할된 데이터베이스 수를 미리 결정하는 것이 중요합니다. 다음 단계를 통해 이 값을 정할 수 있습니다.

- 프로덕션 환경에 배포하려는 것과 동일한 하드웨어 구성을 사용하여 단일 노드 클러스터를 만듭니다.

- 프로덕션 환경에서 사용하려는 구조와 일치하는 인덱스를 만듭니다. 이 인덱스에 하나의 분할된 데이터베이스를 부여하고 복제본은 제공하지 않습니다.

- 인덱스에 특정 수량의 현실적인 프로덕션 데이터를 추가합니다.

- 인덱스에 대해 일반적인 쿼리, 집계 및 기타 워크로드를 수행하고 처리량과 응답 시간을 측정합니다.

- 처리량과 응답 시간이 허용되는 범위 안에 해당하면 3단계부터 프로세스를 반복합니다(다른 데이터 추가).

- 분할된 데이터베이스의 용량에 도달한 것 같으면(응답 시간과 처리량이 허용되지 않는 수준이 됨) 문서의 크기를 기록해 둡니다.

- 하나의 분할된 데이터베이스의 용량에서 프로덕션의 예상 문서 수를 추론하여 필요한 분할된 데이터베이스 수를 산출합니다(추론은 정교한 방법이 아니므로 이 계산에 오류의 여지를 반영해야 함).

> [AZURE.NOTE] 각 분할된 데이터베이스는 메모리, CPU 처리량 및 파일 핸들을 소비하는 Lucene 인덱스로 구현되었다는 점을 기억해야 합니다. 분할된 데이터베이스가 많을수록 필요한 리소스도 늘어납니다.

또한 분할된 데이터베이스가 더 많아지면 확장성이 증대되며(워크로드와 시나리오에 따라 다름) 데이터 수집 처리량은 늘어나지만 대규모 쿼리의 효율성은 떨어질 수 있습니다. 기본적으로 쿼리는 인덱스에서 사용되는 모든 분할된 데이터베이스를 확인합니다(필요한 데이터가 위치한 분할된 데이터베이스를 알고 있으면 [사용자 지정 라우팅][]을 사용하여 이 작업을 수정할 수 있음).

이 프로세스에서는 추정되는 분할된 데이터베이스 수만 생성되며 프로덕션에서 예상되는 문서 크기는 확인하지 못할 수 있습니다. 이 때는 위와 같이 최초 크기와 예상 증가율을 결정해야 합니다. 데이터베이스를 다시 인덱싱하기 전까지의 기간 동안 데이터 증대를 처리하기에 적합한 수의 분할된 데이터베이스를 만듭니다.

이벤트 관리, 롤링 인덱스 사용을 포함한 로깅 등의 시나리오에는 다른 전략을 사용합니다. 매일 수집하는 데이터에 대해 새 인덱스를 만들고 매일 가장 최근의 인덱스를 가리키도록 전환되는 별칭을 통해 이 인덱스에 액세스합니다. 이 방법에서는 오래된 데이터를 더 쉽게 무효화하고(더 이상 필요하지 않은 정보가 담긴 인덱스 삭제 가능) 데이터 규모를 관리 가능한 수준으로 유지할 수 있습니다.

노드 수가 분할된 데이터베이스 수와 일치할 필요는 없습니다. 예를 들어 50개의 분할된 데이터베이스를 만든 경우, 처음에는 10개의 노드에 배포한 다음 작업 규모가 증대하면 노드를 추가하여 시스템을 확장할 수 있습니다. 작은 규모의 노드에 지나치게 많은 분할된 데이터베이스를 만들면 안 됩니다(예를 들어 2개 노드에 분산된 1000개의 분할된 데이터베이스). 시스템은 이론적으로 이 구성에서 최대 1000개 노드까지 확장할 수 있지만 단일 노드에서 500개의 분할된 데이터베이스를 실행하면 노드 성능이 크게 낮아질 수 있습니다.

> [AZURE.NOTE] 데이터 수집이 큰 시스템의 경우 소수의 분할된 데이터베이스를 사용하는 것이 좋습니다. 이 때는 Elasticsearch가 문서를 분할된 데이터베이스에 전달하는 데 사용하는 기본 알고리즘에서 더 큰 분산이 발생합니다.

### 보안

기본적으로 Elasticsearch는 최소한의 보안을 구현하며 인증 및 권한 부여 방법을 제공하지 않습니다. 보안을 위해서는 기본 운영 체제와 네트워크를 구성하고 플러그인 및 타사 유틸리티를 사용해야 합니다. [Shield][], [Search Guard][] 등을 예로 들 수 있습니다.

> [AZURE.NOTE] Shield는 사용자 인증, 데이터 암호화, 역할 기반 액세스 제어, IP 필터링, 감사를 위해 Elastic에서 제공하는 플러그인입니다. 디스크 암호화 등 추가적인 보안 조치를 구현하기 위해서는 기본 운영 체제를 구성해야 할 수 있습니다.

프로덕션 시스템에서는 다음을 위한 방법을 고려해야 합니다.

- 클러스터에 대한 무단 액세스 금지
- 사용자 식별 및 인증.
- 인증된 사용자가 수행할 수 있는 작업의 권한 부여
- 불량 또는 손상된 작업으로부터 클러스터 보호
- 무단 액세스로부터 데이터 보호
- (해당하는 경우) 사용 데이터 보안을 위한 규정 요구 사항 준수

### 클러스터에 대한 액세스 보호

Elasticsearch는 네트워크 서비스입니다. Elasticsearch 클러스터의 노드는 HTTP를 통해 들어오는 클라이언트 요청을 수신하고 TCP 채널을 통해 서로 통신합니다. 무단 클라이언트나 서비스가 HTTP와 TCP 경로를 통해 요청을 보내지 못하도록 차단하기 위한 조치가 필요합니다. 다음 사항을 고려합니다.

VNET 또는 VM의 인바운드 및 아웃바운드 네트워크 트래픽을 특정 포트로만 제한하여 네트워크 보안 그룹을 정의합니다.

클라이언트 웹 액세스(9200)와 프로그램 방식 네트워크 액세스(9300)에 사용되는 기본 포트를 변경합니다. 방화벽을 사용하여 악의적인 인터넷 트래픽으로부터 각 노드를 보호합니다.

클라이언트의 위치 및 연결에 따라 인터넷에 대한 직접적인 액세스가 없는 비공개 서브넷에 클러스터를 배치합니다. 클러스터가 서브넷 외부에 노출되어야 할 경우 보든 요청을 요새 서버나, 클러스터 보호에 충분한 조치가 취해진 프록시를 통해 전달합니다.

노드에 대한 직접 액세스를 제공해야 하는 경우 Elasticsearch Jetty 플러그인을 사용하여 SSL 연결, 인증 및 연결 로깅을 제공합니다. 또는 nginx 프록시 서버와 HTTPS 인증을 구성합니다.

> [AZURE.NOTE] nginx와 같은 프록시 서버를 사용하여 기능에 대한 액세스를 제한할 수도 있습니다. 예를 들어, 클라이언트가 다른 작업을 수행하지 못하게 해야 한다면 \_search 끝점에 대한 요청만 허용하도록 nginx를 구성할 수 있습니다.

더 광범위한 네트워크 액세스 보안이 필요할 경우 Shield나 Search Guard 플러그인을 사용합니다.

### 사용자 식별 및 인증

클러스터에 대한 클라이언트의 모든 요청은 인증되어야 합니다. 또한 무단 노드가 클러스터에 연결하지 않도록 차단해야 합니다. 이 경우 인증을 건너 뛰는 시스템 백도어를 제공할 수 있습니다.

다음과 같은 다양한 형태의 인증을 수행할 수 있는 Elasticsearch 플러그인을 사용할 수 있습니다.

- **HTTP 기본 인증**. 사용자 이름 및 암호는 각각에 포함됩니다. 모든 요청은 SSL/TLS 또는 비슷한 수준의 보호를 통해 암호화되어야 합니다.

- **LDAP 및 Active Directory 통합**. 이 방법은 클라이언트가 LDAP 또는 AD 그룹에서 할당된 역할이 필요합니다.

- Elasticsearch 클러스터 자체 내에 정의된 ID를 사용하는 **네이티브 인증**입니다.

- 모든 노드를 인증하는 클러스터 내에서 **TLS 인증**입니다.

- 무단 서브넷의 클라이언트가 연결되는 것을 막고 클러스터에 조인한 이러한 서브넷의 노드를 방지하는 **IP 필터링**

### 클라이언트 요청 인증

권한 부여는 이 서비스를 제공하는 Elasticsearch 플러그인에 따라 결정됩니다. 예를 들어 기본 인증을 제공하는 플러그인은 보통 인증 수준을 정의하는 기능을 제공하는 반면 LDAP 또는 AD를 사용하는 플러그인은 보통 클라이언트를 역할과 연결한 다음 액세스 권한을 해당 역할에 할당합니다. 지정된 플러그 인을 사용할 때 다음 사항을 고려해야 합니다.

- 클라이언트에서 수행할 수 있는 작업을 제한해야 하나요? 예를 들어, 클라이언트가 클러스터 상태를 모니터링하거나 인덱스를 만들고 삭제할 수 있어야 하나요?

- 클라이언트가 특정 인덱스로 제한되어야 합니까? 테넌트가 자체적인 특정 인덱스 집합을 할당 받을 수 있고 이 인덱스를 다른 테넌트에서 액세스할 수 없어야 하는 다중 테넌트 상황에서 유용합니다.

- 클라이언트가 인덱스에 대한 데이터 읽기 및 쓰기가 가능해야 합니까? 예를 들어, 클라이언트가 인덱스를 사용하여 데이터를 가져오는 검색을 수행할 수 있지만 이 인덱스로부터 데이터를 추가하거나 삭제하는 작업은 차단되어야 합니다.

현재 대부분의 보안 플러그인은 작업 범위를 클러스터나 인덱스 수준으로 제한하며 인덱스 내 문서의 하위 집합에는 적용되지 않습니다. 이는 효율성 문제에 따른 것입니다. 따라서 요청을 단일 인덱스 내 특정 문서로 제한하기는 쉽지 않습니다. 이러한 수준까지의 세밀함이 필요할 경우 문서를 별도의 인덱스에 저장하고 인덱스를 하나로 그룹화하는 별칭을 사용합니다.

예를 들어 인사 시스템에서 사용자 A는 부서 X의 직원에 관한 정보를 포함하는 모든 문서에 액세스해야 하고, 사용자 B는 부서 Y의 직원에 관한 정보가 담긴 모든 문서에 액세스해야 하며, 사용자 C는 두 부서의 직원에 대한 정보가 담긴 모든 문서에 액세스해야 한다면 두 인덱스를 만들고(부서 X와 부서 Y에 대해) 두 인덱스를 모두 참조하는 별칭을 만듭니다. 사용자 A에게는 첫 번째 인덱스에 대한 읽기 권한을, 사용자 B에게는 두 번째 인덱스에 대한 읽기 권한을, 사용자 C에게는 별칭을 통해 두 인덱스 모두에 대한 읽기 액세스 권한을 부여합니다. 자세한 내용은 [별칭을 통한 사용자별 가장 인덱스][]를 참조하세요.

### 클러스터 보호

클러스터는 제대로 보호하지 않으면 악용되기 쉽습니다.

보안 취약성으로 이어질 수 있으므로 **Elasticsearch 쿼리에서 동적 쿼리 스크립팅을 비활성화합니다**. 쿼리 스크립팅보다 기본 스크립트를 사용합니다. 기본 스크립트는 Java로 작성되어 JAR 파일로 컴파일된 Elasticsearch 플러그인입니다.

이제 동적 쿼리 스크립팅이 기본적으로 비활성화되어 있습니다. 정당한 사유가 없는 한 이 기능을 다시 활성화하지 말아야 합니다.

**쿼리 문자열 검색을 사용자에게 노출하지 않도록 방지합니다**. 쿼리 문자열 검색을 통해 사용자가 방해 없이 리소스를 많이 소모하는 쿼리를 수행할 수 있습니다. 이러한 검색은 클러스터의 성능에 심각한 영향을 미치고 시스템을 렌더링하여 DoS 공격에 노출시킬 수 있습니다. 또한 쿼리 문자열 검색은 개인 정보를 노출할 가능성이 있습니다.

노드에서 Elasticsearch 장애를 초래하는 메모리 부족 예외를 초래할 수 있으므로 **작업이 많은 메모리를 사용하지 않게 합니다**. 장기 실행 리소스 집중형 작업도 DoS 공격을 구현하는 데 사용될 수 있습니다. 예를 들면 다음과 같습니다.

다음과 같이 대규모 필드를 메모리로 로드하는 검색 요청(해당 필드에서 쿼리 정렬, 스크립트 또는 패싯)을 방지합니다.

- 동시에 여러 인덱스를 쿼리하는 검색.

- 대규모 필드를 조회하는 검색. 이러한 검색은 대용량의 필드 데이터를 캐시하여 메모리를 소모할 수 있습니다. 기본적으로 필드 데이터 캐시는 크기 제한이 없지만 elasticsearch.yml 파일에서 [indices.fielddata.cache.*](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-fielddata.html) 속성을 설정하여 사용 가능한 리소스를 제한할 수 있습니다. 또한 [필드 데이터 차단기][]를 구성하면 단일 필드의 캐시된 데이터가 메모리를 소모하지 않게 방지하고 [요청 차단기][]를 통해 개별 쿼리가 메모리를 독점하는 것을 중지할 수 있습니다. 이 매개 변수를 설정하면 일부 쿼리가 실패하거나 시간 초과될 가능성이 높아지는 단점이 있습니다.
 
> [AZURE.NOTE] [Doc 값][] 값을 사용하면 필드 데이터를 메모리에 로드하는 것이 아니라 디스크에 저장하여 인덱스의 메모리 요구 사항을 줄일 수 있습니다. 그러면 노드에서의 메모리 사용 가능성은 줄지만 속도도 낮아집니다.

> [AZURE.NOTE] Elasticsearch는 항상 현재 워크로드를 수행하기에 충분한 메모리가 있다고 가정합니다. 그렇지 않은 경우 Elasticsearch 서비스가 충돌할 수 있습니다. Elasticsearch는 리소스 사용 관련 정보를 반환하는 끝점(HTTP [cat API][])을 제공하며 이 정보를 유의해서 모니터링해야 합니다.

**진행 중인 메모리 세그먼트를 플러시하기 위한 대기 시간이 너무 깁니다**. 이 경우 메모리 내 버퍼 공간이 소모될 수 있습니다. 필요한 경우 [translog를 구성][]하여 데이터가 디스크에서 플러시되는 임계값을 낮출 수 있습니다.

**대규모 메타데이터가 있는 인덱스를 만듭니다**. 필드 이름에서 변동이 큰 문서가 담긴 인덱스는 메모리를 많이 소비할 수 있습니다. 자세한 내용은 [매핑 급증][]을 참조하세요.
  
장기 실행 또는 쿼리 집약적 작업의 정의는 시나리오마다 크게 다릅니다. 일반적으로 한 클러스터에서 예상되는 워크로드는 다른 워크로드와는 완전히 다른 프로필을 갖을 수 있습니다. 허용 가능한 작업을 결정할 때는 응용 프로그램에 대한 상당한 연구와 테스트가 필요합니다.

사전 예방적인 태도를 취합니다. 악의적인 작업이 중대한 손상이나 데이터 손실을 초래하기 전에 탐지하여 중지합니다. 데이터 액세스의 비정상적인 패턴을 신속 탐지하고 사용자 로그인 요청이 실패하거나, 예기치 않은 노드가 클러스터를 연결하거나 떠날 경우 또는 작업이 예상보다 길어지는 등의 상황이 발생하면 경고하는보안 모니터링 및 알림 시스템을 사용하는 것이 좋습니다. 이러한 작업을 수행할 수 있는 도구에는 Elasticearch [Watcher][]가 있습니다.

### 데이터 보호

SSL/TLS를 사용하여 처리 중인 데이터를 보호할 수 있지만 Elasticsearch는 디스크에 저장된 정보에 대한 기본적인 형태의 데이터 암호화를 제공하지 않습니다. 이 정보는 일반 디스크 파일에 보관되며 해당 파일에 액세스 권한이 있는 모든 사용자가 자체 클러스터에 파일을 복사하는 등의 방식으로 여기에 담긴 데이터를 손상시킬 수 있다는 점에 유의해야 합니다. 다음 사항을 고려합니다.

- Elasticsearch에서 데이터를 보관하기 위해 사용하는 파일을 보호합니다. Elasticsearch 서비스 이외의 ID에 대한 임의의 읽기 또는 쓰기 액세스를 허용하지 않습니다.

- 파일 시스템 암호화를 사용하여 이러한 파일에 담긴 데이터를 암호화합니다.

> [AZURE.NOTE] 이제 Azure에서는 Linux 및 Windows VM의 디스크 암호화를 지원합니다. 자세한 내용은 [Windows 및 Linux IaaS VM용 Azure 디스크 암호화 미리 보기][]를 참조하세요.

### 규정 요구 사항 충족

외부 기관에 의한 모니터링(및 작업 재생)을 방지하기 위해 이벤트 기록을 유지 관리하는 감사 작업과 작업의 개인 정보 보호를 보장과 관련한 규제 준수 요구 사항이 중요한 관심사가 되고 있습니다. 특히 다음을 위한 방법을 고려해야 합니다.

- 모든 요청(성공 또는 실패) 및 모든 시스템 액세스 추적.

- 클라이언트와 클러스터 간 및 클러스터에서 수행하는 노드 간 통신 암호화. 모든 클러스터 통신에 SSL/TLS를 구현해야 합니다. Elasticsearch에서는 조직에서 SSL/TLS를 통해 제공되는 것과는 별도의 요구 사항이 있는 경우 플러그 방식의 암호화도 지원합니다.

- 모든 감사 데이터를 안전하게 저장. 감사 정보의 규모는 급증할 수 있으며 감사 정보의 변조를 방지하기 위해 강력히 보호되어야 합니다.

- 감사 데이터를 안전하게 보관.

### 모니터링

운영 체제 수준과 Elasticsearch 수준에서 모두 모니터링이 중요합니다.

운영 체제 수준에서는 운영 체제 특정 도구를 사용하여 모니터링을 수행할 수 있습니다. Windows에서는 여기에 적합한 성능 카운터가 있는 성능 모니터 등의 항목이 포함될 수 있지만 Linux에서는 *vmstat*, *iostat* 및 *top* 등의 도구를 사용할 수 있습니다. 운영 체제 수준에서의 모니터링을 위한 핵심 항목에는 CPU 사용률, 디스크 I/O 볼륨, 디스크 I/O 대기 시간, 네트워크 트래픽 등이 포함됩니다. 잘 튜닝된 Elasticsearch 클러스터에서 Elasticsearch 프로세스에 의한 CPU 사용률은 높고 디스크 I/O 대기 시간은 최소화되어야 합니다.

소프트웨어 수준에서는 실패한 요청의 세부 정보와 함께, 요청의 처리량과 응답 시간을 모니터링해야 합니다. Elasticsearch는 클러스터의 다양한 측면에서 성능을 조사하는 데 사용할 수 있는 여러 API를 제공합니다. 가장 중요한 두 API는 *\_cluster/health* 및 *\_nodes/stats*입니다. *\_cluster/health* API를 사용하여 클러스터의 전반적 상태에 대한 스냅숏을 제공하고, 다음 예에서처럼 각 인덱스에 대한 상세 정보를 제공할 수 있습니다.

`GET _cluster/health?level=indices`

아래의 출력 예제는 이 API를 통해 생성되었습니다.

```json
{
    "cluster_name": "elasticsearch",
    "status": "green",
    "timed_out": false,
    "number_of_nodes": 6,
    "number_of_data_nodes": 3,
    "active_primary_shards": 10,
    "active_shards": 20,
    "relocating_shards": 0,
    "initializing_shards": 0,
    "unassigned_shards": 0,
    "delayed_unassigned_shards": 0,
    "number_of_pending_tasks": 0,
    "number_of_in_flight_fetch": 0,
    "indices": {
        "systwo": {
            "status": "green",
            "number_of_shards": 5,
            "number_of_replicas": 1,
            "active_primary_shards": 5,
            "active_shards": 10,
            "relocating_shards": 0,
            "initializing_shards": 0,
            "unassigned_shards": 0
        },
        "sysfour": {
            "status": "green",
            "number_of_shards": 5,
            "number_of_replicas": 1,
            "active_primary_shards": 5,
            "active_shards": 10,
            "relocating_shards": 0,
            "initializing_shards": 0,
            "unassigned_shards": 0
        }
    }
}
```

이 클러스터에는 이름이 *systwo*와 *sysfour*인 두 인덱스가 포함됩니다. 각 인덱스를 모니터링하기 위한 핵심 통계는 status, active\_shards 및 unassigned\_shards입니다. 상태는 녹색이어야 하고, active\_shards는 number\_of\_shards 및 number\_of\_replicas를 반영하며 unassigned\_shards는 0이어야 합니다.

상태가 "빨강"이면 인덱스 일부가 누락되었거나 손상된 것입니다. *active\_shards* 설정이 *number\_of\_shards* - (*number\_of\_replicas* + 1) 미만이고 unassigned\_shards가 0이 아닌 경우 이를 확인할 수 있습니다. 상태가 노랑이면 인덱스가 전환 상태인 것으로, 다른 복제본이나 분할된 데이터베이스가 추가되어 재할당이 진행 중입니다. 전환이 완료되면 상태가 녹색으로 바뀌어야 합니다.

오랜 시간 동안 노랑 상태이거나 빨강으로 바뀌면 운영 체제 수준에서 중대한 I/O 이벤트(예: 디스크 또는 네트워크 오류)가 발생하지 않았는지 확인해야 합니다.

\_nodes/stats API는 클러스터의 각 노드에 대해 광범위한 정보를 내보냅니다.

`GET _nodes/stats`

생성되는 출력에는 각 노드에서 인덱스가 저장된 방법에 대한 세부 정보(문서 크기 및 수 포함), 인덱싱/쿼리/검색/병합 소요 시간, 캐싱, 운영 체제 및 프로세스 정보, JVM 관련 통계(가비지 수집 성능 포함), 스레드 풀 등이 포함됩니다. 자세한 내용은 [개별 노드 모니터링][]을 참조하세요.

Elasticsearch 요청 중 상당 비율이 실패하고 *EsRejectedExecutionException* 오류 메시지가 발생할 경우 Elasticsearch는 전송되는 작업을 제 속도로 처리하지 못합니다. 이 경우 Elasticsearch이 처지는 원인이 되는 병목을 파악해야 합니다. 다음 사항을 고려합니다.

- JVM에 할당된 메모리가 부족하여 과다한 가비지 수집을 초래하는 등, 병목의 원인이 리소스 제약에 있는 경우 추가 리소스를 할당하는 것이 좋습니다(이 경우 JVM이 노드에서 사용 가능한 저장소의 최대 50%까지 더 많은 메모리를 사용하도록 구성, [메모리 요구 사항][] 참조).

- 클러스터에서 I/O 대기 시간이 크며 \_node/stats API를 사용하여 인덱스에 대해 수집된 병합 통계에 큰 값이 있는 경우, 인덱스의 쓰기 작업이 과도한 것입니다. [인덱싱 작업을 위한 리소스 최적화](guidance-elasticsearch-tuning-data-ingestion-performance.md#optimizing-resources-for-indexing-operations) 섹션에서 나타난 요소를 다시 확인하여 인덱싱 성능을 튜닝합니다.

- 데이터 수집 작업을 수행 중인 클라이언트 응용 프로그램을 제한하고 성능에 미치는 영향을 판단합니다. 이 방법에서 성능이 크게 향상되었다면 제한을 그대로 유지할지 또는 더 많은 노드에 쓰기 작업이 많은 인덱스에 대한 로드를 분산하여 확장할지 여부를 고려합니다. 자세한 내용은 [Azure에서 Elasticsearch에 대한 데이터 수집 성능 튜닝][] 문서를 참조하세요.

- 인덱스에 대한 검색 통계에서 쿼리 시간이 길게 나타난 경우 쿼리 최적화 방법을 고려해 봅니다. 자세한 내용은 [쿼리 튜닝][] 섹션을 참조하세요. 검색 통계에서 보고한 *query\_time\_in\_millis* 및 *query\_total* 값을 사용하여 쿼리 효율성에 대한 대략적인 가이드를 산출할 수 있습니다. *query\_time\_in\_millis* / *query\_total* 식으로 각 쿼리의 평균 시간을 산출할 수 있습니다.

### Elasticsearch 모니터링을 위한 도구

다양 한 도구를 사용하여 프로덕션의 Elasticsearch에 대한 일상적인 모니터링을 수행할 수 있습니다. 이러한 도구는 보통 기본 Elasticsearch API를 사용하여 정보를 수집하고 원시 데이터보다 확인하기 쉬운 방식으로 세부 정보를 제시합니다. 일반적인 예로 [Elasticsearch-Head][], [Bigdesk][], [Kopf][] 및 [Marvel][] 등이 있습니다.

Elasticsearch-Head, Bigdesk 및 Kopf는 Elasticsearch 소프트웨어를 위한 플러그인 형태로 실행됩니다. 더 최신 버전의 Marvel을 독립적으로 실행할 수 있지만 [Kibana][]가 데이터 수집 및 호스팅 환경을 제공해야 합니다. Kibana에서 Marvel을 사용하면 Elasticsearch 클러스터와는 별도의 환경에서 모니터링을 구현하여, 모니터링 도구를 Elasticsearch 소프트웨어의 일부로 실행할 때는 불가능했던 Elasticsearch 문제를 탐색할 수 있는 장점이 있습니다. 예를 들어 Elasticsearch가 반복적으로 실패하거나 매우 느리게 실행되면 Elasticsearch 플러그인 형태로 실행되는 도구에도 영향을 미쳐 모니터링과 진단이 더 어려워집니다.

운영 체제 수준에서는 [Azure Operations Management Suite][]의 로그 분석 기능이나 [Azure 포털의 Azure 진단][]을 사용하여 Elasticsearch 노드를 호스팅하는 VM에 대한 성능 데이터를 캡처할 수 있습니다. 또 다른 방법은 [Logstash][]를 사용하여 성능 및 로그 데이터를 수집하고, 이 정보를 별도의 Elasticsearch 클러스터에 저장한 다음(응용 프로그램에 사용한 것과 동일한 클러스터는 사용하지 않음) Kibana를 사용하여 데이터를 시각화하는 것입니다. 자세한 내용은 [ELK에서 Microsoft Azure 진단][]을 참조하세요.

### Elasticsearch 성능 테스트를 위한 도구

Elasticsearch를 벤치마킹하거나 클러스터의 성능을 테스트하는 경우 다른 도구를 사용할 수 있습니다. 이러한 도구는 프로덕션이 아닌 개발 또는 테스트 환경에서 사용하도록 준비된 것입니다. 자주 사용되는 예로는 [Apache JMeter][]가 있습니다.

이 설명서와 관련된 문서에서 설명한 벤치마킹 및 기타 부하 테스트를 수행하기 위해 JMeter가 사용되었습니다. [Azure에서 Elasticsearch에 대한 성능 테스트 환경 만들기][] 문서에서는 JMeter를 구성하고 사용하는 방법을 자세히 설명합니다.

## 다음 단계

- [Elasticsearch: 최선의 가이드](https://www.elastic.co/guide/en/elasticsearch/guide/master/index.html)

[Running Elasticsearch on Azure]: guidance-elasticsearch-running-on-azure.md
[Azure에서 Elasticsearch에 대한 데이터 수집 성능 튜닝]: guidance-elasticsearch-tuning-data-ingestion-performance.md
[Azure에서 Elasticsearch에 대한 성능 테스트 환경 만들기]: guidance-elasticsearch-creating-performance-testing-environment.md
[Implementing a JMeter Test Plan for Elasticsearch]: guidance-elasticsearch-implementing-jmeter-test-plan.md
[Deploying a JMeter JUnit Sampler for Testing Elasticsearch Performance]: guidance-elasticsearch-deploying-jmeter-junit-sampler.md
[Azure에서 Elasticsearch에 대한 데이터 집계 및 쿼리 성능 튜닝]: guidance-elasticsearch-tuning-data-aggregation-and-query-performance.md
[Azure의 Elasticsearch에서 복원력 및 복구 구성]: guidance-elasticsearch-configuring-resilience-and-recovery.md
[Running the Automated Elasticsearch Resiliency Tests]: guidance-elasticsearch-configuring-resilience-and-recovery

[Apache JMeter]: http://jmeter.apache.org/
[Apache Lucene]: https://lucene.apache.org/
[가상 컴퓨터 규모 집합에서 자동으로 컴퓨터 규모 조정]: virtual-machines-vmss-walkthrough/
[Windows 및 Linux IaaS VM용 Azure 디스크 암호화 미리 보기]: azure-security-disk-encryption/
[Azure Load Balancer]: load-balancer-overview/
[Express 경로]: expressroute-introduction/
[내부 부하 분산 장치]: load-balancer-internal-overview/
[가상 컴퓨터 크기]: virtual-machines-size-specs/

[메모리 요구 사항]: #memory-requirements
[네트워크 요구 사항]: #network-requirements
[노드 검색]: #node-discovery
[쿼리 튜닝]: #query-tuning

[A Highly Available Cloud Storage Service with Strong Consistency]: http://blogs.msdn.com/b/windowsazurestorage/archive/2011/11/20/windows-azure-storage-a-highly-available-cloud-storage-service-with-strong-consistency.aspx
[Azure 클라우드 플러그인]: https://www.elastic.co/blog/azure-cloud-plugin-for-elasticsearch
[Azure 포털의 Azure 진단]: https://azure.microsoft.com/blog/windows-azure-virtual-machine-monitoring-with-wad-extension/
[Azure Operations Management Suite]: https://www.microsoft.com/server-cloud/operations-management-suite/overview.aspx
[Azure 빠른 시작 템플릿]: https://azure.microsoft.com/documentation/templates/
[Bigdesk]: http://bigdesk.org/
[cat API]: https://www.elastic.co/guide/en/elasticsearch/reference/1.7/cat.html
[translog를 구성]: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-translog.html
[사용자 지정 라우팅]: https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-routing-field.html
[Doc 값]: https://www.elastic.co/guide/en/elasticsearch/guide/current/doc-values.html
[Elasticsearch]: https://www.elastic.co/products/elasticsearch
[Elasticsearch-Head]: https://mobz.github.io/elasticsearch-head/
[Elasticsearch.Net & NEST]: http://nest.azurewebsites.net/
[Elasticsearch 스냅숏 및 복원 모듈]: https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html
[별칭을 통한 사용자별 가장 인덱스]: https://www.elastic.co/guide/en/elasticsearch/guide/current/faking-it.html
[필드 데이터 차단기]: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-fielddata.html#fielddata-circuit-breaker
[강제 병합]: https://www.elastic.co/guide/en/elasticsearch/reference/2.1/indices-forcemerge.html
[가십]: https://en.wikipedia.org/wiki/Gossip_protocol
[Kibana]: https://www.elastic.co/downloads/kibana
[Kopf]: https://github.com/lmenezes/elasticsearch-kopf
[Logstash]: https://www.elastic.co/products/logstash
[매핑 급증]: https://www.elastic.co/blog/found-crash-elasticsearch#mapping-explosion
[Marvel]: https://www.elastic.co/products/marvel
[ELK에서 Microsoft Azure 진단]: https://github.com/mspnp/semantic-logging/tree/elk
[개별 노드 모니터링]: https://www.elastic.co/guide/en/elasticsearch/guide/current/_monitoring_individual_nodes.html#_monitoring_individual_nodes
[nginx]: http://nginx.org/en/
[노드 클라이언트 API]: https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/node-client.html
[최적화]: https://www.elastic.co/guide/en/elasticsearch/reference/1.7/indices-optimize.html
[PubNub Changes 플러그인]: http://www.pubnub.com/blog/quick-start-realtime-geo-replication-for-elasticsearch/
[요청 차단기]: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-fielddata.html#request-circuit-breaker
[Search Guard]: https://github.com/floragunncom/search-guard
[Shield]: https://www.elastic.co/products/shield
[전송 클라이언트 API]: https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html
[내 노드]: https://www.elastic.co/blog/tribe-node
[Watcher]: https://www.elastic.co/products/watcher
[Zen]: https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html

<!---HONumber=AcomDC_0224_2016-->