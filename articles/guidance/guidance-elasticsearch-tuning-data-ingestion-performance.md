<properties
   pageTitle="Azure에서 Elasticsearch에 대한 데이터 수집 성능 튜닝 | Microsoft Azure"
   description="Azure에서 Elasticsearch로 데이터 수집 성능을 최대화하는 방법"
   services=""
   documentationCenter="na"
   authors="mabsimms"
   manager="marksou"
   editor=""
   tags=""/>

<tags
   ms.service="guidance"
   ms.devlang="na"
   ms.topic="article"
   ms.tgt_pltfrm="na"
   ms.workload="na"
   ms.date="02/18/2016"
   ms.author="masimms"/>

# Azure에서 Elasticsearch에 대한 데이터 수집 성능 튜닝

이 문서는 [시리즈의 일부](guidance-elasticsearch.md)입니다.

## 개요

검색 데이터베이스를 만들 때 중요한 사항은 검색 가능한 데이터를 빠르고 효율적으로 수집하도록 시스템을 구조화할 가장 좋은 방법을 결정하는 것입니다. 이러한 요구 사항과 관련하여, 해당 시스템을 구현하는 데 어떤 인프라를 선택할지와 시스템이 예상되는 데이터 유입 수준을 처리할 수 있도록 지원하기 위한 다양한 최적화도 고려해야 합니다.

이 문서에서는 높은 비율의 데이터 수집이 예상되는 Elasticsearch 클러스터 구현을 위해 고려할 배포 및 구성 옵션을 설명합니다. 설명을 위한 확실한 데이터를 제공하도록 이 문서에서는 간단한 대량 데이터 수집 워크로드를 사용하여 다양한 구성을 통한 벤치마킹의 결과도 보여 줍니다. 워크로드에 대한 세부 정보는 이 문서 끝부분의 [부록](#appendix-the-bulk-load-data-ingestion-performance-test)에 나와 있습니다.

벤치마크의 목적은 Elasticsearch 실행에 대한 절대적인 성능 수치를 생성하거나 특정 토폴로지를 권장하는 것이 아니라 성능을 평가하고 데이터 노드의 크기를 조정하며 사용자 고유의 성능 요구 사항을 충족할 수 있는 클러스터를 구현하는 데 사용할 수 있는 방법을 제시하는 데 있습니다.

사용자 고유의 시스템 크기를 조정할 때는 사용자 고유의 워크로드를 기반으로 성능을 철저하게 테스트하는 것이 중요합니다. 사용할 최적의 하드웨어 구성에 대한 정보를 얻을 수 있는 원격 분석과 고려해야 할 수평적 크기 조정 요인을 수집합니다. 특히, 다음을 수행해야 합니다.

- 각 대량 삽입 요청에서 항목 수뿐만 아니라 전송된 페이로드의 전체 크기를 고려합니다. 각 요청을 처리할 수 있는 리소스에 따라, 각 요청에서 큰 대량 항목 수가 적은 경우 개수가 많을 때보다 더욱 적합할 수 있습니다.

Marvel, JMeter의 *readbytes*/*writebytes* I/O 카운터, Ubuntu의 *iostat* 및 *vmstat*와 같은 운영 체제 도구를 사용하여 다양한 대량 삽입 요청의 효과를 모니터링할 수 있습니다.

- 성능 테스트를 수행하고 원격 분석을 수집하여 CPU 처리 및 I/O 대기 시간, 디스크 대기 시간, 처리량 및 응답 시간을 측정합니다. 이 정보를 통해 잠재적 병목 상태를 파악하고 프리미엄 저장소를 사용할 때의 비용 및 이점을 평가할 수 있습니다. 분할된 데이터베이스 및 복제본이 클러스터에 분산되는 방식에 따라 CPU 및 디스크 사용률이 모든 노드에 고르게 분산되지 않을 수 있다는 점에 주의해야 합니다(일부 노드가 다른 노드에 비해 더 많은 분할된 데이터베이스를 포함할 수 있음).

- 워크로드에 대한 동시 요청 수를 클러스터에 어떻게 배포할 것인지를 고려하고 이 워크로드를 처리하는 데 다양한 수의 노드를 사용할 경우 어떤 영향이 있는지 평가합니다.

- 비즈니스가 확장됨에 따라 워크로드가 어떻게 증가할 수 있는지 고려합니다. 이러한 증가가 노드에 사용된 VM 및 저장소의 비용에 어떤 영향을 주는지 평가합니다.

- 많은 수의 요청이 필요하고 디스크 인프라에서 SLA를 충족하는 처리량을 유지하는 시나리오에서는 일반 디스크에 많은 수의 노드가 있는 클러스터를 사용하는 것이 더욱 경제적임을 인식합니다. 그러나 노드 수를 늘리면 추가 노드 간 통신 및 동기화 형태로 오버헤드가 발생할 수 있습니다.

- 노드당 코어 수가 많을수록 더 많은 문서를 처리할 수 있으므로 더 많은 디스크 트래픽이 발생할 수 있습니다. 이 경우 디스크 사용률을 측정하여 I/O 하위 시스템에 병목 현상이 발생할 수 있는지 여부를 평가하고 프리미엄 저장소 사용 시 이점을 결정합니다.

- 노드 수는 많지만 코어 수는 적은 경우와 노드 수는 적지만 코어 수는 많은 경우를 비교하여 장단점을 테스트 및 분석합니다. 복제본의 수를 늘리면 클러스터에 대한 요구가 증가하여 노드를 추가해야 할 수 있음에 주의합니다.

- 사용 후 삭제 디스크를 사용하면 인덱스를 더욱 자주 복구해야 할 수 있습니다.

- 저장소 볼륨 사용률을 측정하여 저장소의 용량 및 미달 사용을 평가합니다. 예를 들어 이 시나리오에서는 350GB의 저장소를 사용하여 15억 개의 문서를 저장했습니다.

- 워크로드에 대한 전송 속도를 측정하고 가상 디스크를 만든 지정된 저장소 계정에 대한 총 I/O 속도 전송 제한에 얼마나 근접한지를 고려합니다.

## 노드 및 인덱스 디자인

대규모의 데이터 수집을 지원해야 하는 시스템에서는 다음 질문을 확인합니다.

- **데이터가 빠르게 이동하거나 비교적 정적입니까?** 데이터가 동적일수록 Elasticsearch에 대한 유지 관리 오버헤드는 증가합니다. 데이터가 복제되는 경우 각 복제본은 동기적으로 유지됩니다. 제한된 수명을 가지거나 쉽게 재구성할 수 있는 빠르게 이동하는 데이터는 복제를 아예 불가능하게 하는 것이 바람직할 수 있습니다. 이 옵션은 [대규모 데이터 수집 튜닝](#tuning-large-scale-data-ingestion) 섹션에서 자세히 다룹니다.

- **검색을 통해 검색된 데이터는 얼마나 최신이어야 합니까?** 성능을 유지하기 위해 Elasticsearch는 가능한 많은 데이터를 메모리에 버퍼링합니다. 따라서 일부 변경 내용은 검색 요청 시 즉시 사용할 수 없는 경우도 있습니다. Elasticsearch에서 변경 내용을 유지하고 이를 표시하는 프로세스는 [변경 내용을 영구로 만들기](https://www.elastic.co/guide/en/elasticsearch/guide/current/translog.html#translog) 문서에서 온라인으로 설명합니다.

    데이터가 표시되는 속도는 관련 인덱스의 *refresh\_interval*에 의해 제어됩니다. 기본적으로 이 간격은 1초에서 설정됩니다. 그러나 신속하게 새로 고칠 필요가 없는 상황도 있습니다. 예를 들어 로그 데이터를 기록하는 인덱스는 신속하게 수집되어야 하는 정보의 신속하고 지속적인 유입을 처리해야 하지만 이 정보를 즉시 쿼리에 사용할 수 있도록 할 필요는 없습니다. 이 경우 새로 고침 빈도를 줄이는 것이 좋습니다. 또한 이 기능은 [대규모 데이터 수집 튜닝](#tuning-large-scale-data-ingestion) 섹션에서 설명합니다.

- **데이터가 얼마나 빨리 증가할 것으로 예상합니까?** 인덱스 용량은 인덱스를 만들 때 지정된 분할 데이터베이스의 수에 따라 결정됩니다. 증가에 대비할 수 있도록 적절한 수의 분할된 데이터베이스를 지정합니다(기본값은 5). 최초로 인덱스가 단일 노드에 만들어진 경우 해당 노드에서 5개의 분할된 데이터베이스를 모두 찾지만 데이터 양이 늘어남에 따라 노드를 더 추가할 수 있으며 Elasticsearch가 분할된 데이터베이스를 노드 간에 동적으로 배포합니다. 그러나 각 분할된 데이터베이스는 오버헤드를 포함합니다. 인덱스에서 모든 검색은 모든 분할된 데이터베이스를 쿼리하므로 적은 양의 데이터에 대해 많은 분할된 데이터베이스를 만들면 데이터 검색 속도가 느려질 수 있습니다([Kagillion 분할된 데이터베이스](https://www.elastic.co/guide/en/elasticsearch/guide/current/kagillion-shards.html) 시나리오는 피하는 것이 좋습니다).

    로깅과 같은 일부 워크로드는 매일 새 인덱스를 만들 수 있으며 데이터 양에 대해 분할된 데이터베이스 수가 부족한 경우 다음 인덱스를 만들기 전에 변경해야 합니다(기존 인덱스에는 영향 없음). 기존 데이터를 더 많은 분할된 데이터베이스에 배포해야 하는 경우 정보를 다시 인덱싱하고 적절한 구성으로 새 인덱스를 만들고 데이터를 복사하는 것이 한 가지 옵션입니다. 이 프로세스는 [인덱스 별칭](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html)을 사용하여 응용 프로그램에 투명하게 만들 수 있습니다.

- **다중 테넌트 시나리오에서 사용자 간에 데이터를 파티션해야 합니까?** 각 사용자에 대해 별도의 인덱스를 만들 수 있지만 각 사용자에게 보통 수준의 데이터가 있는 경우 비용이 들 수 있습니다. 대신, [공유된 인덱스](https://www.elastic.co/guide/en/elasticsearch/guide/current/shared-index.html)를 만들고 [필터를 기반으로 별칭](https://www.elastic.co/guide/en/elasticsearch/guide/current/faking-it.html)을 사용하여 요청을 사용자별 데이터로 보내도록 합니다. 동일한 분할된 데이터베이스에서 사용자에 대한 데이터를 함께 유지하려면 인덱스에 대한 기본 라우팅 구성을 재정의하고 사용자의 일부 식별 정보를 기반으로 데이터를 경로 지정합니다.

- **데이터 수명이 장기 또는 단기입니까?** Azure VM 집합을 사용하여 Elasticsearch 클러스터를 구현하는 경우 사용 후 삭제 데이터를 연결된 드라이브 대신 로컬 리소스 시스템 디스크에 저장할 수 있습니다. 리소스 디스크에 대해 SSD를 활용하는 VM SKU를 사용하여 I/O 성능을 향상시킬 수 있습니다. 그러나 리소스 디스크가 보유한 모든 정보는 임시이며 VM이 재시작될 경우 손실될 수 있습니다(자세한 내용은 [Microsoft Azure 가상 컴퓨터에서 임시 드라이브 이해](http://blogs.msdn.com/b/mast/archive/2013/12/07/understanding-the-temporary-drive-on-windows-azure-virtual-machines.aspx) 문서의 임시 드라이브의 데이터가 손실될 경우 섹션 참조). 재시작한 사이 데이터를 유지해야 할 경우 영구 데이터 디스크를 만들어 이 정보를 보관하고 VM에 연결합니다.

- **데이터가 얼마나 활성 상태입니까?** Azure VHD에는 읽기/쓰기 작업의 양이 지정된 매개 변수의 양을 초과하는 경우 제한이 적용됩니다(현재 표준 계층 VM에 연결된 디스크의 경우 500 IOPS이고 프리미엄 저장소 디스크의 경우 5000 IOPS).

    제한 가능성을 줄이고 I/O 성능을 향상시키려면 [디스크 및 파일 시스템 요구 사항](guidance-elasticsearch-running-on-azure.md#disk-and-file-system-requirements)에서 설명한 대로 각 VM에 대해 여러 데이터 디스크를 만들고 이러한 디스크에 데이터를 스트라이프하도록 Elasticsearch를 구성하는 것이 좋습니다.

    자주 액세스하는 데이터를 캐시할 만큼 충분한 메모리를 확보하여 디스크 I/O 읽기 작업 수를 최소화하도록 하드웨어 구성을 선택해야 합니다. 이에 대한 내용은 Azure에서 Elasticsearch 구현 문서의 [메모리 요구 사항](guidance-elasticsearch-running-on-azure.md#memory-requirements) 섹션에서 설명합니다.

- **각 노드에서 어떤 유형의 워크로드를 지원해야 합니까?** Elasticsearch는 파일 시스템 캐시 형식으로 JVM 힙에 대해 데이터를 캐시할 수 있는 메모리를 보유하는 이점이 있으며 이에 대한 내용은 Azure에서 Elasticsearch 구현 문서의 [메모리 요구 사항](guidance-elasticsearch-running-on-azure.md#memory-requirements) 섹션에서 설명합니다.

    메모리 양, CPU 코어 수 및 사용 가능한 디스크 수는 가상 컴퓨터의 SKU에 따라 설정됩니다. 자세한 내용은 Azure 웹 사이트의 [가상 컴퓨터 가격 책정](http://azure.microsoft.com/pricing/details/virtual-machines/) 페이지를 참조하세요.

### 가상 컴퓨터 옵션

다양한 수의 SKU를 사용하여 Azure에서 VM을 프로비전할 수 있습니다. Azure VM에 사용할 수 있는 리소스는 선택한 SKU에 따라 다릅니다. 각 SKU는 서로 다른 코어, 메모리 및 저장소의 결합을 제공합니다. 예상되는 워크로드를 처리하면서 비용 효율성이 입증된 적절한 크기의 VM을 선택해야 합니다. 현재 요구 사항을 충족하는 구성부터 시작합니다(이 문서의 뒷부분에 설명된 대로 벤치마킹을 수행하여 테스트). Elasticsearch 노드를 실행하는 VM을 더 추가하여 나중에 클러스터의 규모를 확장할 수 있습니다.

Azure 웹 사이트의 [가상 컴퓨터 크기](virtual-machines-size-specs/) 페이지에는 VM에 사용 가능한 다양한 옵션 및 SKU가 문서화되어 있습니다.

VM의 크기 및 리소스는 VM에서 실행되는 노드가 수행할 역할에 맞아야 합니다.

데이터 노드:

- 최대 30GB 또는 Java 힙에 사용 가능한 RAM 메모리의 50% 중 낮은 값을 할당합니다. 나머지는 운영 체제에서 파일 캐싱에 사용하도록 둡니다. Linux를 사용하는 경우 Elasticsearch를 실행하기 전에 ES\_HEAP\_SIZE 환경 변수를 설정하여 Java 힙에 할당할 메모리의 양을 지정할 수 있습니다. 또는 Windows 또는 Linux를 사용하는 경우 Elasticsearch를 시작할 때 *Xmx* 및 *Xms* 매개 변수로 메모리 크기를 명시할 수 있습니다.

    워크로드에 따라 큰 VM을 적은 수 사용하는 것은 중간 크기의 VM을 많이 사용하는 것만큼 성능 면에서 효율적이지 않을 수 있습니다. 추가 네트워크 트래픽 및 관련 유지 관리와 사용 가능한 코어 수를 늘리고 각 노드에서 디스크 경합을 줄일 경우 비용의 장단점을 측정할 수 있는 테스트를 수행해야 합니다.

- Elasticsearch 데이터를 저장하기 위해 프리미엄 저장소를 사용합니다. 이에 대한 내용은 [저장소 옵션](#storage-options) 섹션에서 자세히 설명합니다.

- 같은 크기의 여러 디스크를 사용하고 이러한 디스크 간에 데이터를 스트라이프합니다. VM의 SKU에 따라 연결할 수 있는 최대 데이터 디스크 수가 결정됩니다. 자세한 내용은 [디스크 및 파일 시스템 요구 사항](guidance-elasticsearch-running-on-azure.md#disk-and-file-system-requirements)을 참조하세요.

- 멀티 코어 CPU SKU를 사용합니다(코어 2개 이상, 4개 이상 권장).

클라이언트 노드:

- Elasticsearch 데이터에 대해 디스크 저장소를 할당하지 마십시오. 전용 클라이언트는 디스크에 데이터를 저장하지 않습니다.

- 워크로드를 처리하기에 적절한 메모리가 있는지 확인합니다. 대량 삽입 요청은 데이터가 다양한 데이터 노드로 전송되기 전에 메모리로 읽히고 집계 및 쿼리 결과가 메모리에 누적된 후 클라이언트 응용 프로그램에 반환됩니다. 최적의 요구 사항을 평가하기 위해 Marvel과 같은 도구 또는 *노드/통계* API(`GET _nodes/stats`)를 사용하여 반환된 [JVM 정보](https://www.elastic.co/guide/en/elasticsearch/guide/current/_monitoring_individual_nodes.html#_jvm_section)를 사용하여 사용자 고유의 워크로드를 벤치마킹하고 메모리 사용을 모니터링합니다. 특히, 각 노드에 대한 *heap\_used\_percent* 메트릭을 모니터링하여 힙 크기를 사용 가능한 공간의 75% 미만으로 유지하는 것을 목표로 합니다.

- 예상되는 요청 볼륨을 수신 및 처리할 만큼 충분한 CPU 코어가 있는지 확인합니다. 요청은 처리 전 수신됨에 따라 큐에 대기되고 큐에 대기할 수 있는 항목 볼륨은 각 노드에서 CPU 코어 수에 따라 결정됩니다. 노드/통계 API를 사용하여 반환된 [ThreadPool 정보](https://www.elastic.co/guide/en/elasticsearch/guide/current/_monitoring_individual_nodes.html#_threadpool_section)의 데이터를 사용하여 큐 길이를 모니터링할 수 있습니다.

    큐에 대해 *거부된* 수는 거부되는 요청을 나타내므로 클러스터에 병목 상태가 시작되고 있음을 나타냅니다. 이는 CPU 대역폭 때문일 수 있지만 메모리 부족 또는 느린 I/O 성능과 같은 다른 요인일 수 있으므로 이 정보와 다른 통계를 함께 사용하여 근본 원인을 파악합니다.

    클라이언트 노드는 워크로드에 따라 필요할 수도, 그렇지 않을 수도 있습니다. 데이터 수집 워크로드는 보통 전용 클라이언트를 사용할 경우 이점이 없는 반면 일부 검색 및 집계는 보다 신속하게 실행할 수 있으므로 사용자 고유의 시나리오를 벤치마킹할 준비를 합니다.

    클라이언트 노드는 전송 클라이언트 API를 사용하여 클러스터에 연결하는 응용 프로그램에 주로 유용합니다. 또한 응용 프로그램 호스트 환경의 리소스를 사용하여 응용 프로그램에 대한 전용 클라이언트를 동적으로 만드는 노드 클라이언트 API를 사용할 수도 있습니다. 응용 프로그램에서 노드 클라이언트 API를 사용하는 경우 클러스터에 미리 구성된 전용 클라이언트 노드를 포함하지 않아도 됩니다.
    
    그러나 클라이언트 노드 API를 사용하여 만든 노드는 클러스터의 첫 번째 클래스 멤버이며 따라서 다른 노드와 네트워크 통신에 참여한다는 것을 유의해야 합니다. 자주 시작 및 중지되는 클라이언트 노드는 클러스터 전체에서 불필요한 노이즈를 만들 수 있습니다.

마스터 노드:

- Elasticsearch 데이터에 대해 디스크 저장소를 할당하지 마십시오. 전용 마스터 노드는 디스크에 데이터를 저장하지 않습니다.

- CPU 요구 사항은 최소화해야 합니다.

- 메모리 요구 사항은 클러스터 크기에 따라 달라집니다. 클러스터의 상태에 대한 정보는 메모리에서 유지됩니다. 소형 클러스터의 경우 필요한 메모리 양이 최소이지만 인덱스가 자주 만들어지고 분할된 데이터베이스가 이동하는 대형의 매우 활동적인 클러스터는 상태 정보의 양이 눈에 띄게 증가할 수 있습니다. JVM 힙 크기를 모니터링하여 추가 메모리가 필요한지 결정합니다.

> [AZURE.NOTE]  클러스터의 안정성을 높이기 위해 항상 여러 마스터 노드를 만들고 나머지 노드를 구성하여 스플릿 브레인이 발생할 가능성을 피합니다. 이상적으로 홀수의 마스터 노드가 있어야 합니다. 이 토픽은 [Azure의 Elasticsearch에서 복원력 및 복구 구성][] 문서에 자세히 설명되어 있습니다.

### 저장소 옵션

Azure VM에는 비용, 성능, 가용성 및 신중하게 고려해야 하는 복구에 영향을 주는 서로 다른 장단점을 포함하는 다양한 저장소 옵션이 제공됩니다.

전용 데이터 디스크에 Elasticsearch 데이터를 저장해야 합니다. 이렇게 하면 운영 체제와 경합이 줄어들고 대량의 Elasticsearch I/O가 I/O 리소스에 대해 운영 체제 함수와 경쟁하지 않도록 할 수 있습니다.

Azure 디스크에는 성능 제약 조건이 적용됩니다. 클러스터에서 정기적으로 작업이 급증하는 것이 발견되는 경우 I/O 요청이 제한될 수 있습니다. 이를 방지하려면 각 디스크에서 수신할 것으로 예상되는 요청 볼륨에 대해 Elasticsearch 문서 크기의 균형이 맞도록 디자인을 조정합니다.

프리미엄 저장소에 기반한 디스크가 최대 5,000개의 IOPS로 작동할 수 있지만 표준 저장소에 기반한 디스크는 500 IOPS의 최대 요청 속도를 지원합니다. 프리미엄 저장소 디스크는 DS 및 GS 시리즈의 VM에만 사용할 수 있습니다. Azure VM에 대한 최대 디스크 IOPS 속도는 [온라인 문서](virtual-machines-size-specs/)로 제공됩니다.

**영구 데이터 디스크**

영구 데이터 디스크는 Azure 저장소에 의해 지원되는 VHD입니다. 중대한 오류 후 VM을 다시 만들어야 하는 경우 기존 VHD를 새 VM에 쉽게 연결할 수 있습니다. VHD는 표준 저장소(회전 미디어) 또는 프리미엄 저장소(SSD)를 기반으로 만들 수 있습니다. SSD를 사용하려는 경우 DS 시리즈 이상을 사용하여 VM을 만들어야 합니다. DS 컴퓨터 비용은 해당하는 D 시리즈 VM과 동일하지만 프리미엄 저장소 사용에 대한 추가 요금이 부과됩니다.

디스크당 최대 전송 속도가 예상된 워크로드를 지원하기에 충분하지 않은 경우 여러 데이터 디스크를 만들거나 Elasticsearch에서 [이러한 디스크 간에 데이터를 스트라이프](guidance-elasticsearch-running-on-azure.md#disk-and-file-system-requirements)하도록 허용하거나 [가상 디스크를 사용하여 시스템 수준 RAID 0 스트라이프](virtual-machines-linux-configure-raid/)를 구현하는 방법을 고려합니다.

> [AZURE.NOTE] Microsoft 내의 경험에 따르면 RAID 0을 사용할 경우 대량의 작업을 자주 생성하는 *불규칙한* 워크로드의 I/O 효과를 원활하게 하는 데 특히 도움이 되는 것으로 나타났습니다.

디스크를 보유하는 저장소 계정에 대해 프리미엄 로컬 중복(또는 최소값이나 QA 워크로드에 대해 로컬 중복)을 사용합니다. Elasticsearch HA에 지역 및 영역 간 복제가 필요하지 않습니다.

**사용 후 삭제 디스크**

SSD 기반의 영구 디스크를 사용하려면 프리미엄 저장소를 지원하는 VM을 만들어야 합니다. 이것은 가격에 영향을 줍니다. Elasticsearch 데이터를 저장하는 데 로컬 사용 후 삭제 디스크를 사용하는 것은 최대 약 800GB의 저장소를 필요로 하는 중간 크기의 노드를 위한 비용 효율적인 솔루션이 될 수 있습니다. VM의 표준 D 시리즈에서 사용 후 삭제 디스크는 일반 디스크보다 훨씬 더 높은 성능 및 훨씬 더 낮은 대기 시간을 제공하는 SSD를 사용하여 구현됩니다.

Elasticsearch를 사용하는 경우 성능은 비용을 들이지 않고 프리미엄 저장소를 사용하는 경우와 동등할 것입니다. 자세한 내용은 [디스크 대기 시간 문제 해결](#addressing-disk-latency-issues) 섹션을 참조하세요.

[D 시리즈 성능 기대 사항](https://azure.microsoft.com/blog/d-series-performance-expectations/) 문서에 설명된 것처럼 VM의 크기는 사용 후 삭제 저장소에서 사용 가능한 공간의 양을 제한합니다.

예를 들어 Standard\_D1 VM이 50GB의 사용 후 삭제 저장소를 제공하고 Standard\_D2 VM에 100GB의 사용 후 삭제 저장소가 있는 경우 Standard\_D14 VM은 800GB의 사용 후 삭제 공간을 제공합니다. 노드가 이 공간 양만 필요로 하는 클러스터의 경우 사용 후 삭제 저장소가 있는 D 시리즈 VM은 비용 효율적일 수 있습니다.

컴퓨터를 다시 시작한 후 이 데이터를 복구하는 데 필요한 시간 및 비용을 기준으로 사용 후 삭제 저장소와 함께 증가한 처리량의 균형을 맞추어야 합니다. VM이 다른 호스트 서버로 이동하거나 호스트가 업데이트되거나 호스트에서 하드웨어 오류가 발생하는 경우 사용 후 삭제 디스크의 내용이 손실됩니다. 데이터 자체의 수명이 제한된 경우 이 데이터 손실은 허용 가능할 수 있습니다. 수명이 긴 데이터의 경우 인덱스를 다시 작성하거나 백업에서 누락된 정보를 복구할 수 있습니다. 다른 VM에 있는 복제본을 사용하여 손실 가능성을 최소화할 수 있습니다.

> [AZURE.NOTE] 중요한 프로덕션 데이터를 보관하는 데 **단일** VM을 사용하지 않습니다. 노드에 오류가 발생하면 모든 데이터를 사용할 수 없습니다. 중요한 정보인 경우 데이터를 적어도 하나의 다른 노드에 복제해 두어야 합니다.

**Azure 파일**

[Azure 파일 서비스](http://blogs.msdn.com/b/windowsazurestorage/archive/2014/05/12/introducing-microsoft-azure-file-service.aspx)는 Azure 저장소를 사용하여 공유 파일 액세스를 제공합니다. 나중에 Azure VM에 탑재할 수 있는 파일 공유를 만들 수 있습니다. 여러 VM이 동일한 파일 공유를 탑재할 수 있어 동일한 데이터에 액세스할 수 있습니다.

성능상의 이유로 노드 간 공유하지 않아도 되는 Elasticsearch 데이터를 보관하는 데 파일 공유를 사용하지 않는 것이 좋습니다. 이러한 용도로는 일반 데이터 디스크가 더 적합합니다. Elasticsearch [섀도 복제본 인덱스](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-shadow-replicas.html)를 만드는 데 파일 공유를 사용할 수 있습니다. 그러나 이 기능은 현재 실험 상태이고 아직은 프로덕션 환경에서 구현할 수 없습니다. 따라서 이 지침에서 섀도 인덱스는 더 이상 고려되지 않습니다.

**네트워크 옵션**

Azure는 공유 네트워킹 체계를 구현합니다. 동일한 하드웨어 랙을 활용하는 VM은 네트워크 리소스에 대해 경쟁합니다. 따라서 사용 가능한 네트워크 대역폭은 하루 중 시간에 따라, 동일한 실제 네트워크 인프라를 공유하는 VM에서 실행되는 작업의 일일 주기에 따라 달라질 수 있습니다. 이러한 요소는 제어하기가 힘듭니다. 네트워크 성능이 시간에 따라 변동될 수 있음을 이해하고 사용자 기대치를 적절하게 설정하는 것이 중요합니다.

## 대규모 데이터 수집을 지원하도록 노드 확장

합당한 보통 수준의 하드웨어를 사용하여 Elasticsearch 클러스터를 빌드한 후 데이터의 양이 증가하고 요청 수가 늘어남에 따라 기능을 강화하거나 규모를 확장할 수 있습니다. Azure에서는 보다 크고 비싼 VM에서 실행하여 기능을 강화하거나 보다 작고 저렴한 VM을 사용하여 규모를 확장할 수 있습니다.

또한 두 전략의 조합을 수행할 수도 있습니다. 모든 시나리오에 대해 보편적으로 적용할 수 있는 솔루션은 없으므로 주어진 환경에 대해 최선의 방법을 평가하여 일련의 성능 테스트를 거치도록 준비해야 합니다.

이 섹션은 확장 방법과 관련됩니다. 확장은 [확장: 결론](#scaling-out-conclusions) 섹션에서 설명됩니다. 이 섹션에서는 다양한 크기의 VM으로 구성된 Elasticsearch 클러스터 집합에 대해 수행된 일련의 벤치마크 결과를 설명합니다. 클러스터 크기는 작음, 중간, 큼으로 지정되었습니다. 다음 표에서는 각 클러스터의 VM에 할당된 리소스를 요약하여 보여 줍니다.

| 프로비전 | VM SKU | 코어 수 | 데이터 디스크 수 | RAM |
|---------|-------------|-----------------|----------------------|------|
| 작음 | 표준 D2 | 2 | 4 | 7GB |
| 중간 | 표준 D3 | 4 | 8 | 14GB |
| 큼 | 표준 D4 | 8 | 16 | 28GB |

각 Elasticsearch 클러스터에는 3개의 데이터 노드를 포함했습니다. 이러한 데이터 노드는 클라이언트 요청을 처리하고 데이터 처리를 담당했습니다. 이 노드는 테스트에 사용된 데이터 수집 시나리오에 거의 이점을 제공하지 않으므로 별도의 클라이언트 노드를 사용했습니다. 또한 클러스터는 3개의 마스터 노드를 포함하는데 이 중 하나는 클러스터를 조정하기 위해 Elasticsearch에서 선택한 것입니다.

테스트는 Elasticsearch 1.7.3을 사용하여 수행되었습니다. 테스트는 Ubuntu Linux 14.0.4를 실행하는 클러스터에서 처음으로 수행된 후 Windows Server 2012를 사용하여 반복되었습니다. 테스트를 수행한 워크로드에 대한 세부 정보는 [부록](#appendix-the-bulk-load-data-ingestion-performance-test)에 나와 있습니다.

### 데이터 수집 성능 - Ubuntu Linux 14.0.4

다음 표에서는 각 구성에 대해 2시간 동안 테스트를 실행한 전체 결과를 요약하여 보여 줍니다.

| 구성 | 샘플 수 | 평균 응답 시간(밀리초) | 처리량(작업/초) |
|---------------|-----------|----------------------------|---------------------------|
| 작음 | 67057 | 636 | 9\.3 |
| 중간 | 123482 | 692 | 17\.2 |
| 큼 | 197085 | 839 | 27\.4 |

세 가지 구성에 대해 처리된 처리량 및 샘플 수는 약 1:2:3의 비율입니다. 그러나 메모리, CPU 코어 및 디스크를 기준으로 사용할 수 있는 리소스 비율은 1:2:4입니다. 이와 같은 결과가 나온 이유를 평가하기 위해 클러스터의 노드에서 하위 수준 성능 세부 정보를 조사할 필요가 있다고 판단되었습니다. 이 정보는 기능 강화에 제한이 있는지 여부와 언제 규모 확장을 고려하는 것이 좋은지를 판단하는 데 도움이 될 수 있습니다.

### 제한 요소 결정: 네트워크 사용률

Elasticsearch는 클러스터의 노드 간에 이동하는 동기화 정보뿐만 아니라 클라이언트 요청의 유입을 지원하는 충분한 네트워크 대역폭이 있는지에 의존합니다. 앞에서 강조한 것처럼 사용자는 사용 중인 데이터 센터, 동일한 네트워크 인프라를 공유하는 다른 VM의 현재 네트워크 부하와 같은 다양한 변수에 의존하는 대역폭 가용성을 제한적으로 제어할 수 있습니다. 그러나 트래픽 볼륨이 과도하지 않은지 확인하는 경우에만 각 클러스터에 대해 네트워크 활동을 검사하는 것이 좋습니다. 아래 그래프는 각 클러스터에서 노드 2가 수신한 네트워크 트래픽을 비교한 것입니다(각 클러스터의 다른 노드에 대한 볼륨은 매우 유사합니다).

![](media/guidance-elasticsearch/data-ingestion-image1.png)

각 클러스터 구성에서 노드 2가 2시간 동안 수신한 초당 평균 바이트는 다음과 같습니다.

| 구성 | 수신한 초당 평균 바이트 수 |
|---------------|--------------------------------------|
| 작음 | 3993640\.3 |
| 중간 | 7311689\.9 |
| 큼 | 11893874\.2 |

시스템이 **안정적인 상태**로 실행되는 동안 테스트를 수행했습니다. 인덱스 균형 재조정 또는 노드 북구가 발생하는 상황에서 주 및 복제본 분할된 데이터베이스를 보유하는 노드 간 데이터 전송은 상당한 네트워크 트래픽을 생성할 수 있습니다. 이 프로세스의 효과는 [Azure의 Elasticsearch에서 복원력 및 복구 구성][] 문서에 자세히 설명되어 있습니다.

### 제한 요소 결정: CPU 사용률

요청이 처리되는 속도는 적어도 사용 가능한 처리 용량에 의해 부분적으로 결정됩니다. Elasticsearch는 대량 삽입 큐에 대량 삽입 요청을 수락합니다. 각 노드에는 사용 가능한 프로세서의 수에 따라 결정되는 대량 삽입 큐 집합이 있습니다. 기본적으로, 각 프로세서마다 하나의 큐가 있으며 각 큐는 거부 시작되기 전 최대 50개의 미해결 요청을 보유할 수 있습니다.

응용 프로그램은 큐에 과잉 공급이 발생하지 않는 속도로 요청을 전송해야 합니다. 임의 시간에 각 큐에서 항목 수는 클라이언트 응용 프로그램에서 요청이 전송되는 속도와 이러한 동일한 요청이 Elasticsearch에서 검색 및 처리되는 속도에 따라 결정됩니다. 따라서 캡처된 한 가지 중요한 통계는 요류율에 대한 것이며 다음 표에 요약됩니다.

| 구성 | 총 샘플 수 | 오류 수 | 오류율 |
|---------------|---------------|-----------|------------|
| 작음 | 67057 | 0 | 0\.00% |
| 중간 | 123483 | 1 | 0\.0008% |
| 큼 | 200702 | 3617 | 1\.8% |

이러한 각 오류는 다음 Java 예외로 인해 발생합니다.

```
org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$1@75a30c1b]; ]
[219]: index [systembase], type [logs], id [AVEAioKb2TRSNcPa_8YG], message [RemoteTransportException[[esdatavm2][inet[/10.0.1.5:9300]][indices:data/write/bulk[s]]]; nested: EsRejectedExecutionException[rejected execution (queue capacity 50)
```

큐의 수를 늘리거나 각 큐의 길이를 늘리면 오류 수는 줄일 수 있지만 이 방법은 단기간에 급증하는 경우만 대처할 수 있습니다. 지속적인 데이터 수집 작업을 실행하는 동안 이 작업을 하면 오류가 발생하기 시작하는 시점을 지연시킬 뿐입니다. 또한 이러한 변경으로 처리량이 향상되지 않으며 요청이 처리되기 전에 큐에 오랫동안 대기하므로 클라이언트의 응답 시간에 부정적인 영향을 줄 수 있습니다.

복제본 1개와 분할된 데이터베이스 5개(모두 10개의 분할된 데이터베이스)에 대한 결과 클러스터의 노드 간에 약간의 부하 불균형이 있습니다. 두 개 노드는 세 개의 분할된 데이터베이스를 포함하는 반면 다른 노드는 네 개를 포함합니다. 가장 많이 사용되는 노드는 처리량이 제한될 가능성이 가장 높은 항목이므로 각 사례에서 이 노드를 선택합니다.

다음 그래프 집합은 각 클러스터에서 가장 많이 사용되는 노드의 CPU 사용률을 보여 줍니다.

![](media/guidance-elasticsearch/data-ingestion-image2.png)

![](media/guidance-elasticsearch/data-ingestion-image3.png)

![](media/guidance-elasticsearch/data-ingestion-image4.png)

소형, 중간 및 대형 클러스터의 경우 이러한 노드에 대한 평균 CPU 사용률은 75.01%, 64.93% 및 64.64%였습니다. 드물게 사용률이 100%에 도달하고 노드 크기와 사용 가능한 CPU 처리 능력이 증가함에 따라 사용률이 떨어집니다. 따라서 CPU 능력은 대형 클러스터의 성능을 제한하는 요인이 될 가능성이 적습니다.

### 제한 요소 결정: 메모리

메모리 사용은 성능에 영향을 줄 수 있는 또 다른 중요한 측면입니다. 테스트를 위해 Elasticsearch는 50%의 사용 가능한 메모리를 할당했으며 이것은 [문서화된 권장 사항](https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html#_give_half_your_memory_to_lucene)을 따른 것입니다. 테스트가 실행되는 동안 JVM에 대해 과도한 가비지 수집 활동(힙 메모리 부족을 나타냄)을 모니터링합니다. 모든 경우에 힙 크기는 안정적이었으며 JVM은 낮은 가비지 수집 활동을 나타냈습니다. 아래 스크린샷은 Marvel의 스냅숏을 보여주며, 대형 클러스터에서 테스트를 실행하는 동안 단기간의 주요 JVM 통계를 강조 표시합니다.

![](media/guidance-elasticsearch/data-ingestion-image5.png)

***대형 클러스터에서 JVM 메모리 및 가비지 수집 활동.***

### 제한 요소 결정: 디스크 I/O 속도

성능을 제한할 수 있는 서버 쪽 나머지 물리적 특성은 디스크 I/O 하위 시스템의 성능입니다. 아래 그래프는 각 클러스터에서 가장 많이 사용되는 노드에 대해 기록된 바이트를 기준으로 디스크 활동을 비교합니다.

![](media/guidance-elasticsearch/data-ingestion-image6.png)

다음 표는 각 클러스터 구성에서 노드 2에 대해 2시간 동안 기록된 초당 평균 바이트 수를 보여 줍니다.

| 구성 | 기록된 초당 평균 바이트 수 |
|---------------|-------------------------------------|
| 작음 | 25502361\.94 |
| 중간 | 48856124\.5 |
| 큼 | 88137675\.46 |

기록된 데이터 볼륨은 클러스터에서 처리한 요청 수와 함께 증가하지만 I/O 속도는 Azure 저장소의 한도 내에 있습니다(Azure 저장소를 사용하여 만든 디스크는 표준 또는 프리미엄 저장소를 사용했는지에 따라 수십 또는 수백 MB/s의 지속적인 속도를 지원할 수 있습니다). 디스크 I/O를 대기하는 데 소요된 시간을 검사하면 디스크 처리량이 이론적인 최대값보다 훨씬 아래에 있는 이유를 설명할 수 있습니다. 아래 그래프 및 표는 동일한 세 개 노드에 대해 이러한 통계를 보여 줍니다.

> [AZURE.NOTE] 디스크 대기 시간은 I/O 작업이 완료되기를 기다리는 프로세서가 차단된 동안 CPU 시간의 비율을 모니터링하여 측정합니다.

![](media/guidance-elasticsearch/data-ingestion-image7.png)

![](media/guidance-elasticsearch/data-ingestion-image8.png)

![](media/guidance-elasticsearch/data-ingestion-image9.png)

| 구성 | 평균 디스크 대기 CPU 시간(%) |
|---------------|--------------------------------|
| 작음 | 21\.04 |
| 중간 | 14\.48 |
| 큼 | 15\.84 |

이 데이터는 디스크 I/O가 완료될 때까지 기다리는 데 CPU 시간의 상당 부분(거의 16% ~ 21%)이 소요된 것을 나타냅니다. 이것은 Elasticsearch에서 요청을 처리하고 데이터를 저장하는 기능을 제한합니다.

테스트 실행 중에 대형 클러스터는 **5억 개 이상의 문서**를 삽입했습니다. 계속 테스트한 결과 데이터베이스가 6억 개 이상의 문서를 포함한 경우 대기 시간이 크게 증가한 것으로 나타났습니다. 이러한 동작이 나타나는 이유를 완벽하게 조사하지는 못했지만 디스크 조각화가 디스크 대기 시간을 증가시켰기 때문일 수 있습니다.

더 많은 노드를 통해 클러스터 크기를 늘리면 이 동작의 영향을 완화할 수 있습니다. 극단적인 경우에는 과도한 I/O 시간을 보이는 디스크에 대해 조각 모음을 수행해야 할 수 있습니다. 그러나 큰 디스크의 조각 모음에는 상당한 시간이 소요될 수 있으며(아마도 2TB VHD 드라이브의 경우 48시간 이상) 디스크를 다시 포맷하고 Elasticsearch가 복제본 분할된 데이터베이스에서 누락된 데이터를 복구하도록 하는 것이 더욱 비용 효율적인 방법일 수 있습니다.

### 디스크 대기 시간 문제 해결

테스트는 처음에 표준 디스크로 구성된 VM을 사용하여 수행되었습니다. 표준 디스크는 회전 미디어를 기반으로 하므로 회전 대기 시간 및 기타 병목 현상이 I/O 속도를 제한할 수 있습니다. Azure에서는 SSD 장치를 사용하여 만든 디스크의 프리미엄 저장소도 제공합니다. 이러한 장치는 회전 대기 시간이 없으므로 향상된 I/O 속도를 제공합니다.

아래 표는 대형 클러스터에서 표준 디스크를 프리미엄 디스크로 대체한 결과를 비교하여 보여 줍니다(대형 클러스터에서 표준 D4 VM을 표준 DS4 VM으로 대체했으며 코어 수, 메모리 및 디스크는 두 경우 모두 동일하고 DS4 VM에서 SSD를 사용한 것만 다릅니다).

| 구성 | 샘플 수 | 평균 응답 시간(밀리초) | 처리량(작업/초) |
|------------------|-----------|----------------------------|---------------------------|
| 큼 - 표준 | 197085 | 839 | 27\.4 |
| 큼 - 프리미엄 | 255985 | 581 | 35\.6 |

응답 시간이 크게 향상되므로 평균 처리량은 소형 클러스터보다 4배에 가깝습니다. 이것은 표준 DS4 VM에서 사용할 수 있는 리소스에 가깝습니다. I/O를 완료하는 데 대기 시간이 줄어듦에 따라 클러스터에서 가장 많이 사용되는 노드의 평균 CPU 사용률(이 경우 노드 1)이 증가했습니다.

![](media/guidance-elasticsearch/data-ingestion-image10.png)

디스크 대기 시간 단축은 다음 그래프를 살펴보면 확연히 나타납니다. 이 통계에서 가장 많이 사용되는 노드에 대해 평균 약 1%까지 감소했습니다.

![](media/guidance-elasticsearch/data-ingestion-image11.png)

그러나 이러한 개선에 대해서는 지불할 비용이 있습니다. 수집 오류 수가 35797건(12.3%)으로 10배 증가했습니다. 마찬가지로 이러한 오류 중 대부분은 대량 삽입 큐 오버플로의 결과였습니다. 하드웨어가 용량에 근접하여 실행되는 것으로 나타나는 것을 고려하면 노드를 더 추가하거나 대량 삽입 비율을 다시 제한하여 오류 볼륨을 줄여야 할 수 있습니다. 이러한 문제는 이 문서의 뒷부분에 설명되어 있습니다.

### 사용 후 삭제 저장소로 테스트

사용 후 삭제 저장소를 사용하여 D4 VM의 클러스터에서 동일한 테스트를 반복했습니다. D4 VM에서 사용 후 삭제 저장소는 단일 400GB SSD로 구현됩니다. 처리된 샘플 수, 응답 시간 및 처리량은 모두 프리미엄 저장소가 있는 DS14 VM 기반 클러스터에 대해 보고된 수치와 매우 유사합니다.

| 구성 | 샘플 수 | 평균 응답 시간(밀리초) | 처리량(작업/초) |
|-----------------------------------|-----------|----------------------------|---------------------------|
| 큼 - 프리미엄 | 255985 | 581 | 35\.6 |
| 큼 - 표준(사용 후 삭제 디스크) | 255626 | 585 | 35\.5 |

오류율 역시 유사했습니다(총 289488개 요청 중 33862개 오류 - 11.7%).

다음 그래프는 클러스터에서 가장 많이 사용되는 노드(이번에는 노드 2)에 대한 CPU 사용률과 디스크 대기 작업 통계를 보여 줍니다.

![](media/guidance-elasticsearch/data-ingestion-image12.png)

![](media/guidance-elasticsearch/data-ingestion-image13.png)

이 경우에 성능 측면만 보면 사용 후 삭제 저장소를 프리미엄 저장소를 사용하는 것보다 비용 효율적인 솔루션으로 고려할 수 있습니다.

### 데이터 수집 성능 - Windows Server 2012

Windows Server 2012를 실행하는 노드가 있는 Elasticsearch 클러스터 집합을 사용하여 동일한 테스트를 반복했습니다. 이러한 테스트의 목적은 운영 체제의 선택이 클러스터 성능에 어떤 영향(있는 경우)을 주는지 확립하는 것입니다.

Windows에서 Elasticsearch의 확장성을 설명하기 위해 다음 표에서는 소형, 중형 및 대형 클러스터 구성에 대해 달성된 처리량 및 응답 시간을 보여 줍니다. Ubuntu 테스트에서 디스크 대기 시간이 최대 성능을 달성하는 데 중요한 요소가 될 수 있음을 확인했으므로 이러한 테스트는 모두 SSD 사용 후 삭제 저장소를 사용하도록 Elasticsearch를 구성한 상태로 수행되었습니다.

| 구성 | 샘플 수 | 평균 응답 시간(밀리초) | 처리량(작업/초) |
|---------------|-----------|----------------------------|---------------------------|
| 작음 | 90295 | 476 | 12\.5 |
| 중간 | 169243 | 508 | 23\.5 |
| 큼 | 257115 | 613 | 35\.6 |

이러한 결과는 Elasticsearch가 Windows에서 사용 가능한 리소스 및 VM 크기를 어떻게 조정하는지를 나타냅니다.

다음 표는 Ubuntu 및 Windows에서 대형 클러스터에 대한 결과를 비교한 것입니다.

| 운영 체제 | 샘플 수 | 평균 응답 시간(밀리초) | 처리량(작업/초) | 오류율(%) |
|------------------|-----------|----------------------------|---------------------------|----------------|
| Ubuntu | 255626 | 585 | 35\.5 | 11\.7 |
| Windows | 257115 | 613 | 35\.6 | 7\.2 |

처리량은 대형 Ubuntu 클러스터에 대한 값과 일치했지만 응답 시간은 약간 높았으며 이것은 낮은 오류율과 관련될 수 있습니다(오류는 성공적인 작업보다 빨리 보고되므로 응답 시간이 낮습니다).

Windows 모니터링 도구에서 보고하는 CPU 사용률은 Ubuntu보다 약간 높았습니다. 그러나 다양한 운영 체제에서 이러한 통계를 보고하는 방식 때문에 이러한 운영 체제 간 측정값을 직접 비교할 때는 세심하게 처리해야 합니다. 또한 I/O 대기에 소요된 CPU 시간을 기준으로 디스크 대기 시간 정보는 Ubuntu와 동일한 방법으로 사용할 수 없습니다. 요점은 CPU 사용률이 높았고 이것은 I/O 대기 시간이 낮다는 것을 나타냅니다.

![](media/guidance-elasticsearch/data-ingestion-image14.png)

### 강화: 결론

잘 조정된 클러스터의 Elasticsearch 성능은 Windows 및 Ubuntu에서 동등할 수 있으며 두 운영 체제에서 유사한 패턴으로 강화됩니다. 최상의 성능을 위해 **Elasticsearch를 보관하는 데 프리미엄 저장소를 사용합니다**.

## 대규모 데이터 수집을 지원하는 클러스터 확장

확장은 이전 섹션에서 조사한 내용을 확장하기 위한 보완 방법입니다. Elasticsearch의 중요한 기능은 소프트웨어에 내장된 고유한 수평적 확장성입니다. 클러스터의 크기를 늘리려면 더 많은 노드를 추가하기만 하면 됩니다. 이 프로세스에 영향을 주는 데 사용할 수 있는 다양한 구성 옵션이 있기는 하지만 작업이 자동으로 처리되므로 인덱스 또는 분할된 데이터베이스를 다시 배포하는 수동 작업을 수행하지 않아도 됩니다.

노드를 더 추가하면 더 많은 컴퓨터에 부하를 분산하여 성능을 개선할 수 있습니다. 노드를 더 추가함에 따라 데이터를 다시 인덱싱하여 사용 가능한 분할된 데이터베이스 수를 늘리는 것을 고려해야 할 수 있습니다. 처음에 사용 가능한 노드에서보다 더 많은 분할된 데이터베이스가 있는 인덱스를 만들어 이 프로세스를 어느 정도 선점할 수 있습니다. 노드를 더 추가하면 분할된 데이터베이스를 배포할 수 있습니다.

Elasticsearch의 수평적 확장성이라는 이점 외에도 노드보다 많은 분할된 데이터베이스를 포함하는 인덱스를 구현하는 다른 이유가 있습니다. 각 분할된 데이터베이스는 별도의 데이터 구조로 구현되며([Lucene](https://lucene.apache.org/) 인덱스) 일관성을 유지하고 동시성을 처리하기 위한 고유한 내부 메커니즘을 보유합니다. 여러 분할된 데이터베이스를 만들면 노드 내에서 병렬 처리를 향상시킬 수 있으며 성능을 개선할 수 있습니다.

하지만 크기를 조정하면서 성능을 유지하는 것은 균형을 잡는 작업입니다. 클러스터에 있는 노드와 분할된 데이터베이스가 많을수록 클러스터에서 수행한 작업을 동기화하는 작업이 늘어나며 이로 인해 처리량이 감소할 수 있습니다. 지정된 모든 워크로드에 대해 유지 관리 오버헤드를 최소화하면서 수집 성능은 최대화하는 최적 지점이 있습니다. 이 최적 지점은 워크로드 및 클러스터의 특성에 따라 크게 달라지는데, 특히 볼륨, 크기 및 문서 내용, 수집이 발생하는 속도 및 시스템이 실행되는 하드웨어에 따라 달라집니다.

이 섹션에서는 이전에 설명한 성능 테스트에 사용된 워크로드를 지원하기 위해 클러스터 크기 조정을 조사한 결과를 요약합니다. Ubuntu Linux 14.0.4를 실행하는 대형 VM 크기(8개의 CPU 코어, 16개 데이터 디스크, 28GB RAM을 포함하는 표준 D4) 기반 VM의 클러스터에서 노드 및 분할된 데이터베이스 수는 다르게 구성하여 동일한 테스트를 수행했습니다. 결과는 하나의 특정 시나리오에만 적용되므로 단정하지 않는 것이 좋으며 클러스터의 수평적 확장성을 분석하고 사용자 요구 사항에 가장 맞는 노드 대 분할된 데이터베이스의 최적 비율에 대한 수치를 생성하는 적절한 시작 지점으로 활용할 수 있습니다.

### 초기 결과 – 노드 3개

초기 수치를 얻기 위해 데이터 수집 성능 테스트를 분할된 데이터베이스 5개와 복제본 1개가 있는 3 노드 클러스터에 대해 실행했습니다. 이것이 Elasticsearch 인덱스에 대한 기본 구성입니다. 이 구성에서 Elasticsearch는 2개의 주 분할된 데이터베이스를 2개의 노드로 배포하고 나머지 주 분할된 데이터베이스는 세 번째 노드에 저장됩니다. 아래 표에서는 초당 대량 수집 작업을 기준으로 한 처리량과 테스트에 의해 성공적으로 저장된 문서 수를 요약하여 보여 줍니다.

> [AZURE.NOTE] 이 섹션의 이어서 나오는 표에서 주 분할된 데이터베이스의 배포는 대시로 구분된 각 노드의 숫자로 표시됩니다. 예를 들어 5-분할된 데이터베이스 3-노드 레이아웃은 2-2-1로 설명되어 있습니다. 복제본 분할된 데이터베이스의 레이아웃은 포함되지 않으며 이는 주 분할된 데이터베이스와 유사한 체계를 따릅니다.

| 구성 | 문서 수 | 처리량(작업/초) | 분할된 데이터베이스 레이아웃 |
|---------------|--------------|-----------------------------|--------------|
| 분할된 데이터베이스 5개 | 200560412 | 27\.86 | 2-2-1 |

### 6-노드 결과

6 노드 클러스터에서 테스트를 반복했습니다. 이러한 테스트의 목적은 노드에 분할된 데이터베이스를 둘 이상 저장해보고 그 효과를 보다 정확하게 파악하는 것입니다.

| 구성 | 문서 수 | 처리량(작업/초) | 분할된 데이터베이스 레이아웃 |
|---------------|--------------|-----------------------------|--------------|
| 분할된 데이터베이스 4개 | 227360412 | 31\.58 | 1-1-0-1-1-0 |
| 분할된 데이터베이스 7개 | 268013252 | 37\.22 | 2-1-1-1-1-1 |
| 분할된 데이터베이스 10개 | 258065854 | 35\.84 | 1-2-2-2-1-2 |
| 분할된 데이터베이스 11개 | 279788157 | 38\.86 | 2-2-2-1-2-2 |
| 분할된 데이터베이스 12개 | 257628504 | 35\.78 | 2-2-2-2-2-2 |
| 분할된 데이터베이스 13개 | 300126822 | 41\.68 | 2-2-2-2-2-3 |

이러한 결과는 다음과 같은 동향을 나타내는 것으로 확인됩니다.

* 노드당 분할된 데이터베이스가 늘어나면 처리량이 향상됩니다. 이러한 테스트를 위해 만든 적은 수의 노드당 분할된 데이터베이스에서도 앞에서 설명한 이유로 이러한 현상이 발생할 수 있습니다.

* 분할된 데이터베이스 수가 홀수인 경우 짝수일 때보다 성능이 더 낫습니다. 이유는 확실하지 않지만 이 경우에 Elasticsearch에서 사용하는 라우팅 알고리즘이 분할된 데이터베이스에 데이터를 보다 잘 배포하여 노드당 부하가 고르게 되기 때문*일 수 있습니다*.

이러한 가설을 테스트하기 위해 더 많은 수의 분할된 데이터베이스로 여러 번 테스트를 더 수행했습니다. Elasticsearch의 충고에 따라 해당 범위의 홀수를 적절히 배포하므로 각 테스트에 분할된 데이터베이스의 소수를 사용하기로 결정했습니다.

| 구성 | 문서 수 | 처리량(작업/초) | 분할된 데이터베이스 레이아웃 |
|---------------|--------------|-----------------------------|-------------------|
| 분할된 데이터베이스 23개 | 312844185 | 43\.45 | 4-4-4-3-4-4 |
| 분할된 데이터베이스 31개 | 309930777 | 43\.05 | 5-5-5-5-6-5 |
| 분할된 데이터베이스 43개 | 316357076 | 43\.94 | 8-7-7-7-7-7 |
| 분할된 데이터베이스 61개 | 305072556 | 42\.37 | 10-11-10-10-10-10 |
| 분할된 데이터베이스 91개 | 291073519 | 40\.43 | 15-15-16-15-15-15 |
| 분할된 데이터베이스 119개 | 273596325 | 38\.00 | 20-20-20-20-20-19 |

이 결과에서 23개의 분할된 데이터베이스 근처에서 티핑 포인트에 도달했음을 알 수 있습니다. 이 시점 이후에는 분할된 데이터베이스의 수를 늘리면 성능 저하가 발생합니다(분할된 데이터베이스 43개에 대한 처리량은 비정상).

### 9-노드 결과

9개 노드가 있는 클러스터에서 다시 분할된 데이터베이스의 소수를 사용하여 테스트를 반복했습니다.

| 구성 | 문서 수 | 처리량(작업/초) | 분할된 데이터베이스 레이아웃 |
|---------------|--------------|-----------------------------|----------------------------|
| 분할된 데이터베이스 17개 | 325165364 | 45\.16 | 2-2-2-2-2-2-2-2-1 |
| 분할된 데이터베이스 19개 | 331272619 | 46\.01 | 2-2-2-2-2-2-2-2-3 |
| 분할된 데이터베이스 29개 | 349682551 | 48\.57 | 3-3-3-4-3-3-3-4-3 |
| 분할된 데이터베이스 37개 | 352764546 | 49\.00 | 4-4-4-4-4-4-4-4-5 |
| 분할된 데이터베이스 47개 | 343684074 | 47\.73 | 5-5-5-6-5-5-5-6-5 |
| 분할된 데이터베이스 89개 | 336248667 | 46\.70 | 10-10-10-10-10-10-10-10-9 |
| 분할된 데이터베이스 181개 | 297919131 | 41\.38 | 20-20-20-20-20-20-20-20-21 |

이러한 결과는 37개의 분할된 데이터베이스 근처에서 티핑 포인트에 도달하는 유사한 패턴을 보여 줍니다.

### 규모 확장: 결론

대략적으로 추정하자면 이 특정 시나리오에서 6-노드 및 9-노드 테스트 결과, 성능을 최대화하는 분할된 데이터베이스의 이상적인 수는 4n+/-1이며, 여기서 n은 노드 수입니다. 사용 가능한 대량 삽입 스레드 수의 함수*일 수 있으며* 이는 다시 CPU 코어 수에 따라 달라집니다. 이론적 근거에 대한 자세한 내용은 [다중 문서 패턴](https://www.elastic.co/guide/en/elasticsearch/guide/current/distrib-multi-doc.html#distrib-multi-doc)을 참조하세요.

- 클라이언트 응용 프로그램에서 보낸 각 대량 삽입 요청은 단일 데이터 노드를 통해 수신됩니다.

- 데이터 노드는 원래 요청의 영향을 받는 주 분할된 데이터베이스 각각에 대한 새 대량 삽입 요청을 작성하고 이를 동시에 다른 노드에 전달합니다.

- 각각의 주 분할된 데이터베이스가 기록됨에 따라 다른 요청이 해당 분할된 데이터베이스의 각 복제본으로 전송됩니다. 주 분할된 데이터베이스는 복제본으로 전송된 요청이 완료되기를 기다렸다가 마무리합니다.

기본적으로 Elasticsearch는 VM에서 사용 가능한 각 CPU 코어에 대해 하나의 대량 삽입 스레드를 만듭니다. 이 테스트에서 사용하는 D4 VM의 경우 각 CPU에 코어 8개가 포함되므로 8개의 대량 삽입 스레드가 만들어집니다. 사용된 인덱스는 각 노드에서 4개(한 경우 5개) 주 분할된 데이터베이스를 스팬하지만 각 노드에는 4(5)개의 복제본도 있습니다. 이러한 분할된 데이터베이스 및 복제본에 데이터를 삽입하면 사용 가능한 수에 맞게 요청당 각 노드에서 최대 8개의 스레드를 사용할 수 있습니다. 분할된 데이터베이스의 수를 늘리거나 줄이면 스레드가 점유되지 않거나 요청이 대기되므로 스레딩 비효율성을 야기할 수 있습니다. 그러나 추가 실험이 없다면 단지 이론일 뿐, 단정할 수 있습니다.

테스트는 한 가지 다른 중요한 사항도 보여 줍니다. 이 시나리오에서 노드 수를 늘리면 데이터 수집 처리량을 향상시킬 수 있지만 결과는 반드시 선형적으로 확장되지는 않습니다. 12 및 15 노드 클러스터에서 테스트를 더 수행하면 규모를 확장해도 추가 이점이 거의 없는 지점이 나타납니다. 이 노드 수로는 저장소 공간이 부족한 경우 기능 강화 전략으로 전환하여 프리미엄 저장소 기반의 보다 크고 더 많은 디스크를 사용해야 할 수 있습니다.

> [AZURE.IMPORTANT] 4n+/-1의 비율을 모든 클러스터에 대해 항상 작동하는 마법 수식으로 여기지 마세요. 사용 가능한 CPU 코어 수가 적거나 많아지면 최적의 분할된 데이터베이스 구성은 달라질 수 있습니다. 이 결과는 데이터 수집만 수행한 특정 워크로드를 기반으로 합니다. 쿼리 및 집계를 함께 포함하는 워크로드의 경우 결과는 매우 다양할 수 있습니다.

> 또한 데이터 수집 워크로드는 단일 인덱스를 사용했습니다. 대부분의 경우 데이터는 여러 인덱스에 분산될 가능성이 높아 서로 다른 패턴 또는 리소스를 사용합니다.

> 이 연습에서 중요한 점은 획득한 결과보다는 사용한 방법을 이해하는 것입니다. 자신의 고유한 워크로드를 기반으로 나만의 확장성 평가를 수행하여 자신의 시나리오에 가장 적합한 정보를 획득할 준비가 되어야 합니다.

## 대규모 데이터 수집 튜닝

Elasticsearch는 특정 사용 사례 및 시나리오에 대해 성능을 최적화하는 데 사용할 수 있는 다양한 스위치와 설정으로 매우 다양하게 구성할 수 있습니다. 이 섹션에서는 몇 가지 일반적인 예를 설명합니다. 이러한 측면에서 Elasticsearch가 제공하는 유연성은 Elasticsearch를 디튜닝하기 매우 쉽고 성능을 악화시킬 수 있다는 경고이기도 합니다. 튜닝할 경우 한 번에 하나만 변경하고 변경 내용으로 인한 영향을 항상 측정하여 시스템에 해롭지 않은지 확인합니다.

### 인덱싱 작업을 위한 리소스 최적화

다음 목록에서는 대규모 데이터 수집을 지원하기 위해 Elasticsearch 클러스터를 튜닝할 때 고려해야 하는 사항에 대해 설명합니다. 처음 두 항목은 성능에 즉각적으로 확연한 영향을 줄 가능성이 높은 반면 나머지는 워크로드에 따라 영향이 미미합니다.

*  인덱스에 추가된 새 문서만 인덱스를 새로 고칠 때 검색에 표시됩니다. 인덱스를 새로 고치는 작업은 비용이 드는 작업이므로 문서를 만들 때마다 수행하기보다는 주기적으로만 수행합니다. 기본 새로 고침 간격은 1초입니다. 대량 작업을 수행하는 경우 인덱스 새로 고침을 일시적으로 사용하지 않도록 설정해야 하며 *refresh\_interval*을 -1로 설정합니다.

	```http
	PUT /my_busy_index
	{
		"settings" : {
			"refresh_interval": -1
		}
	}
	```

	작업 끝에 [*\_refresh*](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-refresh.html) API를 사용하여 데이터가 표시되도록 새로 고침을 수동으로 트리거합니다. 자세한 내용은 [대량 인덱싱 사용](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-update-settings.html#bulk)을 참조하세요. [데이터 수집 시 새로 고침 간격 변경의 영향](#the-impact-of-changing-the-index-refresh-interval-on-data-ingestion-performance)에 대한 자세한 내용은 뒷부분에서 설명합니다.

* 인덱스가 복제되는 경우 주 분할된 데이터베이스에서 인덱싱이 발생하면 복제본 분할된 데이터베이스에서 각 인덱싱 작업(문서 만들기, 업데이트 또는 삭제)이 반복됩니다. 대량 가져오기 작업 중에 복제를 사용하지 않도록 설정한 후 가져오기가 완료되면 다시 사용하도록 설정하는 것이 좋습니다.

    ```http
	PUT /my_busy_index
	{
		"settings" : {
			"number_of_replicas": 0
		}
	}
	```

	복제를 다시 사용할 경우 Elasticsearch는 인덱스에서 각 복제본으로 데이터의 바이트 단위 네트워크 전송을 수행합니다. 이 방법은 각 노드에서 문서 단위로 인덱싱 프로세스를 반복하는 것보다 더 효율적입니다. 대량 가져오기를 수행하는 동안 실패한 주 노드의 데이터가 손실될 수 있다는 위험이 있지만 가져오기를 다시 시작하는 것으로 복구가 가능합니다. [데이터 수집 성능에서 복제가 미치는 영향](#the-impact-of-replicas-on-data-ingestion-performance)은 나중에 자세히 설명합니다.

* Elasticsearch는 쿼리에 필요한 리소스와 데이터 수집에 필요한 리소스 간에 사용 가능한 리소스의 균형을 유지하려고 합니다. 따라서 데이터 수집 성능을 제한할 수 있습니다(제한 이벤트는 Elasticsearch 로그에 기록됩니다). 이 제한은 병합 및 디스크에 저장이 필요하여 리소스를 독점할 수 있는 프로세스인, 대규모 인덱스 세그먼트 동시 만들기를 방지하기 위한 것입니다. 시스템이 현재 쿼리를 수행하지 않는 경우 데이터 수집 제한을 사용하지 않도록 설정할 수 있습니다. 인덱싱을 허용하여 성능을 최대화해야 합니다. 다음과 같이 전체 클러스터에 대해 제한을 사용하지 않도록 할 수 있습니다.

	```http
	PUT /_cluster/settings
	{
		"transient" : {
			"indices.store.throttle.type": "none"
		}
	}
	```

    수집이 완료되면 클러스터의 제한 형식을 다시 *"병합"*으로 설정합니다. 또한 제한을 사용하지 않도록 설정하면 클러스터가 불안정해질 수 있으므로 필요한 경우 클러스터를 복구할 수 있는 절차를 갖추어야 합니다.

* Elasticsearch에서는 힙 메모리의 일정 비율을 인덱싱 작업용으로 예약하고 메모리의 나머지는 주로 쿼리 및 검색에 사용됩니다. 이러한 버퍼의 목적은 조금씩 여러 번 쓰는 것보다 많이씩 더 적게 쓰도록 하여 디스크 I/O 작업의 수를 줄이는 것입니다. 힙 메모리의 기본 비율은 10%입니다. 대량의 데이터를 인덱싱하는 경우 이 값은 부족할 수 있습니다. 대용량 데이터 수집을 지원하는 시스템의 경우 노드에서 활성 상태인 각 분할된 데이터베이스에 대해 최대 512MB까지 허용해야 합니다. 예를 들어, D4 VM(28GB RAM)에서 Elasticsearch를 실행하고 사용 가능한 메모리 중 50%를 JVM(14GB)에 할당한 경우 인덱싱 작업에는 1.4GB를 사용할 수 있게 됩니다. 노드에 활성 상태인 3개의 분할된 데이터베이스가 있는 경우 이 구성은 충분할 것입니다. 그러나 노드에 이보다 분할된 데이터베이스가 더 많은 경우 elasticsearch.yml 구성 파일에서 *indices.memory.index\_buffer\_size* 매개 변수 값을 늘리는 것이 좋습니다. 자세한 내용은 참조 [Elasticsearch 인덱싱에 대한 성능 고려 사항](https://www.elastic.co/blog/performance-considerations-elasticsearch-indexing)을 참조하세요.

    활성 분할된 데이터베이스당 512MB 이상을 할당하면 인덱싱 성능은 향상되지 않겠지만 다른 작업을 수행하는 데 사용할 수 있는 메모리가 적어서 실제로는 해로울 수 있습니다. 또한 인덱스 버퍼에 더 많은 힙 공간을 할당하면 데이터 검색 및 집계와 같은 다른 작업에 대한 메모리가 제거되어 쿼리 작업의 성능이 저하될 수 있습니다.

* Elasticsearch는 분할된 데이터베이스에서 인덱싱 작업을 동시에 수행할 수 있는 스레드 수(기본값은 8)를 제한합니다. 노드가 적은 수의 분할된 데이터베이스만 포함하는 경우 대량의 인덱싱 작업이 수행되거나 대량 삽입 대상인 인덱스에 대한 *index\_concurrency* 설정을 늘리는 것이 좋습니다.

	```http
	PUT /my_busy_index
	{
		"settings" : {
			"index_concurrency": 20
		}
	}
	```

* 단기간에 많은 수의 인덱싱 및 대량 작업을 수행하는 경우 스레드 풀에서 사용할 수 있는 *인덱스* 및 *대량* 스레드 수를 늘리고 각 데이터 노드에 대한 *대량 삽입* 큐의 크기를 확장할 수 있습니다. 요청을 삭제하기보다는 더 많은 요청을 큐에 대기시킬 수 있습니다. 자세한 내용은 [스레드 풀](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html)을 참조하세요. 지속적으로 높은 수준의 데이터 수집을 수행하는 경우 대량 스레드 수를 늘리는 것은 좋지 않습니다. 대신, 추가 노드를 만들고 분할을 사용하여 인덱싱 부하를 이러한 노드 간에 분산시킵니다. 또는 대량 삽입 배치를 병렬이 아닌 직렬로 전송하는 것이 좋습니다. 이렇게 하면 대량 삽입 큐 오버플로로 인한 오류 가능성을 줄일 수 있는 자연스러운 제한 메커니즘 역할을 합니다.

### 데이터 수집 성능에서 인덱스 새로 고침 간격 변경의 영향

새로 고침 간격에 따라 수집된 데이터가 쿼리 및 집계에 표시되는 속도가 결정되지만 자주 새로 고침이 수행되면 데이터 수집 작업의 성능에 영향을 줄 수 있습니다. 기본 새로 고침 간격은 1초입니다. 새로 고침을 완전히 비활성화할 수 있지만 워크로드에 적절하지 않을 수 있습니다. 다양한 간격으로 시도해보고 최신 정보를 표시하려는 요구 사항에 대해 수집 성능과 균형을 이루는 최적 지점을 설정하여 실험할 수 있습니다.

이러한 영향의 예로, 3개의 데이터 노드 간에 분산된 7개의 분할된 데이터베이스가 있는 Elasticsearch 클러스터에서 데이터 수집 성능 테스트를 반복했습니다. 인덱스에는 단일 복제본이 있습니다. 각 데이터 노드는 데이터를 저장하는 데 SSD 지원 사용 후 삭제 저장소를 사용하며 D4 VM(28GB RAM, 프로세서 코어 8개)을 기반으로 합니다. 각 테스트는 1시간 동안 실행되었습니다.

이 테스트에서는 새로 고침 빈도가 기본값 1초로 설정되었습니다. 다음 표에서는 새로 고침 빈도를 30초마다 1번으로 줄인 별도의 실행과 이 테스트에 대한 처리량 및 응답 시간을 비교하여 보여 줍니다.

| 새로 고침 빈도 | 샘플 수 | 평균 응답 시간 – 성공 작업(밀리초) | 처리량 - 성공 작업(작업/초) |
|--------------|------------|----------------------------------------------------|---------------------------------------------------|
| 1초 | 93755 | 460 | 26\.0 |
| 30초 | 117758 | 365 | 32\.7 |

이 테스트에서 새로 고침 빈도를 낮추면 처리량이 18% 향상되고 평균 응답 시간은 21% 감소했습니다. Marvel을 사용하여 생성한 다음 그래프는 이러한 차이가 생기는 주된 이유를 보여 줍니다. 아래 그림은 1초 및 30초로 설정된 새로 고침 간격으로 발생한 인덱스 병합 동작을 보여줍니다.

인덱스 병합은 메모리 내 인덱스 세그먼트의 수가 너무 많아지는 것을 방지하기 위해 수행됩니다. 1초 새로 고침 간격 시 작은 세그먼트 수가 많이 생기는 반면(자주 병합해야 함) 30초 새로 고침 간격은 보다 최적으로 병합할 수 있는 큰 세그먼트를 적은 수로 생성합니다.

![](media/guidance-elasticsearch/data-ingestion-image15.png)

***인덱스 새로 고침 속도(1초)에 대한 인덱스 병합 활동***

![](media/guidance-elasticsearch/data-ingestion-image16.png)

***인덱스 새로 고침 속도(30초)에 대한 인덱스 병합 활동***

### 데이터 수집 성능에 복제본이 미치는 영향

복제본은 모든 복원력 있는 클러스터의 필수 기능이며 복제본을 사용하지 않을 경우 노드가 실패할 때 정보 손실 위험이 있습니다. 그러나 복제본에서 수행되는 디스크 및 네트워크 I/O 양이 증가하면 데이터가 수집되는 속도가 저하될 수 있습니다. 앞에서 설명한 이유로 대규모 데이터 업로드 작업 중에 복제를 일시적으로 사용하지 않도록 설정하는 것이 도움이 될 수 있습니다.

데이터 수집 성능 테스트는 세 가지 구성을 사용하여 반복되었습니다.

* 복제본이 없는 클러스터 사용

* 복제본이 1개 있는 클러스터 사용

* 복제본이 2개 있는 클러스터 사용

모든 경우에 클러스터는 3개 노드에 걸쳐 7개의 분할된 데이터베이스가 분산되어 있으며 이전 테스트 집합에 설명된 대로 VM을 구성하여 실행했습니다. 테스트 인덱스는 새로 고침 간격으로 30초를 사용했습니다.

다음 표에서는 비교를 위해 각 테스트의 응답 시간 및 처리량을 요약하여 보여 줍니다.

| 구성 | 샘플 수 | 평균 응답 시간 – 성공 작업(밀리초) | 처리량 - 성공 작업(작업/초) | 데이터 수집 오류 수 |
|---------------|------------|----------------------------------------------------|---------------------------------------------------|--------------------------|
| 복제본 0개 | 215451 | 200 | 59\.8 | 0 |
| 복제본 1개 | 117758 | 365 | 32\.7 | 0 |
| 복제본 2개 | 94218 | 453 | 26\.1 | 194262 |


복제본 수 증가에 따라 성능 저하가 명확하지만 세 번째 테스트에서는 대량의 데이터 수집 오류가 나타났음을 유의해야 합니다. 이러한 오류에 의해 생성된 메시지에는 대량 삽입 큐 오버플로로 인해 요청이 거부되었기 때문으로 나타났습니다. 이러한 거부는 매우 빨리 발생하므로 대량의 오류가 나타납니다.

> [AZURE.NOTE] 세 번째 테스트의 결과에서는 이와 같은 일시적인 오류가 발생할 때 지능형 재시도 전략을 사용하는 것의 중요성을 강조합니다. 단기간 백오프를 통해 대량 삽입 작업을 반복하기 전에 대량 삽입 큐를 비우도록 합니다.

다음 그래프 집합은 테스트 중에 응답 시간을 비교합니다. 각 사례에서 첫 번째 그래프는 전체 응답 시간을 보여주는 반면 두 번째 그래프는 가장 빠른 작업에 대한 응답 시간을 확대하여 보여 줍니다(첫 번째 그래프의 배율은 두 번째 그래프의 10배입니다). 응답 시간의 프로필이 세 개의 테스트 간에 어떻게 달라지는지 확인할 수 있습니다.

복제본이 없는 경우 대부분의 작업에 75ms ~ 750ms가 소요되었으며 가장 빠른 응답 시간은 약 25ms입니다.

![](media/guidance-elasticsearch/data-ingestion-image17.png)

복제본이 1개 있는 경우 가장 많이 채워진 작업 응답 시간은 125ms ~ 1250ms 범위였습니다. 복제본이 0개 있는 사례에서보다 빠른 응답 수는 적지만 가장 빠른 응답은 약 75ms가 소요되었습니다. 대부분의 일반적인 사례보다 훨씬 긴 시간이 소요되는 응답(1250ms 초과)도 훨씬 많았습니다.

![](media/guidance-elasticsearch/data-ingestion-image18.png)

복제본이 2개 있는 경우 가장 많이 채워진 응답 시간 범위는 200ms ~ 1500ms였지만 1개 복제본 테스트에서보다 최소 범위 미만의 결과 수는 훨씬 적었습니다. 그러나 상한 위의 결과 패턴은 1개 복제본 테스트와 매우 유사했습니다. 대량 삽입 큐 오버플로(50개 요청의 큐 길이 초과) 효과 때문일 가능성이 큽니다. 큐 오버플로를 더 빈번하게 일으키는 2개 복제본을 유지하기 위해서는 조사 작업에 과도한 응답 시간이 소요되는 것을 방지하는 추가 작업이 필요합니다. 이렇게 하면 긴 기간 소요되어 시간 제한 예외가 발생하거나 클라이언트 응용 프로그램의 응답성에 영향을 주는 대신 작업이 빨리 거부됩니다.

![](media/guidance-elasticsearch/data-ingestion-image19.png)

<span id="_The_Impact_of_1" class="anchor"><span id="_Impact_of_Increasing" class="anchor"></span></span>Marvel을 사용하여 대량 인덱스 큐에서 복제본 수가 미치는 영향을 확인할 수 있습니다. 아래 그림은 테스트 중에 대량 삽입 큐가 어떻게 채워지는지를 나타내는 Marvel의 데이터를 보여줍니다. 평균 큐 길이는 약 40개 요청이지만 정기적으로 급증하여 오버플로가 야기되고 그 결과 요청이 거부되었습니다.

![](media/guidance-elasticsearch/data-ingestion-image20.png)

***2개의 복제본이 있는 대량 인덱스 큐 크기 및 거부된 요청 수***

이 그림을 단일 복제본에 대한 결과를 보여주는 아래 그림과 비교해야 합니다. Elasticsearch 엔진은 약 25의 평균 큐 길이를 유지할 만큼 신속하게 요청을 처리할 수 있으며 큐 길이가 50개 요청을 초과하는 지점이 없으므로 작업이 거부되지 않습니다.

![](media/guidance-elasticsearch/data-ingestion-image21.png)

***1개의 복제본이 있는 대량 인덱스 큐 크기 및 거부된 요청 수***

## Elasticsearch에 데이터를 보내는 클라이언트에 대한 모범 사례

시스템 내부적으로 뿐만 아니라 클라이언트 응용 프로그램에서 시스템이 어떻게 사용되는지까지 다양한 성능 측면을 고려합니다. Elasticsearch는 데이터 수집 프로세스에서 활용할 수 있는 다양한 기능을 제공합니다. 예를 들어 문서에 대한 고유 식별자 생성, 문서 분석 수행 및 스크립트를 사용하여 데이터를 저장된 대로 변환 등의 기능이 있습니다. 그러나 이러한 기능은 모두 Elasticsearch 엔진에 부담을 주며 많은 경우 전송되기 전에 클라이언트 응용 프로그램이 더욱 효율적으로 수행할 수 있습니다.

> [AZURE.NOTE] 이 모범 사례 목록은 인덱스에 이미 저장된 기존 데이터를 수정하는 대신 새 데이터를 수집하는 것을 주로 다룹니다. 수집 워크로드는 Elasticsearch에 의해 추가 작업으로 수행되는 반면 데이터 수정은 삭제/추가 작업으로 수행됩니다. 인덱스의 문서는 변경할 수 없기 때문이며 따라서 문서를 수정하면 전체 문서를 새 버전으로 바꿔야 합니다. 기존 문서를 덮어쓰기 위해 HTTP PUT 요청을 수행하거나 쿼리를 추상화하는 Elasticsearch *업데이트* API를 사용하여 기존 문서를 가져오고 변경 내용을 병합한 후 PUT을 수행하여 새 문서를 저장할 수 있습니다.

또한 적절한 경우 다음과 같은 방법으로 구현하는 것이 좋습니다.

* 분석할 필요가 없는 인덱스 필드에 대해서는 텍스트 분석을 사용하지 않도록 합니다. 분석에는 특정 용어를 검색할 수 있는 쿼리를 사용하는 텍스트 토큰화 작업이 포함됩니다. 그러나 이 작업은 CPU 사용량이 많으므로 신중해야 합니다. Elasticsearch를 사용하여 로그 데이터를 저장하는 경우 상세한 로그 메시지를 토큰화하여 복잡한 검색을 허용하는 데 도움이 될 수 있습니다. 오류 코드 또는 식별자를 포함하는 기타 필드는 아마도 토큰화하지 않을 것입니다(예를 들어 오류 코드에 "3"을 포함하는 모든 메시지에 대한 정보를 얼마나 자주 요청할까요?). 다음 코드는 *systembase* 인덱스의 *로그* 형식에서 *이름* 및 *hostip*에 대한 분석을 사용하지 않도록 설정합니다.

	```http
	PUT /systembase
	{
		"settings" : {
			...
		},
		"logs" : {
			...
			"name": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"hostip": {
				"type": "string",
				"index" : "not_analyzed"
			},
			...
		}
	}
	```

* 필요하지 않은 경우 인덱스의 *\_all* 필드를 사용하지 않도록 설정합니다. *\_all* 필드는 분석 및 인덱싱을 위해 문서에 있는 다른 필드의 값을 연결합니다. 문서에서 모든 필드에 대해 일치시킬 수 있는 쿼리를 수행하는 데 유용합니다. 클라이언트가 명명된 필드에 대해 일치를 수행할 것으로 예상되는 경우 *\_all*을 사용하도록 설정하면 CPU 및 저장소 오버헤드만 발생합니다. 다음 예제는 *systembase* 인덱스의 *로그* 형식에서 *\_all* 필드를 사용하지 않도록 설정하는 방법을 보여 줍니다.

	```http
	PUT /systembase
	{
		"settings" : {
			...
		},
		"logs" : {
			"_all": {
				"enabled" : false
			},
			...,
		...
		}
	}
	```

    특정 필드의 정보만 포함하는 *\_all*의 선택적 버전을 만들 수 있습니다. 자세한 내용은 [\_all 필드를 사용하지 않도록 설정](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-all-field.html#disabling-all-field)을 참조하세요.

* 인덱스에서 동적 매핑의 사용을 지양합니다. 동적 매핑은 강력한 기능이기는 하지만 기존 인덱스에 새 필드를 추가하면 노드 간 인덱스 구조에 대한 변경을 조정해야 하며 이로 인해 인덱스가 일시적으로 잠금 상태가 될 수 있습니다. 동적 매핑은 필드 수와 그에 따른 인덱스 메타데이터 볼륨의 폭증을 야기할 수 있으므로 주의해야 합니다. 이는 곧 데이터 수집 및 쿼리 수행 시 저장소 요구 사항 및 I/O 증가로 이어집니다. 이러한 문제는 모두 성능에 영향을 줍니다. 동적 매핑을 사용하지 않도록 설정하고 인덱스 구조를 명시적으로 정의하는 것이 좋습니다. 자세한 내용은 [동적 필드 매핑](https://www.elastic.co/guide/en/elasticsearch/reference/current/dynamic-field-mapping.html#dynamic-field-mapping)을 참조하세요.

* 충돌 요구 사항을 충족하도록 워크로드를 분산하는 방법을 이해합니다. 데이터 수집은 다른 동시 작업(예: 사용자의 쿼리 수행)의 성능에 상당한 영향을 줄 수 있음을 항상 고려해야 합니다. 데이터 수집은 급증할 수 있으며 시스템에서 도착하는 모든 데이터를 즉시 사용하려고 하면 데이터 유입 시 이동하는 쿼리 속도가 느려질 수 있습니다. Elasticsearch는 대량 삽입 큐를 통해 수집 요청을 처리하는 속도를 조정하여 이러한 상황이 발생하는 것을 방지합니다(자세한 내용은 [제한 요소 결정 – CPU 사용률](#determining-limiting-factors-cpu-utilization) 섹션 참조). 하지만 이 메커니즘은 마지막 수단으로 처리되어야 하며 거부된 요청을 처리할 응용 프로그램 코드가 준비되지 않은 경우 데이터 손실 위험이 있습니다. 대신, [큐 기반 부하 평등화](https://msdn.microsoft.com/library/dn589783.aspx)와 같은 패턴을 사용하여 데이터가 Elasticsearch에 전달되는 속도를 제어하는 것이 좋습니다.

* 인덱스가 여러 복제본으로 구성된 경우, 특히 클러스터에 워크로드를 처리할 만큼 충분한 리소스가 있는지 확인합니다.

* 대량 삽입 API를 사용하여 문서의 대규모 배치를 업로드합니다. 대량 요청 크기를 적절하게 조정합니다. 배치가 크다고 해서 성능이 좋은 것은 아닐 때도 있으며 이로 인해 Elasticsearch 스레드 및 기타 리소스가 오버로드되어 다른 동시 작업이 지연될 수 있습니다. 대량 삽입 배치의 문서는 작업이 수행되는 동안 조정 노드에서 메모리에 보관됩니다. 각 배치의 실제 크기가 문서 수보다 더 중요합니다. Elasticsearch에서는 자체 조사에 대한 시작 지점으로 5MB ~ 15MB를 사용할 것을 권장하지만 이상적인 배치 크기를 도출하기 위한 정해진 규칙은 없습니다. 성능 테스트를 수행하여 자신의 시나리오 및 워크로드 혼합에 적합한 배치 크기를 설정합니다.

* 대량 삽입 요청은 단일 노드에 전달하기보다는 여러 노드 간에 배포해야 합니다. 모든 요청을 단일 노드에 전달하면 처리되는 각 대량 삽입 요청이 노드의 메모리에 저장됨에 따라 메모리 부족이 발생할 수 있습니다. 또한 요청을 다른 노드로 리디렉션함에 따라 네트워크 대기 시간이 증가할 수 있습니다.

* Elasticsearch는 데이터를 작성할 때 대부분 주 및 복제본 노드로 구성된 쿼럼을 사용합니다. 쿼럼에서 성공을 보고할 때까지 쓰기 작업이 완료되지 않습니다. 이 방법을 사용하면 네트워크 파티션 (실패) 이벤트로 인해 노드 대부분을 사용할 수 없는 경우 데이터를 쓰지 않도록 할 수 있습니다. 쿼럼을 사용하면 쓰기 작업의 성능이 저하될 수 있습니다. 데이터를 쓸 때 *일관성* 매개 변수를 *하나*로 설정하여 쿼럼 기반 쓰기를 사용하지 않도록 할 수 있습니다. 다음 예제에서는 새 문서를 추가하지만 주 분할된 데이터베이스에 쓰기가 완료되는 즉시 완료됩니다.

	```http
	PUT /my_index/my_data/104?consistency=one
	{
		"name": "Bert",
		"age": 23
	}
	```

	비동기 복제와 마찬가지로 쿼럼 기반 쓰기를 사용하지 않도록 설정하면 주 분할된 데이터베이스와 각 복제본 간에 불일치가 발생할 수 있습니다.

* 쿼럼을 사용할 때, Elasticsearch는 쿼럼에 도달할 수 없어 쓰기 작업을 중단할지 결정하기 전에 부족한 노드를 사용할 수 있을 때가지 대기합니다. 이 대기 기간은 시간 제한 쿼리 매개 변수(기본값은 1 분)에 의해 결정됩니다. 시간 제한 쿼리 매개 변수를 사용하여 이 설정을 수정할 수 있습니다. 아래 예제에서는 새 문서를 만들고 쿼럼이 응답할 때까지 최대 5초 기다렸다가 중단합니다.

	```http
	PUT /my_index/my_data/104?timeout=5s
	{
		"name": "Sid",
		"age": 27
	}
	```

	Elasticsearch를 통해 [외부에서 생성된](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#_version_types) 고유한 버전 번호도 사용할 수 있습니다.

* 인덱스의 *\_source* 필드는 사용하지 않는 것이 좋습니다. 이 필드는 문서를 저장할 때 사용된 원래 JSON 문서의 복사본을 포함합니다. 이 필드를 저장하면 추가 저장소 비용 및 디스크 I/O가 발생합니다. 하지만 이러한 비용은 문서 구조에 따라 미미할 수 있으며 *\_source* 필드를 사용하지 않도록 설정하면 클라이언트가 다음 작업을 수행할 수 없음도 알아야 합니다.

	* 업데이트 API를 사용하여 문서를 수정합니다.
	* 쿼리를 실행할 때 즉시 강조 표시를 수행합니다.
	* 데이터 인덱스를 다시 작성합니다.
	* 원본 문서를 확인하여 쿼리 및 집계를 디버깅합니다.

	다음 예제는 *systembase* 인덱스의 *로그* 형식에서 *\_source* 필드를 사용하지 않도록 설정합니다.

  ```http
  PUT /systembase
  {
		"settings" : {
			...
		},
		"logs" : {
			"_source": {
				"enabled": false
			},
			...,
		...
		}
  }
  ```

## Elasticsearch를 사용하여 데이터 수집 성능 테스트를 수행하기 위한 일반 지침

다음은 Elasticsearch를 사용하여 성능 테스트를 실행하고 결과를 분석할 때 고려해야 할 몇 가지 항목을 강조 표시합니다.

* 성능 테스트는 필연적으로 시간이 오래 걸리고 비용이 많이 드는 작업입니다. 최소한 디스크 및 네트워크에서 전송 속도, CPU 사용률, CPU 대기 시간 및 디스크 대기 시간(있을 경우)을 측정하는 통계를 수집합니다. 이를 통해 적절한 투자 수익과 함께 테스트 작업에 대한 빠른 피드백을 제공할 수 있습니다.

* 부하 테스트 도구에서 제공하는 모든 스크립팅 기능을 활용하여 달리 제공되지 않는 메트릭을 수집합니다. 예를 들어 Linux에는 *vmstat* 및 *iostat*와 같은 유틸리티를 사용하여 수집할 수 있는 안정적이고 유용한 성능 통계가 다양하게 있습니다. JMeter에서 스크립팅을 사용하여 이 데이터를 테스트 계획의 일환으로 캡처할 수 있습니다.

* 성능 엔지니어링은 대부분 안정적이고 반복 가능한 데이터를 기반으로 통계를 분석하는 것과 관련됩니다. 대략적인 수준의 메트릭에서 멈추지 마세요. 필요한 통찰력을 얻지 못합니다. 데이터를 통해 스스로 학습하고 빠른 피드백 루프를 통해 성능 엔지니어링을 개발 프로세스로 만드세요. 항상 동향 및 지난 결과/구성을 비교하여 통계를 살펴보세요. 정기적으로 이 작업을 수행하면 이해되는 데이터가 생성되며 자신의 워크로드로 반복할 수 있게 됩니다. 이를 통해 구성 및 배포 변경 시 효과를 평가할 수 있습니다.

* Marvel과 같은 도구를 사용하여 테스트 중에 클러스터 및 노드 성능을 모니터링하여 이해를 더욱 넓힐 수 있습니다. JMeter는 후속 분석을 위한 원시 데이터를 캡처하는 데 적용할 수 있지만 Marvel을 사용하면 성능이 어떻게 악화되고 있으며 결함 및 속도 저하의 가능한 원인은 무엇인지 실시간으로 느낄 수 있습니다. 또한 여러 부하 테스트 도구는 Elasticsearch의 내부 메트릭에 대한 가시성을 제공하지 않습니다. 인덱스 통계에 제공되는 인덱싱 처리량 속도, 병합 세그먼트 개수, GC 통계 및 제한 시간을 사용 및 비교합니다. 이 분석을 정기적으로 반복합니다.

* Marvel에서 노드 통계와 부하 테스트 도구 통계를 비교(디스크 및 네트워크 트래픽, CPU 사용률, 메모리 및 스레드 풀 사용량)하여 인프라에서 보고된 수치와 특정 Elasticsearch 통계 간 상관관계 패턴을 파악합니다.

* 일반적으로 *한 노드에 하나의 분할된 데이터베이스*를 성능 테스트를 위한 기준으로 하고 노드를 추가하여 응용 프로그램 비용을 평가합니다. 그러나 적은 수의 노드 및 분할된 데이터베이스를 기반으로 성능을 추론하는 방법에 전적으로 의존하지 마세요. 클러스터에서 동기화 및 통신 비용은 노드 및 분할된 데이터베이스 수가 많아질 경우 예측할 수 없는 효과를 나타낼 수 있습니다.

* 통계를 비교하기 위해 노드 간 분할된 데이터베이스 할당을 살펴보세요. 일부 노드에는 복제본과 분할된 데이터베이스 수가 적으며 따라서 리소스 사용률에 불균형이 생깁니다.

* 부하 테스트를 수행하는 경우 클러스터에 작업을 제출하기 위해 테스트 도구에서 사용하는 스레드 수를 오류가 발생할 때까지 늘립니다. 지속 가능한 처리량 테스트를 위해 테스트 수준을 합의된 *한계치* 미만으로 유지하는 것이 좋습니다. 오류율이 한계치를 초과하는 경우 복구 기능으로 인해 백 엔드 리소스에서 비용이 발생합니다. 이러한 상황에서 처리량은 불가피하게 감소됩니다.

* 시스템이 예기치 않은 대규모 급증 활동에 어떻게 반응하는지 시뮬레이트하기 위해 한계치에 해당하는 오류율을 생성하는 테스트를 실행하는 것이 좋습니다. 이렇게 하면 용량뿐만 아니라 복구 기능의 비용까지도 고려한 처리량 수치가 제공됩니다.

* 문서 수를 사용하여 성능 프로필을 평가하고 워크로드 패턴에 따라 문서를 재활용합니다. 더 많은 문서 추가됨에 따라 성능 프로필이 변경될 수 있음을 고려합니다.

* 사용하는 저장소에 대한 IOPS 및 전송 속도 제한에 대한 SLA를 알아야 합니다. 저장소 유형(SSD, 회전 미디어)에 따라 전송 속도가 달라집니다.

* CPU 성능은 디스크 및 네트워크 작업뿐만 아니라, 백 엔드 응용 프로그램이 분산형 처리에 프로세서 사용 미달을 야기할 수 있는 잠금 및 통신 메커니즘을 사용할 수 있기 때문에 저하될 수 있습니다.

* 2시간(몇 분이 아님) 이상 성능 테스트를 실행합니다. 인덱싱은 즉시 표시되지 않는 방법으로 성능에 영향을 줄 수 있습니다. 예를 들어 JVM 가비지 수집 통계 및 인덱싱 병합은 시간에 따라 성능 프로필을 변경할 수 있습니다.

* 인덱스를 새로 고치는 방법은 데이터 수집 처리량 및 클러스터 제한에 큰 영향을 줄 수 있습니다.

## 요약

데이터 볼륨 및 요청 수가 증가함에 따라 솔루션을 확장하는 방법을 이해해야 합니다. Azure에서 실행되는 Elasticsearch를 통해 수직적 및 수평적 확장이 가능합니다. 더 많은 리소스가 있는 큰 VM에서 실행하고 Elasticsearch 클러스터를 VM 네트워크 간에 배포할 수 있습니다. 다양한 옵션으로 결정이 어려울 수 있습니다. 소형 VM을 다수 사용하여 클러스터를 구현할지, 대형 VM을 소수 사용하여 클러스터를 구현할지, 중간 규모로 할지 어떤 방법이 더 비용 효율적인지 결정해야 합니다. 또한 각 인덱스에 얼마나 많은 분할된 데이터베이스를 포함할지, 데이터 수집 및 쿼리 성능을 비교하여 고려할 때 장단점은 무엇인지 결정해야 합니다. 분할된 데이터베이스가 노드 간에 배포되는 방식은 데이터 수집 처리량에 상당한 영향을 줄 수 있습니다. 분할된 데이터베이스 수가 많아지면 분할된 데이터베이스 내부에서 발생하는 내부 경합이 줄어들 수 있지만 많은 분할된 데이터베이스 사용으로 인해 클러스터에서 오버헤드가 발생하므로 이 이점 간에 균형을 맞추어야 합니다. 이러한 질문에 효과적으로 답하려면 시스템을 테스트하여 가장 적합한 전략을 결정할 준비가 되어야 합니다.

데이터 수집 워크로드의 경우 디스크 I/O 하위 시스템의 성능이 중요한 요소입니다. SSD를 사용하면 쓰기 작업의 디스크 대기 시간을 줄여 처리량을 높일 수 있습니다. 노드에 방대한 양의 디스크 공간이 필요하지 않으면 프리미엄 저장소를 지원하는 비싼 VM 대신, 사용 후 삭제 저장소가 있는 표준 VM을 사용하는 것이 좋습니다.

## 부록: 대량 로드 데이터 수집 성능 테스트

이 부록에서는 Elasticsearch 클러스터에 대해 수행하는 성능 테스트를 설명합니다. 별도의 VM 집합에서 실행되는 JMeter를 사용하여 테스트를 실행했습니다. 테스트 환경 구성에 대한 자세한 내용은 [Azure에서 Elasticsearch에 대한 성능 테스트 환경 만들기][]에 설명되어 있습니다. 직접 테스트를 수행하려면 고유한 JMeter 테스트 계획을 수동으로 만들거나 별도로 제공되는 자동화된 테스트 스크립트를 사용할 수 있습니다. 자세한 내용은 [자동화된 Elasticsearch 성능 테스트 실행][]을 참조하세요.

데이터 수집 워크로드는 대량 삽입 API를 사용하여 대규모 업로드를 수행했습니다. 이 인덱스의 목적은 후속 검색 및 분석을 위한 시스템 이벤트를 나타내는 로그 데이터를 수신하는 리포지토리를 시뮬레이션하는 것입니다. 각 문서는 이름이 *systembase*인 단일 인덱스에 저장되며 형식은 *로그*입니다. 모든 문서에는 다음 표에 설명된 것과 동일한 고정된 스키마가 있습니다.

| 필드 | 데이터 형식 | 예 |
|---------------|---------------------|-----------------------------------|
| @timestamp | datetime | 2013-12-11T08:01:45.000Z |
| name | string | checkout.payment |
| message | string | 들어오는 요청 메시지 |
| severityCode | 정수 | 1 |
| severity | string | info |
| hostname | string | sixshot |
| hostip | string(ip 주소) | 10\.0.0.4 |
| pid | int | 123 |
| tid | int | 4325 |
| appId | string(uuid) | {00000000-0000-0000-000000000000} |
| appName | string | mytestapp |
| appVersion | string | 0\.1.0.1234 |
| type | int | 5 |
| subtype | int | 1 |
| CorrelationId | GUID | {00000000-0000-0000-000000000000} |
| os | string | Linux |
| osVersion | string | 4\.1.1 |
| 매개 변수 | [ ] | {key:value,key:value} |

인덱스를 만드는 데 다음 요청을 사용할 수 있습니다. 많은 테스트에서 *number\_of\_replicas*, *refresh\_interval* 및 *number\_of\_shards* 설정은 아래 표시된 값에서 다양하게 설정할 수 있습니다.

> [AZURE.IMPORTANT] 각 테스트가 실행되기 전에 인덱스가 삭제되고 다시 작성되었습니다.

```http
PUT /systembase
{
	"settings" : {
		"number_of_replicas": 1,
		"refresh_interval": "30s",
		"number_of_shards": "5"
	},
	"logs" : {
		"properties" : {
			"@timestamp": {
			"type": "date",
			"index" : "not_analyzed"
			},
			"name": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"message": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"severityCode": {
				"type": "integer",
				"index" : "not_analyzed"
			},
			"severity": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"hostname": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"hostip": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"pid": {
				"type": "integer",
				"index" : "not_analyzed"
			},
			"tid": {
				"type": "integer",
				"index" : "not_analyzed"
			},
			"appId": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"appName": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"appVersion": {
				"type": "integer",
				"index" : "not_analyzed"
			},
			"type": {
				"type": "integer",
				"index" : "not_analyzed"
			},
			"subtype": {
				"type": "integer",
				"index" : "not_analyzed"
			},
			"correlationId": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"os": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"osVersion": {
				"type": "string",
				"index" : "not_analyzed"
			},
			"parameters": {
				"type": "string",     
				"index" : "not_analyzed"
			}
		}
	}
}
```

각 대량 삽입 배치는 1000개의 문서로 구성되었습니다. 각 문서는 *severityCode*, *hostname*, *hostip*, *pid*, *tid*, *appName*, *appVersion*, *type*, *subtype*, *correlationId* 필드에 대한 임의 값의 조합을 기반으로 생성되었으며 *name*, *message*, *severity*, *os*, *osVersion*, *parameters*, *data1*, *data2* 필드에 대한 고정 조건 집합에서 텍스트를 임의 선택하여 생성되었습니다. 데이터를 업로드하는 데 사용된 클라이언트 응용 프로그램 인스턴스 수는 성공적인 입력 볼륨을 최대화하기 위해 신중하게 선택했습니다. 전체 결과에서 클러스터가 일시적인 결함의 영향을 줄이고 안정화할 수 있도록 2시간 동안 테스트를 실행했습니다. 여기서 일부 테스트는 거의 15억 개의 문서를 업로드했습니다.

데이터는 JMeter 테스트 계획에서 스레드 그룹에 추가된 사용자 지정 JUnit 요청 샘플러를 사용하여 동적으로 생성되었습니다. JUnit 코드는 Eclipse IDE의 JUnit 테스트 사례 템플릿을 사용하여 작성되었습니다.

> [AZURE.NOTE] JMeter용으로 JUnit 테스트를 만드는 방법은 [Elasticsearch 성능 테스트를 위해 JMeter JUnit 샘플러 배포][]를 참조하세요.

다음 코드 조각은 Elasticsearch 1.7.3을 테스트하는 Java 코드를 보여 줍니다. 이 예에서 JUnit 테스트 클래스 이름은 *ElasticsearchLoadTest2*입니다.

```java
/* Java */
package elasticsearchtest2;

	import static org.junit.Assert.*;

	import org.junit.*;

	import java.util.*;

	import java.io.*;

	import org.elasticsearch.action.bulk.*;
	import org.elasticsearch.common.transport.*;
	import org.elasticsearch.client.transport.*;
	import org.elasticsearch.common.settings.*;
	import org.elasticsearch.common.xcontent.*;

	public class ElasticsearchLoadTest2 {

		private String [] names={"checkout","order","search","payment"};
		private String [] messages={"Incoming request from code","incoming operation succeeded with code","Operation completed time","transaction performed"};
		private String [] severity={"info","warning","transaction","verbose"};
		private String [] apps={"4D24BD62-20BF-4D74-B6DC-31313ABADB82","5D24BD62-20BF-4D74-B6DC-31313ABADB82","6D24BD62-20BF-4D74-B6DC-31313ABADB82","7D24BD62-20BF-4D74-B6DC-31313ABADB82"};

		private String hostname = "";
		private String indexstr = "";
		private String typestr = "";
		private int port = 0;
		private int itemsPerInsert = 0;
		private String clustername = "";
		private static Random rand=new Random();

		@Before
		public void setUp() throws Exception {
		}

		public ElasticsearchLoadTest2(String paras) {
		* Paras is a string containing a set of comma separated values for:
			hostname
			indexstr
			typestr
			port
			clustername
			node
			itemsPerInsert
		*/

			// Note: No checking/validation is performed

			String delims = "[ ]*,[ ]*"; // comma surrounded by zero or more spaces
			String[] items = paras.split(delims);

			hostname = items[0];
			indexstr = items[1];
			typestr = items[2];
			port = Integer.parseInt(items[3]);
			clustername = items[4];
			itemsPerInsert = Integer.parseInt(items[5]);

			if (itemsPerInsert == 0)
				itemsPerInsert = 1000;
			}

		@After
		public void tearDown() throws Exception {
		}

		@Test
		public void BulkBigInsertTest() throws IOException {

			Settings settings = ImmutableSettings.settingsBuilder().put("cluster.name", clustername).build();

			TransportClient client;
			client = new TransportClient(settings);

			try {
				client.addTransportAddress(new InetSocketTransportAddress(hostname, port));
				BulkRequestBuilder bulkRequest = client.prepareBulk();
				Random random = new Random();
				char[] exmarks = new char[12000];
				Arrays.fill(exmarks, 'x');
				String dataString = new String(exmarks);

				for(int i=1; i &lt; itemsPerInsert; i++){
					random.nextInt(10);
					int host=random.nextInt(20);

					bulkRequest.add(client.prepareIndex(indexstr, typestr).setSource(XContentFactory.jsonBuilder().startObject()
						.field("@timestamp", new Date())
						.field("name", names[random.nextInt(names.length)])
						.field("message", messages[random.nextInt(messages.length)])
						.field("severityCode", random.nextInt(10))
						.field("severity", severity[random.nextInt(severity.length)])
						.field("hostname", "Hostname"+host)
						.field("hostip", "10.1.0."+host)
						.field("pid",random.nextInt(10))
						.field("tid",random.nextInt(10))
						.field("appId", apps[random.nextInt(apps.length)])
						.field("appName", "application" + host)
						.field("appVersion", random.nextInt(5))
						.field("type", random.nextInt(6))
						.field("subtype", random.nextInt(6))
						.field("correlationId", UUID.randomUUID().toString())
						.field("os", "linux")
						.field("osVersion", "14.1.5")
						.field("parameters", "{key:value,key:value}")
						.field("data1",dataString)
						.field("data2",dataString)
					.endObject()));
				}

				BulkResponse bulkResponse = bulkRequest.execute().actionGet();
				assertFalse(bulkResponse.hasFailures());
			}
			finally {
				client.close();
			}
		}

		@Test
		public void BulkDataInsertTest() throws IOException {
			Settings settings = ImmutableSettings.settingsBuilder().put("cluster.name", clustername).build();

			TransportClient client;
			client = new TransportClient(settings);

			try {
				client.addTransportAddress(new InetSocketTransportAddress(hostname, port));
				BulkRequestBuilder bulkRequest = client.prepareBulk();

				for(int i=1; i&lt; itemsPerInsert; i++){
					rand.nextInt(10);
					int host=rand.nextInt(20);

					bulkRequest.add(client.prepareIndex(indexstr, typestr).setSource(XContentFactory.jsonBuilder().startObject()
						.field("@timestamp", new Date())
						.field("name", names[rand.nextInt(names.length)])
						.field("message", messages[rand.nextInt(messages.length)])
						.field("severityCode", rand.nextInt(10))
						.field("severity", severity[rand.nextInt(severity.length)])
						.field("hostname", "Hostname" + host)
						.field("hostip", "10.1.0."+host)
						.field("pid",rand.nextInt(10))
						.field("tid",rand.nextInt(10))
						.field("appId", apps[rand.nextInt(apps.length)])
						.field("appName", "application"+host)
						.field("appVersion", rand.nextInt(5))
						.field("type", rand.nextInt(6))
						.field("subtype", rand.nextInt(6))
						.field("correlationId", UUID.randomUUID().toString())
						.field("os", "linux")
						.field("osVersion", "14.1.5")
						.field("parameters", "{key:value,key:value}")
					.endObject()));
				}

				BulkResponse bulkResponse = bulkRequest.execute().actionGet();
				assertFalse(bulkResponse.hasFailures());
			}
			finally {
				client.close();
			}
		}
	}
```

사설 *문자열* 배열 *이름*, *메시지*, *심각도* 및 *앱*은 항목이 임의로 선택되는 값의 작은 집합을 포함합니다. 각 문서에 대한 나머지 데이터 항목은 런타임에 생성됩니다.

*문자열* 매개 변수를 사용하는 생성자는 JMeter에서 호출되며 문자열에 전달된 값은 JUnit 요청 샘플러 구성의 일부로 지정됩니다. 이 JUnit 테스트에서 *문자열* 매개 변수는 다음 정보를 포함해야 합니다.

* **호스트 이름**. Azure 부하 분산 장치의 이름 또는 IP 주소입니다. 부하 분산 장치는 클러스터의 데이터 노드 간에 요청을 분산하려고 합니다. 부하 분산 장치를 사용하지 않는 경우 클러스터에서 노드의 주소를 지정할 수 있지만 모든 요청이 해당 노드로 전달되고 이로 인해 병목 현상이 발생할 수 있습니다.

* **Indexstr**. JUnit 테스트에 의해 생성된 데이터가 추가되는 인덱스의 이름입니다. 위에서 설명한 대로 인덱스를 만든 경우 이 값은 *systembase*입니다.

* **Typestr**. 인덱스에서 데이터가 저장되는 형식입니다. 위에서 설명한 대로 인덱스를 만든 경우 이 값은 *로그*입니다.

* **포트**. 호스트에서 연결할 포트입니다. 대부분의 경우에서 9300으로 설정해야 합니다(Elasticsearch가 클라이언트 API 요청을 수신 대기하는 데 사용하는 포트. 9200은 HTTP 요청에만 사용).

* **Clustername**. 인덱스를 포함하는 Elasticsearch 클러스터의 이름입니다.

* **ItemsPerInsert**. 각 대량 삽입 배치에서 추가할 문서 수를 나타내는 숫자 매개 변수입니다. 기본 배치 크기는 1000입니다.

JMeter에서 JUnit 샘플러를 구성하는 데 사용된 JUnit 요청 페이지에서 생성자 문자열에 대한 데이터를 지정합니다. 다음 이미지는 예를 보여 줍니다.

![](media/guidance-elasticsearch/data-ingestion-image22.png)

*BulkInsertTest* 및 *BigBulkInsertTest* 메서드는 데이터를 생성하고 업로드하는 실제 작업을 수행합니다. 두 메서드는 매우 비슷합니다. Elasticsearch 클러스터에 연결된 후 문서의 배치를 만듭니다(*ItemsPerInsert* 생성자 문자열 매개 변수로 결정됨). 이 문서는 Elasticsearch 대량 API를 사용하여 인덱스에 추가됩니다. 두 메서드 간의 차이점은 *BulkInsertTest* 메서드의 업로드에서는 각 문서에서 *data1* 및 *data2* 문자열 필드가 생략되지만 *BigBulkInsertTest* 메서드에서는 12000자의 문자열로 채워집니다. JMeter의 JUnit 요청 페이지에서 *테스트 메서드* 상자를 사용하여 실행할 메서드를 선택합니다(이전 그림에 강조 표시됨).

> [AZURE.NOTE] 여기에 있는 샘플 코드는 Elasticsearch 1.7.3 전송 클라이언트 라이브러리를 사용합니다. Elasticsearch 2.0.0 이상을 사용하는 경우 선택한 버전에 적합한 라이브러리를 사용해야 합니다. Elasticsearch 2.0.0 전송 클라이언트 라이브러리에 대한 자세한 내용은 Elasticsearch 웹 사이트의 [클라이언트 전송](https://www.elastic.co/guide/en/elasticsearch/client/java-api/2.0/transport-client.html) 페이지를 참조하세요.

[Azure의 Elasticsearch에서 복원력 및 복구 구성]: guidance-elasticsearch-configuring-resilience-and-recovery.md
[Azure에서 Elasticsearch에 대한 성능 테스트 환경 만들기]: guidance-elasticsearch-creating-performance-testing-environment.md
[자동화된 Elasticsearch 성능 테스트 실행]: guidance-elasticsearch-running-automated-performance-tests.md
[Elasticsearch 성능 테스트를 위해 JMeter JUnit 샘플러 배포]: guidance-elasticsearch-deploying-jmeter-junit-sampler.md

<!---HONumber=AcomDC_0224_2016-->