---
title: Azure VM에 대한 TCP/IP 성능 조정 | 마이크로 소프트 문서
description: 다양한 일반적인 TCP/IP 성능 튜닝 기술과 Azure VM과의 관계에 대해 알아봅니다.
services: virtual-network
documentationcenter: na
author: rimayber
manager: paragk
editor: ''
ms.assetid: ''
ms.service: virtual-network
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: infrastructure-services
ms.date: 04/02/2019
ms.author: rimayber
ms.reviewer: dgoddard, stegag, steveesp, minale, btalb, prachank
ms.openlocfilehash: bb23484903ac3ce129c6e7a7a27e0765c227fb1d
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 03/27/2020
ms.locfileid: "68297779"
---
# <a name="tcpip-performance-tuning-for-azure-vms"></a>Azure VM에 대한 TCP/IP 성능 조정

이 문서에서는 일반적인 TCP/IP 성능 튜닝 기술과 Azure에서 실행되는 가상 컴퓨터에 사용할 때 고려해야 할 몇 가지 사항을 설명합니다. 그것은 기술의 기본 개요를 제공하고 그들이 조정할 수있는 방법을 탐구한다.

## <a name="common-tcpip-tuning-techniques"></a>일반적인 TCP/IP 튜닝 기술

### <a name="mtu-fragmentation-and-large-send-offload"></a>MTU, 조각화 및 대형 송신 오프로드

#### <a name="mtu"></a>MTU

최대 전송 장치(MTU)는 네트워크 인터페이스를 통해 전송될 수 있는 바이트단위로 지정된 가장 큰 크기 프레임(패킷)입니다. MTU는 구성 가능한 설정입니다. Azure VM에서 사용되는 기본 MTU와 전 세계 대부분의 네트워크 장치에서 기본 설정은 1,500바이트입니다.

#### <a name="fragmentation"></a>조각화

조각화는 네트워크 인터페이스의 MTU를 초과하는 패킷을 보낼 때 발생합니다. TCP/IP 스택 인터페이스의 MTU를 준수 하는 작은 조각 (조각)으로 패킷을 나누기 합니다. 조각화는 IP 계층에서 발생하며 기본 프로토콜(예: TCP)과 독립적입니다. MTU가 1,500인 네트워크 인터페이스를 통해 2,000바이트 패킷이 전송되면 패킷은 1,500바이트 패킷 1개, 500바이트 패킷 하나로 세분화됩니다.

원본과 대상 사이의 경로에 있는 네트워크 장치는 MTU를 초과하는 패킷을 삭제하거나 패킷을 더 작은 조각으로 조각화할 수 있습니다.

#### <a name="the-dont-fragment-bit-in-an-ip-packet"></a>IP 패킷의 조각 화 안 하기

DF(조각 방지) 비트는 IP 프로토콜 헤더의 플래그입니다. DF 비트는 송신자와 수신기 사이의 경로에 있는 네트워크 장치가 패킷을 조각화해서는 안 함을 나타냅니다. 이 비트는 여러 가지 이유로 설정할 수 있습니다. (한 가지 예는 이 문서의 "경로 MTU 검색" 섹션을 참조하십시오.) 네트워크 장치가 조각 방지 비트 집합이 있는 패킷을 수신하고 해당 패킷이 장치의 인터페이스 MTU를 초과하는 경우 표준 동작은 장치가 패킷을 삭제하는 것입니다. 장치는 ICMP 조각화 필요한 메시지를 패킷의 원래 원본소스로 다시 보냅니다.

#### <a name="performance-implications-of-fragmentation"></a>조각화의 성능 영향

조각화는 성능에 부정적인 영향을 미칠 수 있습니다. 성능에 미치는 영향을 미치는 주된 이유 중 하나는 패킷의 조각화 및 재조립이 CPU/메모리에 미치는 영향입니다. 네트워크 장치가 패킷을 조각화해야 하는 경우 조각화를 수행하기 위해 CPU/메모리 리소스를 할당해야 합니다.

패킷을 다시 조립할 때도 마찬가지입니다. 네트워크 장치는 원래 패킷으로 다시 어셈블할 수 있도록 모든 조각을 수신할 때까지 저장해야 합니다. 이 조각화 및 재조립 프로세스는 대기 시간을 유발할 수도 있습니다.

조각화의 다른 부정적인 성능 의미는 조각난 패킷이 순서에 따라 도착할 수 있다는 것입니다. 패킷이 순서에 따라 수신되면 일부 유형의 네트워크 장치가 패킷을 삭제할 수 있습니다. 이 경우 전체 패킷을 다시 전송해야 합니다.

일반적으로 네트워크 방화벽과 같은 보안 장치나 네트워크 장치의 수신 버퍼가 소진되면 조각이 삭제됩니다. 네트워크 장치의 수신 버퍼가 소진되면 네트워크 장치가 조각난 패킷을 다시 어셈블하려고 시도하지만 패킷을 저장하고 다시 가정할 리소스가 없습니다.

조각화는 부정적인 작업으로 볼 수 있지만 인터넷을 통해 다양한 네트워크를 연결할 때 조각화에 대한 지원이 필요합니다.

#### <a name="benefits-and-consequences-of-modifying-the-mtu"></a>MTU 수정의 이점과 결과

일반적으로 MTU를 늘려 보다 효율적인 네트워크를 만들 수 있습니다. 전송되는 모든 패킷에는 원래 패킷에 추가된 헤더 정보가 있습니다. 조각화로 인해 더 많은 패킷이 생성되면 헤더 오버헤드가 많아지므로 네트워크 효율성이 떨어집니다.

예를 들면 다음과 같습니다. 이더넷 헤더 크기는 프레임 일관성을 보장하기 위해 14바이트와 4바이트 프레임 검사 시퀀스입니다. 2,000바이트 패킷 이 하나가 전송되면 네트워크에 18바이트의 이더넷 오버헤드가 추가됩니다. 패킷이 1,500바이트 패킷과 500바이트 패킷으로 조각화된 경우 각 패킷에는 총 36바이트의 이더넷 헤더가 18바이트입니다.

MTU를 늘리면 반드시 더 효율적인 네트워크가 생성되는 것은 아닙니다. 응용 프로그램이 500바이트 패킷만 보내는 경우 MTU가 1,500바이트 또는 9,000바이트인지 여부에 관계없이 동일한 헤더 오버헤드가 존재합니다. 네트워크는 MTU의 영향을 받는 더 큰 패킷 크기를 사용하는 경우에만 더 효율적입니다.

#### <a name="azure-and-vm-mtu"></a>Azure 및 VM MTU

Azure VM의 기본 MTU는 1,500바이트입니다. Azure 가상 네트워크 스택은 1,400바이트로 패킷을 조각화하려고 시도합니다.

VM의 MTU가 1,500인 경우에도 1,400바이트로 패킷을 조각화하기 때문에 가상 네트워크 스택은 본질적으로 비효율적이지 않습니다. 네트워크 패킷의 큰 비율은 1,400 또는 1,500바이트보다 훨씬 작습니다.

#### <a name="azure-and-fragmentation"></a>Azure 및 조각화

가상 네트워크 스택은 원래 조각된 순서로 도착하지 않는 조각화된 패킷인 "순서 가 없는 조각"을 삭제하도록 설정되어 있습니다. 이러한 패킷은 2018년 11월에 FragmentSmack라는 네트워크 보안 취약점으로 인해 주로 삭제됩니다.

FragmentSmack는 리눅스 커널이 조각난 IPv4 및 IPv6 패킷의 재조립을 처리하는 방식의 결함입니다. 원격 공격자는 이 결함을 사용하여 비용이 많이 드는 조각 재조립 작업을 트리거할 수 있으며, 이로 인해 CPU가 증가하고 대상 시스템에서 서비스 거부가 발생할 수 있습니다.

#### <a name="tune-the-mtu"></a>MTU 조정

다른 운영 체제에서도 마찬가지로 Azure VM MTU를 구성할 수 있습니다. 그러나 MTU를 구성할 때 위에서 설명한 Azure에서 발생하는 조각화를 고려해야 합니다.

고객이 VMM2를 늘리도록 권장하지 않습니다. 이 설명은 Azure에서 MTU를 구현하고 조각화를 수행하는 방법에 대한 세부 정보를 설명하기 위한 것입니다.

> [!IMPORTANT]
>MTU를 늘리면 성능이 향상되지 않으며 응용 프로그램 성능에 부정적인 영향을 줄 수 있습니다.
>
>

#### <a name="large-send-offload"></a>큰 송신 오프로드

LSO(대형 송신 오프로드)는 패킷 의 세분화를 이더넷 어댑터로 오프로드하여 네트워크 성능을 향상시킬 수 있습니다. LSO를 사용하도록 설정하면 TCP/IP 스택은 큰 TCP 패킷을 만들고 전달하기 전에 분할을 위해 이더넷 어댑터로 보냅니다. LSO의 장점은 CPU가 패킷을 MTU에 맞는 크기로 분할하지 않고 해당 처리를 하드웨어에서 수행하는 이더넷 인터페이스로 오프로드할 수 있다는 것입니다. LSO의 이점에 대해 자세히 알아보려면 [대규모 송신 오프로드 지원](https://docs.microsoft.com/windows-hardware/drivers/network/performance-in-network-adapters#supporting-large-send-offload-lso)을 참조하십시오.

LSO를 사용하도록 설정하면 Azure 고객은 패킷 캡처를 수행할 때 큰 프레임 크기를 볼 수 있습니다. 이러한 큰 프레임 크기로 인해 일부 고객은 조각화가 발생하거나 그렇지 않을 때 큰 MTU가 사용되고 있다고 생각할 수 있습니다. LSO를 사용하면 이더넷 어댑터가 TCP/IP 스택에 더 큰 최대 세그먼트 크기(MSS)를 보급하여 더 큰 TCP 패킷을 만들 수 있습니다. 이 전체 비분할 프레임은 이더넷 어댑터로 전달되고 VM에서 수행되는 패킷 캡처에 표시됩니다. 그러나 패킷은 이더넷 어댑터의 MTU에 따르면 이더넷 어댑터에 의해 많은 작은 프레임으로 세분화됩니다.

### <a name="tcp-mss-window-scaling-and-pmtud"></a>TCP MSS 윈도우 스케일링 및 PMTUD

#### <a name="tcp-maximum-segment-size"></a>TCP 최대 세그먼트 크기

TCP 최대 세그먼트 크기(MSS)는 TCP 세그먼트의 크기를 제한하는 설정으로 TCP 패킷의 조각화를 방지합니다. 운영 체제는 일반적으로 이 수식을 사용하여 MSS를 설정합니다.

`MSS = MTU - (IP header size + TCP header size)`

IP 헤더와 TCP 헤더는 각각 20바이트 또는 총 40바이트입니다. 따라서 MTU가 1,500인 인터페이스에는 MSS가 1,460입니다. 그러나 MSS는 구성 할 수 있습니다.

이 설정은 TCP 세션이 소스와 대상 간에 설정될 때 TCP 3방향 핸드셰이크에 동의됩니다. 양측은 MSS 값을 보내고 둘 중 아래쪽은 TCP 연결에 사용됩니다.

원본 및 대상의 MCO가 MSS 값을 결정하는 유일한 요소는 아닙니다. Azure VPN 게이트웨이를 비롯한 VPN 게이트웨이와 같은 중개 네트워크 장치는 최적의 네트워크 성능을 보장하기 위해 소스 및 대상과 독립적으로 MTU를 조정할 수 있습니다.

#### <a name="path-mtu-discovery"></a>경로 MTU 디스커버리

MSS는 협상되지만 사용할 수 있는 실제 MSS를 나타내지 않을 수 있습니다. 이는 원본과 대상 사이의 경로에 있는 다른 네트워크 장치가 원본 및 대상보다 MTU 값이 낮을 수 있기 때문입니다. 이 경우 MTU가 패킷보다 작은 장치는 패킷을 삭제합니다. 장치는 MTU를 포함하는 ICMP 조각화 필요한 (유형 3, 코드 4) 메시지를 다시 보냅니다. 이 ICMP 메시지를 사용하면 소스 호스트가 경로 MTU를 적절하게 줄일 수 있습니다. 이 프로세스를 경로 MTU 검색(PMTUD)이라고 합니다.

PMTUD 프로세스는 비효율적이며 네트워크 성능에 영향을 줍니다. 네트워크 경로의 MTU를 초과하는 패킷을 전송하면 더 낮은 MSS로 패킷을 다시 전송해야 합니다. 보낸 사용자가 ICMP 조각화 필요한 메시지를 받지 못하는 경우 경로의 네트워크 방화벽(일반적으로 *PMTUD 블랙홀이라고*함)으로 인해 보낸 사람은 MSS를 낮게 해야 한다는 것을 모르고 패킷을 계속 다시 전송합니다. 따라서 Azure VM MTU를 늘리는 것은 권장하지 않습니다.

#### <a name="vpn-and-mtu"></a>VPN 및 MTU

캡슐화를 수행하는 VM(예: IPsec VPN)을 사용하는 경우 패킷 크기 및 MTU와 관련하여 몇 가지 추가 고려 사항이 있습니다. VPN은 패킷에 헤더를 더 추가하여 패킷 크기를 늘리고 더 작은 MSS를 필요로 합니다.

Azure의 경우 TCP MSS 클램핑을 1,350바이트로 설정하고 터널 인터페이스 MTU를 1,400으로 설정하는 것이 좋습니다. 자세한 내용은 VPN [장치 및 IPSec/IKE 매개 변수 페이지를](https://docs.microsoft.com/azure/vpn-gateway/vpn-gateway-about-vpn-devices)참조하십시오.

### <a name="latency-round-trip-time-and-tcp-window-scaling"></a>대기 시간, 왕복 시간 및 TCP 창 크기 조정

#### <a name="latency-and-round-trip-time"></a>대기 시간 및 왕복 시간

네트워크 대기 시간은 광섬유 네트워크를 통해 빛의 속도에 의해 제어됩니다. TCP의 네트워크 처리량은 두 네트워크 장치 간의 왕복 시간(RTT)에 의해 효과적으로 제어됩니다.

| | | | |
|-|-|-|-|
|**경로**|**Distance**|**편도 시간**|**Rtt**|
|뉴욕 - 샌프란시스코|4,148 km|21ms|42 ms|
|뉴욕 - 런던|5,585 km|28 ms|56 ms|
|뉴욕 - 시드니|15,993 km|80ms|160 ms|

이 표는 두 위치 사이의 직선 거리를 보여 주십습니다. 네트워크에서 거리는 일반적으로 직선 거리보다 깁니다. 다음은 빛의 속도에 의해 제어되는 최소 RTT를 계산하는 간단한 수식입니다.

`minimum RTT = 2 * (Distance in kilometers / Speed of propagation)`

전파 속도에 200을 사용할 수 있습니다. 이 거리는 미터 단위로 1밀리초 단위로 이동합니다.

뉴욕을 샌프란시스코로 가봅시다. 직선 거리는 4,148km입니다. 방정식에 해당 값을 연결하면 다음을 얻을 수 있습니다.

`Minimum RTT = 2 * (4,148 / 200)`

방정식의 출력은 밀리초단위입니다.

최상의 네트워크 성능을 얻으려면 논리적 옵션은 네트워크 간의 최단 거리가 있는 대상을 선택하는 것입니다. 또한 가상 네트워크를 설계하여 트래픽 경로를 최적화하고 대기 시간을 줄여야 합니다. 자세한 내용은 이 문서의 "네트워크 디자인 고려 사항" 섹션을 참조하십시오.

#### <a name="latency-and-round-trip-time-effects-on-tcp"></a>TCP에 대한 대기 시간 및 왕복 시간 효과

왕복 시간은 최대 TCP 처리량에 직접적인 영향을 미칩니다. TCP 프로토콜에서 *창 크기는* 보낸 사람이 수신자로부터 승인을 받아야 하기 전에 TCP 연결을 통해 전송될 수 있는 최대 트래픽 양입니다. TCP MSS가 1,460으로 설정되고 TCP 창 크기가 65,535로 설정된 경우 발신자는 수신자로부터 승인을 받기 전에 45개의 패킷을 보낼 수 있습니다. 보낸 사람은 승인을 받지 못하면 데이터를 다시 전송합니다. 수식은 다음과 같습니다.

`TCP window size / TCP MSS = packets sent`

이 예제에서는 65,535 / 1,460이 45로 반올림됩니다.

데이터의 안정적인 전달을 보장하는 메커니즘인 이 "승인 대기" 상태는 RTT가 TCP 처리량에 영향을 주는 원인입니다. 보낸 사람의 승인 대기가 길어질수록 더 많은 데이터를 보내기 전에 기다려야 하는 시간도 길어집니다.

단일 TCP 연결의 최대 처리량을 계산하는 수식은 다음과 같습니다.

`Window size / (RTT latency in milliseconds / 1,000) = maximum bytes/second`

이 표에는 단일 TCP 연결의 최대 초당 처리량이 표시됩니다. 가독성을 위해 측정 단위에 메가바이트가 사용됩니다.

| | | | |
|-|-|-|-|
|**TCP 창 크기(바이트)**|**RTT 대기 시간(ms)**|**최대 메가바이트/초 처리량**|**최대 메가비트/초 처리량**|
|65,535|1|65.54|524.29|
|65,535|30|2.18|17.48|
|65,535|60|1.09|8.74|
|65,535|90|.73|5.83|
|65,535|120|.55|4.37|

패킷이 손실되면 보낸 사람은 이미 보낸 데이터를 다시 전송하는 동안 TCP 연결의 최대 처리량이 줄어듭니다.

#### <a name="tcp-window-scaling"></a>TCP 창 크기 조정

TCP 창 크기 조정은 승인이 필요하기 전에 더 많은 데이터를 보낼 수 있도록 TCP 창 크기를 동적으로 늘리는 기술입니다. 이전 예제에서는 승인이 필요하기 전에 45개의 패킷이 전송됩니다. 승인이 필요하기 전에 보낼 수 있는 패킷 수를 늘리면 보낸 사람이 승인을 기다리는 횟수를 줄여 TCP 최대 처리량이 증가합니다.

이 표에서는 이러한 관계를 보여 줍니다.

| | | | |
|-|-|-|-|
|**TCP 창 크기(바이트)**|**RTT 대기 시간(ms)**|**최대 메가바이트/초 처리량**|**최대 메가비트/초 처리량**|
|65,535|30|2.18|17.48|
|131,070|30|4.37|34.95|
|262,140|30|8.74|69.91|
|524,280|30|17.48|139.81|

그러나 TCP 창 크기에 대한 TCP 헤더 값은 2바이트 길이에 불과하므로 수신 창의 최대 값은 65,535입니다. 최대 창 크기를 늘리기 위해 TCP 창 배율 계수가 도입되었습니다.

배율 계수는 운영 체제에서 구성할 수 있는 설정이기도 합니다. 축척 계수를 사용하여 TCP 창 크기를 계산하는 수식은 다음과 같습니다.

`TCP window size = TCP window size in bytes \* (2^scale factor)`

다음은 창 축척 률 3과 창 크기 65,535에 대한 계산입니다.

`65,535 \* (2^3) = 262,140 bytes`

배율 계수가 14이면 TCP 창 크기가 14(허용되는 최대 오프셋)가 됩니다. TCP 창 크기는 1,073,725,440바이트(8.5기가비트)입니다.

#### <a name="support-for-tcp-window-scaling"></a>TCP 창 크기 조정 지원

Windows는 서로 다른 연결 유형에 대해 다른 배율 계수로 설정할 수 있습니다. (연결 클래스에는 데이터 센터, 인터넷 등이 포함됩니다. `Get-NetTCPConnection` PowerShell 명령을 사용하여 창 크기 조정 연결 유형을 봅니다.

```powershell
Get-NetTCPConnection
```

`Get-NetTCPSetting` PowerShell 명령을 사용하여 각 클래스의 값을 볼 수 있습니다.

```powershell
Get-NetTCPSetting
```

`Set-NetTCPSetting` PowerShell 명령을 사용하여 Windows에서 초기 TCP 창 크기 및 TCP 크기 조정 계수를 설정할 수 있습니다. 자세한 내용은 [설정-NetTCPSet](https://docs.microsoft.com/powershell/module/nettcpip/set-nettcpsetting?view=win10-ps)을 참조하십시오.

```powershell
Set-NetTCPSetting
```

다음은 `AutoTuningLevel`다음의 효과적인 TCP 설정입니다.

| | | | |
|-|-|-|-|
|**자동 조정 수준**|**스케일링 팩터**|**배율 조정**|**최대<br/>창 크기를 계산하는 수식**|
|사용 안 함|None|None|창 크기|
|제한|4|2^4|창 크기 * (2^4)|
|매우 제한됨|2|2^2|창 크기 * (2^2)|
|정상|8|2^8|창 크기 * (2^8)|
|실험적|14|2^14|창 크기 * (2^14)|

이러한 설정은 TCP 성능에 영향을 줄 가능성이 가장 높지만 Azure의 제어 외부인 인터넷의 다른 많은 요소도 TCP 성능에 영향을 줄 수 있습니다.

#### <a name="increase-mtu-size"></a>MTU 크기 증가

MTU가 클수록 MSS가 커지므로 MTU를 늘리면 TCP 성능이 향상될 수 있는지 궁금할 수 있습니다. 그렇지 않을 가능성이 있습니다. 단지 TCP 트래픽을 넘어 패킷 크기에 장단점이 있습니다. 앞서 설명한 것처럼 TCP 처리량 성능에 영향을 미치는 가장 중요한 요소는 TCP 창 크기, 패킷 손실 및 RTT입니다.

> [!IMPORTANT]
> Azure 고객은 가상 컴퓨터에서 기본 MTU 값을 변경하지 않는 것이 좋습니다.
>
>

### <a name="accelerated-networking-and-receive-side-scaling"></a>신속한 네트워킹 및 측면 확장 수신

#### <a name="accelerated-networking"></a>가속된 네트워킹

가상 시스템 네트워크 기능은 지금까지 게스트 VM과 하이퍼바이저/호스트 모두에서 CPU집약적이었습니다. 호스트를 통해 전송되는 모든 패킷은 모든 가상 네트워크 캡슐화 및 캡슐화를 포함하여 호스트 CPU에 의해 소프트웨어로 처리됩니다. 따라서 호스트를 통과하는 트래픽이 많을수록 CPU 로드가 높아집니다. 또한 호스트 CPU가 다른 작업으로 사용량이 많은 경우 네트워크 처리량 및 대기 시간에도 영향을 미칩니다. Azure는 빠른 네트워킹을 통해 이 문제를 해결합니다.

가속화된 네트워킹은 Azure의 사내 프로그래밍 가능한 하드웨어와 SR-IOV와 같은 기술을 통해 일관된 초저 네트워크 대기 시간을 제공합니다. 가속화된 네트워킹은 Azure 소프트웨어 정의 네트워킹 스택의 대부분을 CPU에서 FPGA 기반 SmartNIC로 이동합니다. 이러한 변경을 통해 최종 사용자 응용 프로그램은 컴퓨팅 주기를 회수할 수 있으므로 VM에 부하가 줄어들고 지연 시간이 지터와 불일치가 줄어듭니다. 즉, 성능이 더 결정적일 수 있습니다.

가속화된 네트워킹은 게스트 VM이 호스트를 우회하고 호스트의 SmartNIC을 사용하여 직접 데이터 경로를 설정할 수 있도록 하여 성능을 향상시킵니다. 다음은 가속화된 네트워킹의 몇 가지 이점입니다.

- **낮은 대기 시간 / 초당 더 높은 패킷 (pps)**: 데이터 경로에서 가상 스위치를 제거하면 정책 처리를 위해 호스트에서 패킷이 소비되는 시간을 제거하고 VM에서 처리 할 수있는 패킷 수를 증가시킵니다.

- **지터 감소**: 가상 스위치 처리는 적용해야 하는 정책의 양과 처리를 수행하는 CPU의 워크로드에 따라 달라집니다. 정책 적용을 하드웨어로 오프로드하면 VM에 직접 패킷을 전달하여 이러한 가변성이 제거되므로 호스트-VM 통신 및 모든 소프트웨어 인터럽트 및 컨텍스트 전환이 제거됩니다.

- **CPU 사용률 감소**: 호스트의 가상 스위치를 우회하면 네트워크 트래픽을 처리하기 위한 CPU 사용률이 줄어듭니다.

가속 네트워킹을 사용하려면 적용 가능한 각 VM에서 명시적으로 사용하도록 설정해야 합니다. 지침에 대 [한 가속된 네트워킹으로 리눅스 가상 컴퓨터 만들기를](https://docs.microsoft.com/azure/virtual-network/create-vm-accelerated-networking-cli) 참조 하십시오.

#### <a name="receive-side-scaling"></a>측 크기 조정 받기

수신 측 크기 조정(RSS)은 멀티프로세서 시스템에서 여러 CPU에 수신 처리를 배포하여 네트워크 트래픽 수신을 보다 효율적으로 분산하는 네트워크 드라이버 기술입니다. 간단히 말해서 RSS를 사용하면 시스템이 수신된 트래픽을 처리하는 데 만, 사용 가능한 모든 CPU가 아닌 모든 CPU를 사용하기 때문에 시스템이 더 많이 수신된 트래픽을 처리할 수 있습니다. RSS에 대한 보다 기술적인 설명은 [측 확장 수신 소개를](https://docs.microsoft.com/windows-hardware/drivers/network/introduction-to-receive-side-scaling)참조하십시오.

VM에서 가속 네트워킹을 사용하도록 설정하면 최상의 성능을 얻으려면 RSS를 사용하도록 설정해야 합니다. RSS는 가속 네트워킹을 사용하지 않는 VM에 이점을 제공할 수도 있습니다. RSS가 활성화되어 있는지 확인하는 방법과 이를 사용하도록 설정하는 방법에 대한 개요는 [Azure 가상 시스템에 대한 네트워크 처리량 최적화를](https://aka.ms/FastVM)참조하십시오.

### <a name="tcp-time_wait-and-time_wait-assassination"></a>TCP TIME_WAIT TIME_WAIT 암살

TCP TIME_WAIT 네트워크 및 응용 프로그램 성능에 영향을 주는 또 다른 일반적인 설정입니다. TCP의 정상적인 작동 중에 클라이언트 또는 서버(소스 IP:Source Port + 대상 IP:대상 포트)로 많은 소켓을 열고 닫는 바쁜 VM에서 지정된 소켓은 오랫동안 TIME_WAIT 상태로 끝날 수 있습니다. TIME_WAIT 상태는 추가 데이터를 닫기 전에 소켓에 전달할 수 있도록 하기 위한 것입니다. 따라서 TCP/IP 스택은 일반적으로 클라이언트의 TCP SYN 패킷을 자동으로 삭제하여 소켓의 재사용을 방지합니다.

소켓이 TIME_WAIT 시간은 구성할 수 있습니다. 30초에서 240초까지의 범위입니다. 소켓은 유한 한 리소스이며 지정된 시간에 사용할 수 있는 소켓 수를 구성할 수 있습니다. 사용 가능한 소켓 의 수는 일반적으로 약 30,000입니다.) 사용 가능한 소켓이 사용되거나 클라이언트와 서버가 TIME_WAIT 설정이 일치하지 않는 경우 VM이 TIME_WAIT 상태에서 소켓을 재사용하려고 하면 TCP SYN 패킷이 자동으로 삭제되면 새 연결이 실패합니다.

아웃바운드 소켓의 포트 범위 값은 일반적으로 운영 체제의 TCP/IP 스택 내에서 구성할 수 있습니다. TCP TIME_WAIT 설정 및 소켓 재사용에도 마찬가지입니다. 이러한 숫자를 변경하면 확장성이 향상될 수 있습니다. 그러나 상황에 따라 이러한 변경으로 인해 상호 운용성 문제가 발생할 수 있습니다. 이러한 값을 변경하는 경우 주의해야 합니다.

TIME_WAIT 암살을 사용하여 이 확장 제한을 해결할 수 있습니다. TIME_WAIT 암살은 새 연결의 IP 패킷의 시퀀스 번호가 이전 연결에서 마지막 패킷의 시퀀스 번호를 초과하는 경우와 같이 특정 상황에서 소켓을 재사용할 수 있도록 합니다. 이 경우 운영 체제를 통해 새 연결을 설정(새 SYN/ACK허용)하고 TIME_WAIT 상태였던 이전 연결을 강제로 닫을 수 있습니다. 이 기능은 Azure의 Windows VM에서 지원됩니다. 다른 VM의 지원에 대해 알아보려면 OS 공급업체에 문의하십시오.

TCP TIME_WAIT 설정 및 소스 포트 범위 구성에 대한 자세한 내용은 [네트워크 성능을 향상시키기 위해 수정할 수 있는 설정을](https://docs.microsoft.com/biztalk/technical-guides/settings-that-can-be-modified-to-improve-network-performance)참조하십시오.

## <a name="virtual-network-factors-that-can-affect-performance"></a>성능에 영향을 줄 수 있는 가상 네트워크 요인

### <a name="vm-maximum-outbound-throughput"></a>VM 최대 아웃바운드 처리량

Azure는 다양한 VM 크기와 형식을 제공하며 각 유형은 서로 다른 성능 기능을 제공합니다. 이러한 기능 중 하나는 네트워크 처리량(또는 대역폭)이며, 이는 초당 메가비트(Mbps)로 측정됩니다. 가상 시스템은 공유 하드웨어에서 호스팅되므로 동일한 하드웨어를 사용하는 가상 시스템 간에 네트워크 용량을 공정하게 공유해야 합니다. 대형 가상 시스템은 더 작은 가상 시스템보다 더 많은 대역폭을 할당받습니다.

각 가상 머신에 할당된 네트워크 대역폭은 가상 머신에서의 송신(아웃바운드) 트래픽으로 측정됩니다. 가상 컴퓨터에서 나가는 모든 네트워크 트래픽은 대상에 관계없이 할당된 제한에 대해 계산됩니다. 예를 들어 가상 시스템에 1,000Mbps 제한이 있는 경우 이 제한은 아웃바운드 트래픽이 동일한 가상 네트워크의 다른 가상 컴퓨터또는 Azure 외부에 있는 다른 가상 시스템에 대한 대상인지 여부에 관계없이 적용됩니다.

수신은 측정되거나 직접 제한되지 않습니다. 그러나 들어오는 데이터를 처리하는 가상 시스템의 기능에 영향을 줄 수 있는 CPU 및 저장소 제한과 같은 다른 요인이 있습니다.

가속 네트워킹은 대기 시간, 처리량 및 CPU 사용률을 포함하여 네트워크 성능을 향상시내도록 설계되었습니다. 빠른 네트워킹은 가상 컴퓨터의 처리량을 향상시킬 수 있지만 가상 컴퓨터의 할당된 대역폭까지만 향상시킬 수 있습니다.

Azure 가상 시스템에는 하나 이상의 네트워크 인터페이스가 연결되어 있습니다. 그들은 몇 가지있을 수 있습니다. 가상 시스템에 할당된 대역폭은 컴퓨터에 연결된 모든 네트워크 인터페이스에서 모든 아웃바운드 트래픽의 합계입니다. 즉, 대역폭은 컴퓨터에 연결된 네트워크 인터페이스 수에 관계없이 가상 시스템별로 할당됩니다.

예상 아웃바운드 처리량 과 각 VM 크기에서 지원되는 네트워크 인터페이스 의 수는 [Azure의 Windows 가상 컴퓨터크기에](https://docs.microsoft.com/azure/virtual-machines/windows/sizes?toc=%2fazure%2fvirtual-network%2ftoc.json)자세히 설명되어 있습니다. 최대 처리량을 보려면 **범용**과 같은 유형을 선택한 다음 결과 페이지(예: "Dv2 시리즈")에서 크기 계열에 대한 섹션을 찾습니다. 각 시리즈에 대해 "최대 NIC / 예상 네트워크 대역폭(Mbps)"이라는 제목의 마지막 열에 네트워킹 사양을 제공하는 테이블이 있습니다.

처리량 제한은 가상 컴퓨터에 적용됩니다. 처리량은 다음과 같은 요인의 영향을 받지 않습니다.

- **네트워크 인터페이스 수**: 대역폭 제한은 가상 시스템의 모든 아웃바운드 트래픽합계에 적용됩니다.

- **가속 네트워킹**: 이 기능은 게시된 제한을 달성하는 데 도움이 될 수 있지만 제한을 변경하지는 않습니다.

- **트래픽 대상**: 모든 대상이 아웃바운드 제한에 대해 계산됩니다.

- **프로토콜**: 모든 프로토콜을 통한 모든 아웃바운드 트래픽이 제한에 대해 계산됩니다.

자세한 내용은 [가상 컴퓨터 네트워크 대역폭을](https://aka.ms/AzureBandwidth)참조하십시오.

### <a name="internet-performance-considerations"></a>인터넷 성능 고려 사항

이 문서에서 설명한 것처럼 인터넷 및 Azure 제어 외부의 요소는 네트워크 성능에 영향을 줄 수 있습니다. 다음은 이러한 요소 중 일부입니다.

- **대기 시간**: 두 대상 사이의 왕복 시간은 중간 네트워크, "최단" 거리 경로를 이용하지 않는 트래픽 및 최적이 아닌 피어링 경로의 문제의 영향을 받을 수 있습니다.

- **패킷 손실**: 패킷 손실은 네트워크 정체, 물리적 경로 문제 및 네트워크 장치 성능 저하로 인해 발생할 수 있습니다.

- **MTU 크기/조각화**: 경로를 따라 조각화하면 데이터 도착이 지연되거나 패킷이 순서대로 도착하는 패킷이 지연될 수 있으며, 이는 패킷 전달에 영향을 줄 수 있습니다.

Traceroute는 소스 장치와 대상 장치 사이의 모든 네트워크 경로를 따라 네트워크 성능 특성(예: 패킷 손실 및 대기 시간)을 측정하는 데 유용한 도구입니다.

### <a name="network-design-considerations"></a>네트워크 설계 고려 사항

이 문서의 앞에서 설명한 고려 사항과 함께 가상 네트워크의 토폴로지가 네트워크 성능에 영향을 줄 수 있습니다. 예를 들어 전 세계적으로 단일 허브 가상 네트워크로 트래픽을 백홀하는 허브 및 스포크 설계는 네트워크 대기 시간을 발생시키고 전체 네트워크 성능에 영향을 미칩니다.

네트워크 트래픽이 통과하는 네트워크 장치의 수도 전체 대기 시간에 영향을 줄 수 있습니다. 예를 들어 허브 및 스포크 디자인에서 트래픽이 스포크 네트워크 가상 어플라이언스와 허브 가상 어플라이언스를 통과한 후 인터넷으로 전송하는 경우 네트워크 가상 어플라이언스가 대기 시간을 유발할 수 있습니다.

### <a name="azure-regions-virtual-networks-and-latency"></a>Azure 지역, 가상 네트워크 및 대기 시간

Azure 영역은 일반 지리적 영역 내에 있는 여러 데이터 센터로 구성됩니다. 이러한 데이터 센터는 물리적으로 서로 옆에 있지 않을 수 있습니다. 어떤 경우에는 10킬로미터까지 분리됩니다. 가상 네트워크는 Azure 물리적 데이터 센터 네트워크 위에 있는 논리적 오버레이입니다. 가상 네트워크는 데이터 센터 내의 특정 네트워크 토폴로지(topology)를 의미하지 않습니다.

예를 들어 동일한 가상 네트워크 및 서브넷에 있는 두 개의 VM이 서로 다른 랙, 행 또는 데이터 센터에 있을 수 있습니다. 그들은 광섬유 케이블의 피트 또는 광섬유 케이블의 킬로미터에 의해 분리 될 수있다. 이러한 변화는 서로 다른 VM 간에 가변 대기 시간(몇 밀리초 차이)을 가져올 수 있습니다.

VM의 지리적 배치와 두 VM 간의 잠재적인 지연 시간은 가용성 집합 및 가용성 영역의 구성에 의해 영향을 받을 수 있습니다. 그러나 지역의 데이터 센터 간의 거리는 지역별로 다르며 주로 해당 지역의 데이터 센터 토폴로지의 영향을 받습니다.

### <a name="source-nat-port-exhaustion"></a>소스 NAT 포트 소모

Azure의 배포는 공용 인터넷 및/또는 공용 IP 공간에서 Azure 외부의 끝점과 통신할 수 있습니다. 인스턴스가 아웃바운드 연결을 시작하면 Azure는 개인 IP 주소를 공용 IP 주소에 동적으로 매핑합니다. Azure에서 이 매핑을 만들면 아웃바운드 시작 흐름에 대한 반환 트래픽이 흐름이 시작된 개인 IP 주소에 도달할 수도 있습니다.

모든 아웃바운드 연결에 대해 Azure Load Balancer는 어느 정도 동안 이 매핑을 유지 관리해야 합니다. Azure의 다중 테넌트 특성을 사용하면 모든 VM에 대한 모든 아웃바운드 흐름에 대해 이 매핑을 유지 관리하는 것이 리소스 집약적일 수 있습니다. 따라서 Azure 가상 네트워크의 구성에 따라 설정되고 있는 제한이 있습니다. 또는 보다 정확하게 말하자면 Azure VM은 지정된 시간에 특정 수의 아웃바운드 연결만 만들 수 있습니다. 이러한 제한에 도달하면 VM은 더 많은 아웃바운드 연결을 만들 수 없습니다.

그러나 이 동작은 구성할 수 있습니다. SNAT 및 SNAT 포트 소모에 대한 자세한 내용은 [이 문서를](https://docs.microsoft.com/azure/load-balancer/load-balancer-outbound-connections)참조하십시오.

## <a name="measure-network-performance-on-azure"></a>Azure에서 네트워크 성능 측정

이 문서의 여러 성능 최대값은 두 VM 간의 네트워크 대기 시간/왕복 시간(RTT)과 관련이 있습니다. 이 섹션에서는 대기 시간/RTT를 테스트하는 방법과 TCP 성능 및 VM 네트워크 성능을 테스트하는 방법에 대한 몇 가지 제안 사항을 제공합니다. 이 섹션에서 설명한 기술을 사용하여 앞에서 설명한 TCP/IP 및 네트워크 값을 조정하고 성능 테스트할 수 있습니다. 대기 시간, MTU, MSS 및 창 크기 값을 이전에 제공된 계산에 연결하고 이론적 최대값을 테스트 중에 관찰한 실제 값과 비교할 수 있습니다.

### <a name="measure-round-trip-time-and-packet-loss"></a>왕복 시간 및 패킷 손실 측정

TCP 성능은 RTT 및 패킷 손실에 크게 의존합니다. Windows 및 Linux에서 사용할 수 있는 PING 유틸리티는 RTT 및 패킷 손실을 측정하는 가장 쉬운 방법을 제공합니다. PING의 출력은 소스와 대상 사이의 최소/최대/평균 대기 시간을 표시합니다. 또한 패킷 손실을 표시합니다. PING은 기본적으로 ICMP 프로토콜을 사용합니다. PsPing을 사용하여 TCP RTT를 테스트할 수 있습니다. 자세한 내용은 [PsPing](https://docs.microsoft.com/sysinternals/downloads/psping)을 참조하십시오.

### <a name="measure-actual-throughput-of-a-tcp-connection"></a>TCP 연결의 실제 처리량 측정

NTttcp는 리눅스 또는 윈도우 VM의 TCP 성능을 테스트하기위한 도구입니다. 다양한 TCP 설정을 변경한 다음 NTttcp를 사용하여 이점을 테스트할 수 있습니다. 이러한 응용 프로그램은 Azure AD Graph API를 사용할 수 있습니다. 자세한 내용은 다음 리소스를 참조하세요.

- [대역폭/처리량 테스트(NTttcp)](https://aka.ms/TestNetworkThroughput)

- [NTttcp 유틸리티](https://gallery.technet.microsoft.com/NTttcp-Version-528-Now-f8b12769)

### <a name="measure-actual-bandwidth-of-a-virtual-machine"></a>가상 시스템의 실제 대역폭 측정

iPerf라는 도구를 사용하여 다양한 VM 유형의 성능, 가속화된 네트워킹 등의 성능을 테스트할 수 있습니다. iPerf는 리눅스와 윈도우에서도 사용할 수 있습니다. iPerf는 TCP 또는 UDP를 사용하여 전체 네트워크 처리량을 테스트할 수 있습니다. iPerf TCP 처리량 테스트는 이 문서에서 설명하는 요소(예: 대기 시간 및 RTT)의 영향을 받습니다. 따라서 최대 처리량을 테스트하려는 경우 UDP가 더 나은 결과를 얻을 수 있습니다.

자세한 내용은 다음 문서를 참조하세요.

- [익스프레스루트 네트워크 성능 문제 해결](https://docs.microsoft.com/azure/expressroute/expressroute-troubleshooting-network-performance)

- [가상 네트워크에 대한 VPN 처리량의 유효성을 검사하는 방법](https://docs.microsoft.com/azure/vpn-gateway/vpn-gateway-validate-throughput-to-vnet)

### <a name="detect-inefficient-tcp-behaviors"></a>비효율적인 TCP 동작 감지

패킷 캡처에서 Azure 고객은 네트워크 성능 문제를 나타낼 수 있는 TCP 플래그(SACK, DUP ACK, RETRANSMIT 및 FAST RETRANSMIT)가 있는 TCP 패킷을 볼 수 있습니다. 이러한 패킷은 특히 패킷 손실로 인한 네트워크 비효율성을 나타냅니다. 그러나 패킷 손실이 Azure 성능 문제로 인해 반드시 발생하는 것은 아닙니다. 성능 문제는 응용 프로그램 문제, 운영 체제 문제 또는 Azure 플랫폼과 직접 관련이 없는 기타 문제의 결과일 수 있습니다.

또한 일부 재전송 및 중복 ACK는 네트워크에서 정상입니다. TCP 프로토콜은 신뢰할 수 있도록 구축되었습니다. 패킷 캡처에서 이러한 TCP 패킷의 증거는 반드시 조직 네트워크 문제를 나타내지 않습니다., 그들은 과도 하지 않는 한.

그러나 이러한 패킷 유형은 이 문서의 다른 섹션에서 설명하는 이유로 TCP 처리량이 최대 성능을 달성하지 못하고 있음을 나타냅니다.

## <a name="next-steps"></a>다음 단계

Azure VM에 대한 TCP/IP 성능 튜닝에 대해 배웠으므로 [가상 네트워크 계획에](https://docs.microsoft.com/azure/virtual-network/virtual-network-vnet-plan-design-arm) 대한 다른 고려 사항에 대해 알아보거나 가상 [네트워크 연결 및 구성에 대해 자세히 알아볼](https://docs.microsoft.com/azure/virtual-network/)수 있습니다.
