---
title: Azure Vm에 대 한 TCP/IP 성능 튜닝 | Microsoft Docs
description: 관계는 Azure Vm에 다양 한 일반적인 TCP/IP 성능 튜닝 기술에 알아봅니다.
services: virtual-network
documentationcenter: na
author:
- rimayber
- dgoddard
- stegag
- steveesp
- minale
- btalb
- prachank
manager: paragk
editor: ''
ms.assetid: ''
ms.service: virtual-network
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: infrastructure-services
ms.date: 3/30/2019
ms.author:
- rimayber
- dgoddard
- stegag
- steveesp
- minale
- btalb
- prachank
ms.openlocfilehash: c5d4f67e9c1e4e983133675c440b8c5b64183227
ms.sourcegitcommit: 04716e13cc2ab69da57d61819da6cd5508f8c422
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 04/02/2019
ms.locfileid: "58851863"
---
# <a name="tcpip-performance-tuning-for-azure-vms"></a>Azure Vm에 대 한 튜닝 TCP/IP 성능

이 문서의 목적은 일반적인 TCP/IP 성능 튜닝 기술 및 Microsoft Azure에서 실행 중인 virtual machines에 대 한 해당 고려 사항에 설명 하는 것입니다. 먼저 개념이 기본적으로 이해 하 고 다음 튜닝 하는 방법에 대해 설명 하는 것이 반드시 합니다.

## <a name="common-tcpip-tuning-techniques"></a>일반 TCP/IP 튜닝 기법

### <a name="mtu-fragmentation-and-large-send-offload-lso"></a>MTU, 조각화 및 대규모 전송 오프 로드 LSO)

#### <a name="explanation-of-mtu"></a>MTU 설명

최대 전송 단위 (MTU)는 네트워크 인터페이스를 통해 보낼 수 있는 바이트 단위로 지정 된 최대 크기 프레임 (패킷)입니다. MTU 구성 가능한 설정 및 기본 Azure Vm에서 MTU 사용 이며 대부분의 네트워크 장치에서 기본 설정은 1500 바이트를 전체적으로입니다.

#### <a name="explanation-of-fragmentation"></a>설명은 조각화

조각화는 네트워크 인터페이스의 MTU를 초과 하는 패킷을 보낼 때 발생 합니다. TCP/IP 스택을 패킷이 MTU 인터페이스를 준수 하는 작은 조각 (조각)으로 중단 됩니다. 조각화 IP 계층에서 발생 하 고 기본 프로토콜 (예: TCP)와 무관 합니다. 2000 바이트 패킷이 MTU 1500 사용 하 여 네트워크 인터페이스를 통해 전송 되 면 다음이 세분화 됩니다 하나 1500 바이트 패킷이에 500 바이트 패킷이 하나입니다.

원본과 대상 간의 경로에 네트워크 장치 옵션이 패킷을 더 작은 부분으로 조각 또는 MTU를 초과 하는 패킷을 삭제 합니다.

#### <a name="the-dont-fragment-df-bit-in-an-ip-packet"></a>IP 패킷에 "하지 조각 (DF)" 비트

Don't Fragment 비트가 IP 프로토콜 헤더에 있는 플래그입니다. DF 비트가 설정 된 경우 발신자와 수신자 간의 경로에서 중간 네트워크 장치 패킷의 조각화 하지 해야 나타냅니다. 여러 가지 이유가 있습니다 이유는이 비트를 설정할 수 있습니다 (한 예로 아래 경로 검색 섹션 참조). 경우 Don't Fragment 비트 집합과 패킷을 수신 하는 네트워크 장치를 해당 패킷이 MTU 장치 인터페이스를 초과 하 고 표준 동작은 장치가 원래 소스에 다시 "ICMP 조각화 필요" 패킷을 보내고 패킷을 삭제 합니다 패킷입니다.

#### <a name="performance-implications-of-fragmentation"></a>조각화의 성능에 미치는 영향

조각화 음수 성능 영향을 미칠 수 있습니다. 성능에 미치는 영향에 대 한 주된 이유 중 하나에 조각화 p U/메모리 영향 및 패킷 리어셈블리입니다. 패킷을 조각화 해야 하는 네트워크 장치를 조각화 하는 데 p U/메모리 리소스를 할당 해야 합니다. 동일한 패킷을 다시 조합 하는 경우 수행 해야 합니다. 네트워크 장치를 원래 패킷이 어셈블할 수 있도록 받을 때까지 모든 조각을 저장 해야 합니다. 이 프로세스의 조각화/리어셈블리 조각화/리어셈블리 프로세스로 인해 대기 시간이 발생할 수 있습니다.

조각화의 다른 가능한 부정적인 영향은 조각화 된 패킷이 순서로 도착할 수 있습니다. 뒤바뀐 순서의 패킷 특정 유형의 네트워크 장치를 다시 전송 해야 전체 패킷 해야 하는 순서가 패킷을-삭제 될 수 있습니다. 조각 삭제에 대 한 일반적인 시나리오는 네트워크 방화벽과 같은 보안 장치를 포함 하거나 버퍼 고갈 되는 네트워크 장치의 수신 하는 경우. 네트워크 장치의 수신 버퍼를 모두 사용 했습니다., 네트워크 장치를 조각화 된 패킷이 다시 어셈블해야 하므로 하려고 하지만 리소스를 저장 하 고 패킷을 reassume 필요는 없습니다.

조각화가 인터넷을 통해 다양 한 네트워크를 연결 하는 데 필요한에 대 한 조각화 지원 않지만 음수 작업으로 감지 될 수 있습니다.

#### <a name="benefits-and-consequences-of-modifying-the-mtu"></a>이점 및 MTU를 수정한 결과

일반 문으로 MTU 증가 효율적 네트워크를 만들 수 있습니다. 모든 패킷이 전송 되는 원래 패킷이에 추가 되는 추가 헤더 정보에는 자세한 패킷 자세한 머리글 오버 헤드를 의미 하 고 네트워크 결과적으로 덜 효율적입니다.

예를 들어, 이더넷 헤더 크기는 14 바이트 +는 4 바이트 프레임 확인 순서 fcs 프레임 일관성이 유지 되도록 합니다. 하나의 2000 바이트 패킷이 전달 되 면 네트워크에서 18 바이트의 이더넷 오버 헤드 추가 됩니다. 패킷을 1500 바이트 패킷이에 500 바이트 패킷 조각화 된 경우 이더넷 헤더-18 바이트 또는 36 바이트 그러면 각 패킷의 가지게 됩니다. 반면 단일 2000 바이트 패킷이 18 바이트는 이더넷 헤더 하기만 하면 됩니다.

자체 MTU 증가 반드시 만들지 보다 효율적인 네트워크를 두는 것이 반드시 합니다. 응용 프로그램이 500 바이트 패킷을 보내면, MTU 1500 또는 9000 바이트 인지 동일한 머리글 오버 헤드 존재 합니다. 더이 네트워크에서 효율적이 고 다음 사용 해야 합니다도 MTU와 관련 된 더 큰 패킷 크기입니다.

#### <a name="azure-and-vm-mtu"></a>Azure 및 VM MTU

기본 Azure Vm에 대 한 MTU는 1500 바이트입니다. Azure Virtual Network 스택 1400 바이트에서 패킷 조각화 하려고 합니다. 그러나 Azure 가상 네트워크 스택을 사용 하면 패킷 2006 바이트까지 "Don't Fragment" 비트 IP 헤더에 설정 된 경우.

Vm은 1500 MTU를 포함 하는 동안에 1400 바이트로 패킷을 조각 Azure Virtual Network 스택 본질적으로 효율적 이므로이 조각화 의미 하지 않습니다 하는 것이 반드시 합니다. 실제로 네트워크 패킷 비율을 1400 또는 1500 바이트 보다 훨씬 더 작기 때문입니다.

#### <a name="azure-and-fragmentation"></a>Azure 및 조각화

현재 azure의 가상 네트워크 스택 "순서 비지정 조각"-의미 없는 원래 조각화 된 순서로 도착 하는 조각화 된 패킷이 삭제 하도록 구성 됩니다. 이러한 패킷은 FragmentStack 호출 2018 년 11 월에에서 발표 하는 네트워크 보안 취약점으로 인해 주로 삭제 됩니다.

FragmentSmack은 조각화 된 IPv4 및 IPv6 패킷 리어셈블리를 처리 하는 Linux 커널 방식이 결함입니다. 원격 공격자가 대상 시스템에 향상 된 CPU 및 서비스 거부로 인해 트리거 비용이 많이 드는 조각 리어셈블리 작업에 이러한 결함을 사용할 수 있습니다.

#### <a name="tune-the-mtu"></a>MTU를 조정 합니다.

Azure Vm에는 다른 운영 체제 처럼 구성 가능한 MTU 지원. 그러나 Azure 내에서 발생 하 고 위에서 자세히 설명 되어 있는 조각화 MTU를 구성할 때 고려해 야 합니다.

Azure에는 고객이 늘립니다 그 VM MTU 하도록 권장 하지 않습니다. 이 문서는 Azure MTU를 구현 하 고 조각화를 지금 수행 하는 방법을 자세히 설명 하는 데 사용 됩니다.

> [!IMPORTANT]
>MTU를 늘리면 성능 향상을 위해 표시 되지 않은 하 고 응용 프로그램 성능에 부정적인 영향을 있을 수 있습니다.
>
>

#### <a name="large-send-offload-lso"></a>Large Send Offload LSO)

큰 보낼 오프 로드 LSO () 이더넷 어댑터 패킷의 조각화를 오프 로드 하 여 네트워크 성능을 향상 시킬 수 있습니다. TCP/IP 스택을 사용 하도록 하므로 LSO를 사용 하 여 큰 TCP 패킷을 만들고이 이더넷 어댑터에 전달 하기 전에 조각화에 대 한 다음 보냅니다. 하도록 하므로 LSO의 장점은 패킷이 MTU 따르며 하드웨어에서 수행 되는 이더넷 인터페이스에 해당 처리를 오프 로드 하는 패킷 크기를 세그먼트화 CPU 사용 가능한 수 있습니다. 하도록 하므로 LSO의 이점에 대 한 자세한 내용은에서 찾을 수 있습니다 [Microsoft 네트워크 어댑터 설명서에서 성능을](https://docs.microsoft.com/windows-hardware/drivers/network/performance-in-network-adapters#supporting-large-send-offload-lso)합니다.

하도록 하므로 LSO 사용 하는 경우에 Azure 고객은 수행 하 고 패킷 캡처 때 많은 프레임 크기에 나타날 수 있습니다. 이러한 많은 프레임 크기는 조각화 나 MTU 없는 경우 되는 jumbo 생각 일부 고객은 발생할 수 있습니다. 이더넷 어댑터 하도록 하므로 LSO를 사용 하 여 더 큰 TCP 패킷을 만들기 위해는 더 큰 MSS를 TCP/IP 스택을 보급할 수 있습니다. 이 전체에서 분할 되지 않은 프레임 이더넷 어댑터에 전달 됩니다 및 VM에서 수행 하는 패킷 캡처에 표시 됩니다. 그러나 패킷은 나눌 수 많은 작은 프레임 이더넷 어댑터의 MTU에 따라 이더넷 어댑터.

### <a name="tcpmss-window-scaling-and-pmtud"></a>TCP/MSS 창 크기 조정 및가 PMTUD

#### <a name="explanation-of-tcp-mss"></a>TCP MSS 설명

TCP MSS 최대 세그먼트 크기 ()는 TCP 패킷의 조각화를 방지 하는 것에 대 한 최대 TCP 세그먼트 크기를 설정 하는 데는 설정입니다. 운영 체제는 일반적으로 MSS MSS MTU-= IP & TCP 헤더 크기 (20 바이트 또는 총 40 바이트)입니다. 따라서 1500의 MTU 사용 하 여 인터페이스 64240 MSS 해야 합니다. 그러나 MSS를는 구성할 수 있습니다.

원본과 대상 간에 TCP 세션을 설정할 때이 설정은 TCP 세 방향 핸드셰이크에서 동의 됩니다. 양쪽 모두 MSS 값을 보내고 둘 중 더 작은 TCP 연결에 사용 됩니다.

Azure VPN Gateway를 포함 하 여 VPN Gateway와 같은 중간 네트워크 장치에 최적의 네트워크 성능을 보장 하기 위해 원본 및 대상의 독립적인 MTU를 조정할 수가 있습니다. 따라서 유의 해야 원본 및 대상만의 MTU 실제 MSS 값의 유일한 요소를 아닙니다.

#### <a name="explanation-of-path-mtu-discovery-pmtud"></a>Path MTU Discovery (가 PMTUD) 설명

MSS 협상은 원본과 대상 간의 경로에 다른 네트워크 장치로 사용할 수 있는 실제 MSS 원본 및 대상 보다 낮은 MTU 값 있을 나타낼 없습니다 수 있습니다. 이 경우 해당 MTU 패킷을 보다 작습니다. 장치를 패킷을 삭제 하 해당 MTU를 포함 하는 제어 메시지 ICMP (Internet Protocol) 조각화 필요 (유형 3, 코드 4) 메시지를 반송 합니다. 이 ICMP 메시지 원본 호스트를 경로 MTU를 적절 하 게 줄일 수 있습니다. 프로세스는 Path MTU discovery를 라고 합니다.

가 PMTUD 프로세스 본질적으로 효율적 이며 네트워크 성능에 영향을 미칩니다. 네트워크 경로 MTU를 초과 하는 패킷이 전송 될 때 해당 패킷으로 낮은 MSS를 사용 하 여 재전송 될 해야 합니다. 보낸 사람에 게 알 수 없습니다는 MSS를 절감 하 고 지속적으로 해야 하는 다음 보낸 ICMP 조각화 필요한 패킷 (일반적으로 라고가 PMTUD 블랙홀)를 경로에 네트워크 방화벽으로 인해 받지 못하면 패킷의 재전송입니다. 따라서 Azure VM MTU 증가 권장 하지 않습니다.

#### <a name="vpn-considerations-with-mtu"></a>MTU 사용 하 여 VPN 고려 사항

캡슐화 (예: IPSec Vpn)을 수행 하는 Vm을 사용 하는 고객 MTU 및 패킷 크기가 추가 영향을 줄 수 있습니다. Vpn 원래 패킷이 따라서 패킷 크기를 늘리고 작은 MSS를 요구에 추가할 추가 헤더를 추가 합니다.

Azure에 대 한 현재 권장 TCP MSS 고정 1350 바이트 터널 인터페이스 MTU 1400 설정 하는 것입니다. 자세한 정보를 찾을 수 있습니다 합니다 [VPN 장치 및 IPSec/IKE 매개 변수 페이지](https://docs.microsoft.com/azure/vpn-gateway/vpn-gateway-about-vpn-devices)합니다.

### <a name="latency-round-trip-time-and-tcp-window-scaling"></a>대기 시간, 왕복 시간 및 TCP 창 크기 조정

#### <a name="latency-and-round-trip-time"></a>대기 시간 및 왕복 시간

네트워크 대기 시간 속도의 광섬유 파이버 네트워크를 통해 적용 됩니다. 현실적으로, tcp 네트워크 처리량은 또한 효과적으로 관리 (실제 최대값) 때문에 왕복 시간 (RTT)의 두 가지 네트워크 장치 간의 합니다.

| | | | |
|-|-|-|-|
|라우팅|Distance|단방향 시간|RTT (왕복 시간)|
|San Francisco에 New York|4,148 km|21 ms|42 ms|
|런던에 New York|5,585 km|28ms|56 ms|
|시드니에 New York|15,993 km|80ms|160 밀리초|

그러나이 표에서 두 위치 간에 직선 거리를 보여 줍니다. 네트워크에서 거리 일반적으로 보다 긴 경우 직선 거리 간단한 수식을 광원의 속도 따라 제어 되 최소 RTT를 계산 하는: 최소 RTT = 2 * (킬로미터 거리의 전파 속도 /).

표준 200 값의 전파 속도에 사용할 수 있습니다-값이 1 밀리초 동안의 미터 light의 거리를 이동 합니다.

San Francisco에 예제 뉴욕, 4,148 km 직선 거리입니다. 최소 RTT = 2 * (4,148 / 20). 수식의 출력 시간 (밀리초) 됩니다.

두 위치 간의 실제 거리는 고정된 실제로 가장 논리적 옵션 간의 최소 거리를 사용 하 여 대상을 선택 하는 최대 네트워크 성능이 필요한 경우 둘째로, 가상 네트워크 내에서 디자인 의사 결정 트래픽의 경로 최적화 하 여 대기 시간을 줄일 수 있습니다. 이러한 virtual network 고려 사항 아래의 네트워크 디자인 고려 사항 섹션에 설명 되어 있습니다.

#### <a name="latency-and-round-trip-time-effects-on-tcp"></a>대기 시간 및 왕복 시간이 미치는 TCP

왕복 시간 (RTT) 최대 TCP 처리량에 직접적인 영향을 미칩니다. TCP 프로토콜에 창 크기의 개념이 있습니다. 창 크기가 트래픽 수신기에서 보낸 승인을 받아야 전에 TCP 연결을 통해 보낼 수 있는 최대 용량입니다. TCP MSS 1460로 설정 된 경우 TCP 창 크기가 65535로 설정 됩니다 다음 보낸 사람에 게 패킷을 보낼 수 있는 45 전에 수신자에서 승인을 받아야 합니다. 승인 수신 되지 않으면 보낸 사람에 게 다시 전송 합니다. 이 예제에서는 TCP 창 크기 / TCP MSS = 패킷 전송 합니다. 65535 또는 1460 반올림 됩니다 / 최대 45.

이 "승인을 기다리는 중" 상태를 전달 하는 신뢰할 수 있는 데이터를 만드는 메커니즘으로 효과적으로 놓이게 되어 RTT TCP 처리량에 영향을 줄. 보낸 사람에 게 승인을 기다리는 오래, 길수록 더 많은 데이터를 보내기 전에 대기 해야 합니다.

단일 TCP 연결의 최대 처리량을 계산 하는 것에 대 한 수식 아래와 같습니다. 창 크기 / (RTT 대기 시간 (밀리초) / 1000) = 최대 바이트 수/초입니다. 다음은 가독성을 높이기 위해 메가바이트에서 서식이 지정 되어 있으며 최대 메가바이트를 보여 줍니다. / 초당 단일 TCP 연결의 처리량입니다.

| | | | |
|-|-|-|-|
|TCP 창 크기 (바이트)|RTT Latency<br/>밀리초|최대<br/>초당 처리량 (메가바이트)|최대<br/> 두 번째 처리량 당 메가 비트|
|65535|1|65.54|524.29|
|65535|30|2.18|17.48|
|65535|60|1.09|8.74|
|65535|90|.73|5.83|
|65535|120|.55|4.37|

패킷 손실이 발생 하는 경우 다음 줄어듭니다 TCP 연결의 최대 처리량 발신자가 이미 전송 되는 데이터를 다시 전송 하는 동안.

#### <a name="explanation-of-tcp-window-scaling"></a>TCP 창 크기 조정 설명

TCP 창 크기 조정 하는 것은 승인을 반드시 전에 보낼 자세한 데이터 TCP 창 크기를 동적으로 증가 하는 개념입니다. 이전 예에서 45 패킷은 보낼 전에 승인 필요 했습니다. 수가, 승인을 증가 전에 전송 되는 패킷의 다음 TCP 최대 처리량도 증가 보낸 승인을 기다리는 횟수를 줄여 합니다.

아래는 간단한 테이블에서 TCP 처리량을 보여 줍니다.

| | | | |
|-|-|-|-|
|TCP 창 크기<br/>(바이트)|RTT 대기 시간 (밀리초)|최대<br/>초당 처리량 (메가바이트)|최대<br/> 두 번째 처리량 당 메가 비트|
|65535|30|2.18|17.48|
|131,070|30|4.37|34.95|
|262,140|30|8.74|69.91|
|524,280|30|17.48|139.81|

그러나 TCP 창 크기에 대 한 TCP 헤더 값을는 2 바이트 long, 즉, 수신 창에 대 한 최대값은 65535입니다. 최대 창 크기를 늘리기 위해 TCP 창 크기 조정 비율을 도입 되었습니다.

배율 인수는 운영 체제에서 구성할 수 있는 설정 이기도 합니다. 배율 인수를 사용 하 여 TCP 창 크기를 계산 하는 것에 대 한 수식 아래와 같습니다. TCP 창 크기 TCP 창 크기 (바이트) = \* (2 ^ 비율 크기 조정). 창 크기 조정 인수가 3 및 65535의 창 크기를 계산은 다음과 같습니다. 65535 \* (2 ^3) = 262,140 바이트입니다. TCP 창 크기 (최대 오프셋 허용), 14에서 14의 배율 인수 결과 다음 TCP 창 크기를 1,073,725,440 바이트 (8.5 기가 비트) 됩니다.

#### <a name="support-for-tcp-window-scaling"></a>TCP 창 크기 조정에 대 한 지원

Windows에서 다른 배율 인수를 설정 하는 기능에는 연결 형식별으로-일부의 클래스가 있습니다 (데이터 센터, 인터넷 및 등)에 연결 합니다. Get-NetTCPConnection powershell 명령 사용 하 여 창 크기 조정 연결 분류를 볼 수 있습니다.

```powershell
Get-NetTCPConnection
```

Get-NetTCPSetting powershell 명령 사용 하 여 각 클래스의 값을 볼 수 있습니다.

```powershell
Get-NetTCPSetting
```

집합 NetTCPSetting powershell 명령을 통해 Windows에서 초기 TCP 창 크기와 TCP 크기 조정 비율을 설정할 수 있습니다. 자세한 정보를 찾을 수 있습니다는 [집합 NetTCPSetting 페이지](https://docs.microsoft.com/powershell/module/nettcpip/set-nettcpsetting?view=win10-ps)

```powershell
Set-NetTCPSetting
```

AutoTuningLevel 효과적인 TCP 설정을 아래와 같습니다.

| | | | |
|-|-|-|-|
|AutoTuningLevel|배율|승수를 크기 조정|수식<br/>최대 창 크기를 계산 합니다.|
|사용 안 함|없음|없음|창 크기|
|제한|4|2^4|창 크기 * (2 ^4)|
|매우 제한|2|2^2|창 크기 * (2 ^2)|
|정상|8|2^8|창 크기 * (2 ^8)|
|실험적|14|2^14|창 크기 * (2 ^14)|

이러한 설정은 TCP 성능에 영향을 줄 가능성이 가장 이지만, 유의 해야 Azure의 컨트롤 외부에서 인터넷을 통해 다른 요인 TCP 성능 영향을 줄 수도 있습니다.

#### <a name="increase-mtu-size"></a>MTU 크기 늘리기

논리는 질문은 "MTU를 늘려 성능을 향상 시킬 수 TCP 큰 MTU 큰 MSS를 방법 으로"? 간단히 말해 – 없는 것입니다. 앞에서 설명한 것 처럼 장단점을 패킷 크기로 TCP 트래픽에 적용할 수 있는 있습니다. 영향을 주는 가장 중요 한 요소를 위에서 설명한 것 처럼 TCP 창 크기, 패킷 손실 및 RTT TCP 처리량 성능이 됩니다.

> [!IMPORTANT]
> Azure는 Azure 고객은 Virtual Machines에서 기본 MTU 값을 수정 하는 것을 권장 하지 않습니다.
>
>

### <a name="accelerated-networking-and-receive-side-scaling"></a>가속화 된 네트워킹 및 수신측 배율

#### <a name="accelerated-networking"></a>가속 네트워킹

가상 머신 네트워크 함수는 CPU를 많이 VM 게스트와 하이퍼바이저/호스트에서 지금까지 되었습니다. 호스트를 통해 자신을 모든 패킷이 호스트 CPU-모든는 가상 네트워크 캡슐화/de-capsulation를 포함 하 여 소프트웨어에서 처리 됩니다. 따라서 자세한는 트래픽은 호스트 다음 높을수록 CPU 부하를 통해. 및 기타 작업을 수행 중이면 하는 호스트 CPU, 다음은 또한에 영향을 주는 네트워크 처리량 및 대기 시간입니다. 이 문제는 가속화 된 네트워킹을 통해 해결 되었습니다.

가속화 된 네트워킹 Azure의 내부 프로그래밍 가능한 하드웨어 및 SR-IOV와 같은 기술을 통해 일관 된 짧은 네트워크 대기 시간을 제공합니다. 이동 하 여 cpu가 off 대부분 Azure의 소프트웨어 정의 네트워킹 스택 및 SmartNICs FPGA 기반으로 주기 최종 사용자 응용 프로그램, VM에서 부하를 줄이고 배치, 지터 및 불일치 대기 시간 감소에서 회수 된을 계산 합니다. 즉, 성능이 보다 명확한 수 있습니다.

가속화 된 네트워킹 게스트 VM이 호스트를 우회 하 고 호스트의 SmartNIC 사용 하 여 직접 데이터를 설정 함으로써 성능 향상을 달성 합니다. 가속화 된 네트워킹의 이점은 다음과 같습니다.

- **더 낮은 대기 시간 / 더 높은 초당 패킷 초 (pps)**: 데이터 경로에서 가상 스위치를 제거하면 정책 처리를 위해 패킷이 호스트에 머무는 시간이 없어지며 VM 내에서 처리할 수 있는 패킷 수가 늘어납니다.

- **감소 된 지터**: 적용해야 하는 정책의 수와 처리를 수행하는 CPU의 워크로드에 따라 가상 스위치 처리 성능이 달라집니다. 정책 적용을 하드웨어로 오프로드하면 패킷이 VM으로 직접 전달되고, 호스트-VM 통신과 모든 소프트웨어 인터럽트 및 컨텍스트 전환이 제거되어 이러한 가변성이 해소됩니다.

- **CPU 사용률 감소**: 이 기능을 사용할 때는 호스트에서 가상 스위치를 바이패스하므로 네트워크 트래픽 처리에 CPU를 더 적게 사용하게 됩니다.

VM 당 기준 가속화 된 네트워킹을 명시적으로 활성화 되어야 합니다. VM에서 가속 네트워킹을 사용 하도록 설정 하는 것에 대 한 지침은 합니다 [가속 네트워킹 페이지를 사용 하 여 Linux 가상 머신 만들기](https://docs.microsoft.com/azure/virtual-network/create-vm-accelerated-networking-cli)합니다.

#### <a name="receive-side-scaling-rss"></a>수신 측 배율 (RSS)

수신측 배율 네트워크 드라이버 기술을 배포 하 여 네트워크 트래픽의 수신을 보다 효율적으로 배포 하는 다중 프로세서 시스템에서 여러 Cpu에서 처리를 수신 합니다. 간단히 말해 RSS 하나가 아닌 모든 사용 가능한 Cpu를 사용 하기 때문에 더 많이 수신된 트래픽 처리 하는 시스템 수 있습니다. RSS에 대 한 보다 기술적인 설명은에서 찾을 수 있습니다 합니다 [수신측 배율 페이지로 소개](https://docs.microsoft.com/windows-hardware/drivers/network/introduction-to-receive-side-scaling)합니다.

RSS VM에서 가속 네트워킹을 사용 하는 때 최대 성능을 얻기 위해 필요 합니다. 또한에 있을 수 있습니다 혜택 가속된 네트워킹이 설정 되지 않은 Vm에서 RSS를 사용 합니다. RSS 가능 및 사용 하도록 설정 하는 것에 대 한 구성에서 찾을 수 있습니다 하는 경우를 결정 하는 방법의 개요는 [Azure virtual machines 페이지에 대 한 네트워크 처리량 최적화](http://aka.ms/FastVM)합니다.

### <a name="tcp-time-wait-and-time-wait-assassination"></a>TCP 시간 대기 시간과 암살 대기

네트워크 및 응용 프로그램 성능에 영향을 주는 또 다른 일반적인 문제는 TCP 시간 대기 설정입니다. 중괄호와 닫는 여러 소켓, 클라이언트 또는 TCP의 정상 작동 하는 동안 서버 (원본 IP:Source 포트 + 대상 IP:Destination 포트)는 사용 중인 Vm에 지정 된 소켓에에서 놓일 수도 시간 대기 상태는 상당한 양의 시간에 대 한 합니다. 이 "대기 시간" 상태를 닫기 전에 소켓에 전달할 추가 데이터를 허용 하도록 것입니다. 따라서 TCP/IP 스택을 일반적으로 자동으로 클라이언트 TCP SYN 패킷을 삭제 하 여 소켓의 재사용을 방지 합니다.

이 기간 소켓은 대기 상태를 구성할 수 있지만 30 초에서 240 초 사이 수 없습니다. 소켓은 한정 된 리소스 및 특정된 시점에 사용할 수 있는 소켓 수가 구성할 수 있습니다 (수 약 30,000 잠재적인 소켓을 일반적으로 특정 데 사용). 이 번호를 모두 사용 하면 또는 클라이언트 및 서버 일치 하지 않는 시간 대기 설정에 있고 시간 대기 상태에서 소켓을 다시 사용 하려고 시도 VM, TCP SYN 패킷을 자동으로 삭제는 새 연결이 실패 합니다.

일반적으로 아웃 바운드 소켓이 뿐만 아니라 TCP 시간 대기 설정 및 소켓 재사용에 대 한 포트 범위에 대 한 값은 운영 체제의 TCP/IP 스택 내에서 구성할 수 있습니다. 확장성을 향상 시킬 수 있지만 시나리오에 따라 상호 운용성 문제가 발생할 수 있습니다 및 주의 해 서 변경 해야 이러한 번호를 변경 합니다.

시간 대기 암살 이라는 기능 제한 크기를 조정 하는이 문제를 해결 도입 되었습니다. 시간 대기 암살 소켓을을 새 연결의 IP 패킷에 시퀀스 번호는 이전 연결에서 마지막 패킷의 시퀀스 번호를 초과 하는 경우와 같은 특정 시나리오에서 다시 사용할 수 있습니다. 이 경우 운영 체제는 새 연결을 설정 하면 (새 SYN ACK 그대로) 강제 된 시간 대기 상태는 이전 연결을 닫습니다. 이 기능은 Azure 내에서 Windows Vm에서 현재 지원 및 해당 OS 공급 업체를 사용 하 여 Azure 고객이 지원 다른 Vm 내에서 조사 해야 합니다.

원본 포트 범위 및 TCP 시간 대기 설정 구성에 대 한 설명서에서 제공 되는 [네트워크 성능을 높이 페이지를 수정할 수 있는 설정을](https://docs.microsoft.com/biztalk/technical-guides/settings-that-can-be-modified-to-improve-network-performance)합니다.

## <a name="virtual-network-factors-that-can-affect-performance"></a>성능에 영향을 줄 수 있는 가상 네트워크 요소

### <a name="vm-maximum-outbound-throughput"></a>VM의 최대 아웃 바운드 처리량

Azure는 다양한 VM 크기 및 유형, 다양한 성능 기능을 제공합니다. 하나의 이러한 성능 기능은 네트워크 처리량, 대역폭 (Mbps) 초당 메가 비트 단위로 지정 합니다. 가상 머신은 공유 하드웨어에서 호스트되므로 동일한 하드웨어를 공유하는 가상 머신 간에 네트워크 용량을 공평하게 공유해야 합니다. 용량이 큰 가상 머신은 작은 가상 머신보다 상대적으로 더 많은 대역폭이 할당됩니다.

각 가상 머신에 할당된 네트워크 대역폭은 가상 머신에서의 송신(아웃바운드) 트래픽으로 측정됩니다. 가상 컴퓨터에서 나가는 모든 네트워크 트래픽은 대상에 관계없이 할당된 제한에 대해 계산됩니다. 예를 들어, 가상 컴퓨터를 1,000 1,000mbps 제한이 있으면 해당 제한 아웃 바운드 트래픽이 동일한 가상 네트워크에서 또는 Azure 외부의 다른 가상 머신에 대 한 대상이 인지 여부를 적용 합니다.
수신은 측정되거나 직접 제한되지 않습니다. 그러나 들어오는 데이터를 처리하는 가상 머신의 기능에 영향을 줄 수 있는 다른 요인(예: CPU 및 저장소 제한)이 있습니다.

가속화 된 네트워킹은 대기 시간, 처리량 및 CPU 사용률을 포함 하 여 네트워크 성능을 향상 시키기 위해 설계 된 기능입니다. 가속 네트워킹은 가상 머신의 처리량을 향상 시킬 수 있지만, 그렇게 할 수 있습니다만 가상 컴퓨터의 최대 대역폭을 할당 합니다.

Azure 가상 머신은 하나지만 여기에 연결된 네트워크 인터페이스는 여러 개일 수 있습니다. 가상 머신에 할당된 대역폭은 가상 머신에 연결된 모든 네트워크 인터페이스에서 모든 아웃바운드 트래픽의 총 합계입니다. 즉, 대역폭은 가상 컴퓨터에 연결된 네트워크 인터페이스 수에 관계없이 가상 컴퓨터별로 할당됩니다.
 
예상 되는 아웃 바운드 처리량 및 각 VM 크기에서 지 원하는 네트워크 인터페이스 수가 자세히 나와 여기 있습니다. 최대 처리량을 확인 하려면와 같은 범용 형식을 선택한 다음 크기 시리즈는 Dv2 시리즈와 같은 결과 페이지를 선택 합니다. 각 계열에 최대 Nic 이라는 마지막 열에 네트워킹 사양이 포함 된 테이블 / 예상 네트워크 성능 (Mbps)입니다.

처리량 제한은 가상 컴퓨터에 적용됩니다. 처리량은 다음 요인에 영향을 받지 않습니다.

- **네트워크 인터페이스 수가**: 대역폭 제한은 가상 머신에서 모든 아웃 바운드 트래픽의 누적입니다.

- **가속화 된 네트워킹**: 기능 게시 된 제한에 달성 하는 데 도움이 될 수 있습니다, 있지만 한계는 변경 되지 않습니다.

- **트래픽 대상**: 아웃 바운드 제한에 대해 모든 대상 수입니다.

- **프로토콜**: 모든 프로토콜을 통한 모든 아웃 바운드 트래픽을 제한에 대해 계산 됩니다.

A [이 페이지를 방문 하 여 VM 유형별로 최대 대역폭의 테이블을 찾을 수 있습니다](https://docs.microsoft.com/azure/virtual-machines/windows/sizes) 각 VM 유형에 따라 클릭 합니다. 각 유형 페이지에서 테이블의 최대 예상된 네트워크 대역폭 및 최대 Nic를 알아보겠습니다.

VM 네트워크 대역폭에 대 한 자세한 정보를 찾을 수 있습니다 [가상 머신 네트워크 대역폭](http://aka.ms/AzureBandwidth)합니다.

### <a name="internet-performance-considerations"></a>인터넷 성능 고려 사항

이 문서 전체에서 설명 했 듯이 요소 인터넷과 Azure의 컨트롤 외부의 네트워크 성능이 저하 될 수 있습니다. 이러한 요인은 다음과 같습니다.

- **Latency**: 두 개의 대상 간의 왕복 시간 트래픽 "최단"을 가능 하 고 최적이 아닌 피어 링 경로 고려 하지 않고 중간 네트워크에서 문제로 인해 영향을 받을 수합니다 있습니다.

- **패킷 손실**: 네트워크 정체, 실제 경로 문제 및 네트워크 장치에서 수행 하 여 패킷 손실이 발생할 수 있습니다.

- **MTU 크기/조각화**: 경로 따라 조각화 패킷 배달에 영향을 줄 수 있는 데이터 도착 순서 대로 도착 하는 패킷을에 지연이 발생할 수 있습니다.

Traceroute는 원본 및 대상 장치 간의 모든 네트워크 경로 (예: 패킷 손실 및 대기 시간) 네트워크 성능 특성을 측정 하는 좋은 도구입니다.

### <a name="network-design-considerations"></a>네트워크 디자인 고려 사항

위의 고려 사항, 함께 가상 네트워크의 토폴로지 가상 네트워크 성능이 저하 될 수 있습니다. 예를 들어 허브 및 스포크 디자인 단일 허브 가상 네트워크에 전역적으로 귀로 화물 트래픽이 네트워크 대기 시간이 발생할 되며 따라서 전반적인 네트워크 성능에 영향을 합니다. 마찬가지로, 네트워크 트래픽이 통과 하는 네트워크 장치 수가 전체 대기 시간을 달라질 수 있습니다. 예를 들어 허브 및 스포크 디자인에서는 트래픽이 통과 하는 스포크 네트워크 가상 어플라이언스와 허브 가상 어플라이언스를 인터넷에 전송을 전에 하는 경우 다음 대기 시간에서 제공할 수 있습니다 네트워크 가상 어플라이언스입니다.

### <a name="azure-regions-virtual-networks-and-latency"></a>Azure 지역, 가상 네트워크 및 대기 시간

지역 내에 있는 여러 데이터 센터의 azure 지역 구성 됩니다. 이러한 데이터 센터 될 물리적으로 서로 옆에 있는 수 없으며 일부의 경우에서 10 킬로미터 만큼에서 분리 될 수 있습니다. Virtual Network는 Azure의 물리적 데이터 센터 네트워크를 기반으로 논리 오버레이 및 가상 네트워크는 데이터 센터 내에서 어떤 특정 네트워크 토폴로지에 의미 하지는 않습니다. 예를 들어 VM A와 B VM은 동일한 가상 네트워크 및 서브넷에 있지만 다른 랙, 행 또는 심지어 데이터 센터에에서 있을 수 있습니다. 이러한 미터 광섬유 케이블 또는 광섬유 케이블 킬로미터 분리 될 수 있습니다. 이 실제로 다른 Vm 간의 가변 대기 시간 (몇 시간 (밀리초) 차이) 발생할 수 있습니다.

이 지리적 배치 이므로 두 개의 Vm 간의 대기 시간이 구성 가용성 집합과 가용성 영역에 영향을 수 있습니다, 그리고 있지만 지역의 데이터 센터 간의 거리는 지역별 주로 영향을 받습니다. 지역의 데이터 센터 토폴로지입니다.

### <a name="source-nat-port-exhaustion"></a>NAT 포트 소모 원본

Azure에 배포 되는 공용 인터넷 및/또는 공용 IP 공간에는 Azure 외부에서 끝점과 통신할 수 있습니다. 인스턴스가이 아웃 바운드 연결에서 시작 하는 경우 Azure 공용 IP 주소를 개인 IP 주소를 동적으로 매핑합니다. 이 매핑이 생성되면 이 아웃바운드에서 시작된 흐름에 대한 반환 트래픽도 흐름이 시작된 개인 IP 주소에 연결할 수 있습니다.

모든 아웃 바운드 연결에 대 한 Azure Load Balancer에는 일부 기간에 대 한이 매핑을 유지 해야 합니다. Azure의 다중 테 넌 트 본질적으로 모든 VM에 대 한 모든 아웃 바운드 흐름에 대 한이 매핑을 유지 관리 리소스를 많이 사용 될 수 있습니다. 따라서 설정 및 Azure Virtual Network의 구성에 따라 제한이 있습니다. 또는 보다 정확 하 게-명시 된 Azure VM에만 가능 특정 수의 아웃 바운드 연결을 지정된 된 시간에 있습니다. 이러한 제한은 고갈 되 면 Azure VM 추가 아웃 바운드 연결 하지 못하도록 차단 됩니다.

그러나이 동작은, 구성할 수 있습니다. 에 대 한 자세한 내용은 [SNAT 및 SNAT 포트 소모]를 참조 하세요 [이 문서에서는](https://docs.microsoft.com/azure/load-balancer/load-balancer-outbound-connections)합니다.

## <a name="measure-network-performance-on-azure"></a>Azure에서 네트워크 성능 측정

네트워크 대기 시간에 관련 된 다양 한이 문서의 성능 최대값 왕복 / 두 Vm 간의 시간 (RTT). 이 섹션에서는 대기 시간/RTT 뿐만 아니라 TCP 성능 및 VM 간 네트워크 성능을 테스트 하는 방법에 대 한 몇 가지 제안을 제공 합니다. 위에서 설명한 TCP/IP 및 네트워크 값을 조정할 수 있습니다 하 고 아래에 설명 된 기술을 사용 하 여 성능을 테스트 합니다. 대기 시간, MTU, MSS 및 창 크기의 값을 위에 나열 된 계산에 사용할 수 및 이론적인 최대값 테스트 중에 관찰 되는 실제 값을 비교할 수 있습니다.

### <a name="measure-round-trip-time-and-packet-loss"></a>측정값 왕복 시간과 패킷 손실

TCP 성능 RTT 및 패킷 손실에 크게 의존 합니다. RTT 및 패킷 손실을 측정 하는 가장 간단한 방법은 Windows 및 Linux에서 사용할 수 있는 ping 유틸리티를 사용 합니다. Ping의 출력에는 원본 및 대상으로 패킷 손실이 간의 최소/최대/평균 대기 시간이 표시 됩니다. Ping은 기본적으로 ICMP 프로토콜을 사용합니다. TCP RTT를 테스트 하려면 다음 PsPing 사용할 수 있습니다. PsPing에 자세한 내용은 [이 링크](https://docs.microsoft.com/sysinternals/downloads/psping)합니다.

### <a name="measure-actual-throughput-of-a-tcp-connection"></a>TCP 연결의 실제 처리량 측정

NTttcp를 Linux 또는 Windows VM의 TCP 성능을 테스트 하는 데 사용 되는 도구 이며 다양 한 TCP 설정 조정이 가능 하 고 이점을 NTttcp를 사용 하 여 테스트 합니다. NTttcp에 대 한 자세한 내용은 다음 링크에서 찾을 수 있습니다.

- [대역폭/처리량 테스트 (NTttcp)](https://aka.ms/TestNetworkThroughput)

- [NTttcp 유틸리티](https://gallery.technet.microsoft.com/NTttcp-Version-528-Now-f8b12769)

### <a name="measure-actual-bandwidth-of-a-virtual-machine"></a>가상 컴퓨터의 실제 대역폭 측정값

다른 VM 유형, 가속화 된 네트워킹 및 등의 성능 테스트 라는 Iperf, Linux 및 Windows 에서도 다운로드 가능 도구를 사용 하 여 테스트할 수 있습니다. Iperf는 TCP 또는 UDP를 사용 하 여 전반적인 네트워크 처리량 테스트를 수 있습니다. Iperf를 사용 하 여 TCP 처리량 테스트는 (대기 시간, RTT 및 등)이이 문서에서 설명한 요인에 의해 영향을 받습니다. 따라서 UDP 단순히 최대 처리량을 테스트 하는 것에 대 한 더 나은 결과 생성할 수 있습니다.

아래 추가 정보를 찾을 수 있습니다.

- [Expressroute 네트워크 성능 문제 해결](https://docs.microsoft.com/azure/expressroute/expressroute-troubleshooting-network-performance)

- [가상 네트워크에 대한 VPN 처리량의 유효성을 검사하는 방법](https://docs.microsoft.com/azure/vpn-gateway/vpn-gateway-validate-throughput-to-vnet)

### <a name="detect-inefficient-tcp-behaviors"></a>비효율적인 TCP 동작 감지

Azure 고객은 네트워크 성능 문제를 나타낼 수 있는 패킷 캡처에 TCP 플래그 (SACK, 중복 된 ACK, 재전송 및 고속 재전송)를 사용 하 여 TCP 패킷 나타날 수 있습니다. 특히 이러한 패킷은 네트워크 패킷 손실의 결과로 비효율성을 나타냅니다. 그러나 패킷 손실이 아닌 경우 반드시 Azure 성능 문제로 인해 성능 문제 때문일 수의 응용 프로그램, 운영 체제 또는 기타 문제를 Azure 플랫폼에 직접 연결 되지 않을 수 있습니다. 일부 재전송 또는 네트워크에 중복 된 Ack는 보통 – TCP 프로토콜을 안정적으로 빌드된 명심 해야 이기도 합니다. 및 패킷 캡처에 이러한 TCP 패킷의 증거 나타내지는지 않습니다 시스템 네트워크 문제가 않은 과도 하 게 합니다.

그러나이 해야 명백히 이러한 패킷의 형식 TCP 처리량은 다른 섹션에 설명 된 이유로 – 최대 성능을 달성 하지 못하는 징후는 합니다.
