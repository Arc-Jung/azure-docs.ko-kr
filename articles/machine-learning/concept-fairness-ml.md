---
title: 기계 학습 모델의 공정성 평가 및 완화
titleSuffix: Azure Machine Learning
description: 기계 학습 모델의 공정성에 대한 내용과 Fairlearn Python 패키지로 보다 공정한 모델을 빌드하는 방법을 알아봅니다.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: conceptual
ms.author: luquinta
author: luisquintanilla
ms.date: 05/02/2020
ms.openlocfilehash: c21ec0329a7b5716a00262b7422296df3afe208b
ms.sourcegitcommit: bb0afd0df5563cc53f76a642fd8fc709e366568b
ms.translationtype: HT
ms.contentlocale: ko-KR
ms.lasthandoff: 05/19/2020
ms.locfileid: "83596362"
---
# <a name="fairness-in-machine-learning-models"></a>기계 학습 모델의 공정성

기계 학습 모델의 공정성에 대한 내용과 Fairlearn 오픈 소스 Python 패키지로 보다 공정한 모델을 빌드하는 방법을 알아봅니다.

## <a name="what-is-fairness-in-machine-learning-systems"></a>기계 학습 시스템의 공정성이란?

AI와 기계 학습 시스템이 불공정한 동작을 보일 수 있습니다. 불공정한 동작을 정의하는 한 가지 방법은 사람들에게 미치는 피해 또는 영향입니다. AI 시스템은 여러 가지 유형의 피해를 입힐 수 있습니다. 다음 두 가지는 AI로 인한 대표적인 피해 유형입니다.

- 할당 피해: AI 시스템이 기회, 리소스 또는 정보를 확장하거나 알려주지 않습니다. 예를 들어 채용, 입학 및 대출 심사에서 모델이 다른 그룹보다 특정 그룹에서 유력 후보를 선택하는 경향을 보일 수 있습니다.

- 서비스 품질 피해: AI 시스템이 특정 그룹에 대해서는 다른 그룹에 하는 것처럼 작동하지 않습니다. 예를 들어 음성 인식 시스템이 여성에게는 남성에게 하듯이 작동하지 않을 수 있습니다.

AI 시스템의 불공정한 동작을 줄이려면 이러한 피해를 평가하고 완화해야 합니다.

>[!NOTE]
> 공정성은 사회 기술적 과제입니다. 공평, 적법 절차 등 공정성의 여러 측면은 정량적 공정성 메트릭으로 캡처할 수 없습니다. 또한 여러 정성적 공정성 메트릭을 모두 동시에 만족할 수는 없습니다. 목표는 여러 가지 완화 전략을 평가한 다음, 각자 시나리오에 맞게 적당히 타협할 수 있도록 도와주는 것입니다.

## <a name="fairness-assessment-and-mitigation-with-fairlearn"></a>Fairlearn을 사용한 공정성 평가 및 완화

Fairlearn은 기계 학습 시스템 개발자가 시스템의 공정성을 평가하고 관찰된 공정성 문제를 완화할 수 있는 오픈 소스 Python 패키지입니다.

Fairlearn은 다음 두 가지 구성 요소로 이루어집니다.

- 평가 대시보드: 모델의 예측이 다른 그룹에 어떤 영향을 주는지 평가하기 위한 Jupyter Notebook 위젯입니다. 또한 공정성 및 성능 메트릭을 사용하여 여러 모델을 비교할 수 있습니다.
- 완화 알고리즘: 이진 분류 및 회귀의 불공정을 완화하는 알고리즘 세트입니다.

두 구성 요소가 함께 작동하여 데이터 과학자 및 비즈니스 리더가 공정성과 성능 사이에서 적절히 타협하고, 요구 사항에 가장 적합한 완화 전략을 선택할 수 있도록 도와줍니다.

## <a name="fairness-assessment"></a>공정성 평가

Fairlearn에서 공정성은 **그룹 공정성**이라고 하는 방법을 사용하여 개념화되며, 이 방법에서는 피해를 입을 위험에 노출되는 개인 그룹은 누구인가요?라는 질문을 던집니다.

아집단이라고도 하는 관련 그룹은 **중요한 기능** 또는 중요한 특성을 통해 정의됩니다. 중요한 기능은 Fairlearn 예측 도구에 벡터 또는 `sensitive_features`라는 매트릭스로 전달됩니다. 이 용어는 시스템 디자이너가 그룹 공정성을 평가할 때 이러한 기능을 중요하게 생각해야 한다는 의미입니다. 이러한 기능과 관련하여 개인 식별 정보로 인한 프라이버시 침해 여부에 주의해야 합니다. 하지만 "중요한"이라는 단어는 이러한 기능을 예측에 사용하면 안 된다는 의미가 아닙니다.

평가 단계에서 공정성은 차이 메트릭을 통해 정량화됩니다. **차이 메트릭**은 여러 그룹에서 모델의 동작을 평가하고 비율 또는 차이로 비교할 수 있습니다. Fairlearn은 다음과 같은 두 가지 차이 메트릭 클래스를 지원합니다.


- 모델 성능의 차이: 이 메트릭 세트는 여러 하위 그룹에서 선택한 성능 메트릭 값의 차이를 계산합니다. 일부 사례:

  - 정확도 차이
  - 오류율 차이
  - 정확도 차이
  - 재현율 차이
  - MAE 차이
  - 기타

- 선택 비율의 차이: 이 메트릭은 여러 하위 그룹 간의 선택 비율 차이를 포함합니다. 대출 승인율의 차이를 예로 들 수 있습니다. 선택 비율은 각 클래스에서 1로 분류된 데이터 요소의 비율(이진 분류) 또는 예측 값의 분포(회귀)를 의미합니다.

## <a name="unfairness-mitigation"></a>불공정 완화

### <a name="parity-constraints"></a>패리티 제약 조건

Fairlearn에는 다양한 불공정 완화 알고리즘이 포함되어 있습니다. 이러한 알고리즘은 예측 요인의 동작에 대한 **패리티 제약 조건** 또는 조건이라는 제약 조건 세트를 지원합니다. 패리티 제약 조건을 사용하려면 중요한 기능이 정의하는 그룹(예: 여러 인종)에서 예측 요인 동작의 일부 측면을 비교할 수 있어야 합니다. Fairlearn의 완화 알고리즘은 이러한 패리티 제약 조건을 사용하여 관찰된 공정성 문제를 완화합니다.

Fairlearn은 다음과 같은 유형의 패리티 제약 조건을 지원합니다.

|패리티 제약 조건  | 목적  |기계 학습 작업  |
|---------|---------|---------|
|인구 통계 패리티     |  할당 피해 완화 | 이진 분류, 회귀 |
|균등 배당률  | 할당 및 서비스 품질 피해 진단 | 이진 분류        |
|제한된 그룹 손실     |  서비스 품질 피해 완화 | 회귀 |

### <a name="mitigation-algorithms"></a>완화 알고리즘

Fairlearn은 다음과 같은 후처리 및 축소 불공정 완화 알고리즘을 제공합니다.

- 축소: 이 알고리즘은 표준 블랙박스 ML 예측 도구(예: LightGBM 모델)를 통해 일련의 재가중치 학습 데이터 세트를 사용하여 재학습된 모델을 생성합니다. 예를 들어 특정 성별의 신청자에게 가중치를 더하거나 빼서 모델을 다시 학습시켜 성별 그룹 간의 차이를 줄일 수 있습니다. 그러면 사용자는 정확도(또는 다른 성능 메트릭)와 차이 간에 적절하게 절충하는 모델을 선택할 수 있으며, 일반적으로 절충은 비즈니스 규칙과 비용 계산을 기반으로 해야 합니다.  
- 후처리: 이 알고리즘은 기존 분류자와 중요한 기능을 입력으로 사용합니다. 그런 다음, 분류자의 예측을 변환하여 지정된 공정성 제약 조건을 적용합니다. 임계값 최적화의 가장 큰 장점은 모델을 다시 학습시킬 필요가 없기 때문에 간단하고 유연하다는 점입니다. 

| 알고리즘 | Description | 기계 학습 작업 | 중요한 기능 | 지원되는 패리티 제약 조건 | 알고리즘 유형 |
| --- | --- | --- | --- | --- | --- |
| `ExponentiatedGradient` | [공정 분류에 대한 축소 접근 방식](https://arxiv.org/abs/1803.02453)에 설명된 공정성 분류에 대한 블랙박스 접근 방식 | 이진 분류 | 범주 | [인구 통계 패리티](#parity-constraints), [균등 배당률](#parity-constraints) | 축소 |
| `GridSearch` | [공정 분류에 대한 축소 접근 방식](https://arxiv.org/abs/1803.02453)에 설명된 블랙박스 접근 방식| 이진 분류 | 이진 | [인구 통계 패리티](#parity-constraints), [균등 배당률](#parity-constraints) | 축소 |
| `GridSearch` | [공정 회귀: 정량적 정의 및 축소 기반 알고리즘](https://arxiv.org/abs/1905.12843)에 설명된 제한된 그룹 손실에 대한 알고리즘을 사용하여 공정 회귀의 그리드 검색 변형을 구현하는 블랙박스 접근 방식 | 회귀 | 이진 | [제한된 그룹 손실](#parity-constraints) | 축소 |
| `ThresholdOptimizer` | [감독 학습의 기회 균등](https://arxiv.org/abs/1610.02413) 논문을 기반으로 하는 후처리 알고리즘입니다. 이 기술은 기존 분류자와 중요한 기능을 입력으로 사용하고, 분류자의 예측을 모노톤으로 변환하여 지정된 패리티 제약 조건을 적용합니다. | 이진 분류 | 범주 | [인구 통계 패리티](#parity-constraints), [균등 배당률](#parity-constraints) | 후처리 |

## <a name="next-steps"></a>다음 단계

- 다른 구성 요소를 사용하는 방법을 알아보려면 [Fairlearn GitHub 리포지토리](https://github.com/fairlearn/fairlearn/) 및 [샘플 Notebook](https://github.com/fairlearn/fairlearn/tree/master/notebooks)를 확인하세요.
- [차등 프라이버시 및 WhisperNoise 패키지](concept-differential-privacy.md)를 사용하여 데이터 프라이버시를 유지하는 방법을 알아보세요.