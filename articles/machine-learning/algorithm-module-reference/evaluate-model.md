---
title: 모델을 평가 합니다. 모듈 참조
titleSuffix: Azure Machine Learning service
description: Azure Machine Learning 서비스에서 모델 평가 모듈을 사용 하 여 학습된 된 모델의 정확도 측정 하는 방법에 알아봅니다.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: reference
author: xiaoharper
ms.author: zhanxia
ms.date: 05/06/2019
ROBOTS: NOINDEX
ms.openlocfilehash: 40a8247c22da1f7a057e222565ffb2ec4c6b7fb3
ms.sourcegitcommit: 4b9c06dad94dfb3a103feb2ee0da5a6202c910cc
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 05/02/2019
ms.locfileid: "65028742"
---
# <a name="evaluate-model-module"></a>평가 모델 모듈

이 문서에서는 Azure Machine Learning 서비스에 대 한 시각적 인터페이스 (미리 보기)의 모듈을 설명 합니다.

이 모듈을 사용 하 여 학습된 된 모델의 정확도를 측정 합니다. 모델에서 생성 된 점수를 포함 하는 데이터 집합을 제공 하며 **모델 평가** 모듈에서는 업계 표준 평가 메트릭 집합을 계산 합니다.
  
 반환 되는 메트릭에 **모델 평가** 평가 하는 모델의 유형에 따라 달라 집니다.  
  
-   **분류 모델**    
-   **회귀 모델**    



> [!TIP]
> 모델 평가를 처음 이라면 비디오 시리즈를에서 Dr 좋습니다. 일부로 Stephen Elston 합니다 [machine learning 과정](https://blogs.technet.microsoft.com/machinelearning/2015/09/08/new-edx-course-data-science-machine-learning-essentials/) EdX에서. 


세 가지 방법으로 사용 하 여 **모델 평가** 모듈:

+ 학습 데이터에 대해 점수를 생성 하 고 이러한 점수에 따라 모델 평가
+ 모델에서 점수를 생성 하지만 예약 된 테스트 집합에 점수를 해당 점수를 비교 합니다.
+ 동일한 데이터 집합을 사용 하 여 두 개의 서로 다르지만 관련 된 모델에 대 한 점수를 비교 합니다.

## <a name="use-the-training-data"></a>학습 데이터를 사용 합니다.

모델을 평가 하려면 입력된 열과 점수 집합이 포함 된 데이터 집합을 연결 해야 합니다.  사용 가능한 다른 데이터가 없는 경우에 원래 데이터 집합을 사용할 수 있습니다.

1. 연결 합니다 **점수가 매겨진 데이터 집합** 의 출력을 [모델 점수 매기기](./score-model.md) 의 입력 **모델 평가**합니다. 
2. 클릭 **모델 평가** 모듈과 평가 점수를 생성 하는 실험을 실행 합니다.

## <a name="use-testing-data"></a>테스트 데이터 사용

Machine learning에서 일반적인 시나리오는 원래 데이터 집합 학습 및 테스트를 사용 하 여 데이터 집합으로 구분 하는 [분할](./split-data.md) 모듈 또는 [분할 및 샘플링](./partition-and-sample.md) 모듈입니다. 

1. 연결 합니다 **점수가 매겨진 데이터 집합** 의 출력을 [모델 점수 매기기](score-model.md) 의 입력 **모델 평가**합니다. 
2. 오른쪽 입력 테스트 데이터를 포함 하는 데이터 분할 모듈의 출력에 연결 **모델 평가**합니다.
2. 클릭 **모델 평가** 모듈과 선택 **선택 항목 실행** 평가 점수를 생성 합니다.

## <a name="compare-scores-from-two-models"></a>두 모델의 점수를 비교 합니다.

두 번째에 점수 집합을 연결할 수도 있습니다 **모델 평가**합니다.  점수 결과 알려진 공유 평가 집합 또는 동일한 데이터에 대 한 다른 모델에서 결과 집합을 수 있습니다.

이 기능은 동일한 데이터에서 서로 다른 두 모델의 결과 쉽게 비교할 수 있으므로 유용 합니다. 또는 다른 매개 변수를 사용 하 여 동일한 데이터에 대해 두 가지 다른 실행에서 점수를 비교할 수 있습니다.

1. 연결 합니다 **점수가 매겨진 데이터 집합** 의 출력을 [모델 점수 매기기](score-model.md) 의 입력 **모델 평가**합니다. 
2. 오른쪽 입력으로 두 번째 모델에 대 한 모델 점수 매기기 모듈의 출력에 연결 **모델 평가**합니다.
3. 마우스 오른쪽 단추로 클릭 **모델 평가**, 선택한 **선택 항목 실행** 평가 점수를 생성 합니다.

## <a name="results"></a>결과

실행 한 후 **모델 평가**모듈을 마우스 오른쪽 단추로 클릭 하 고 선택 **평가 결과** 결과를 확인 합니다. 다음을 수행할 수 있습니다.

+ 다른 도구를 사용 하 여 쉽게 분석에 대 한 데이터 집합으로 결과 저장
+ 인터페이스에서 시각화를 생성 합니다.

데이터 집합의 두 입력에 연결한 경우 **모델 평가**, 결과 두 데이터 집합 또는 두 모델에 대 한 메트릭을 포함 됩니다.
모델 또는 왼쪽된 포트에 연결 된 데이터는 보고서, 데이터 집합에 대 한 메트릭을 뒤 나 오른쪽 포트에 연결 된 모델에서 먼저 표시 됩니다.  

예를 들어, 다음 이미지는 동일한 데이터에 있지만 서로 다른 매개 변수를 사용 하 여 작성 된 두 가지 클러스터링 모델에서 결과 비교를 나타냅니다.  

![AML&#95;Comparing2Models](media/module/aml-comparing2models.png "AML_Comparing2Models")  

클러스터링 모델 이기 때문에 평가 결과 두 가지 회귀 모델의 점수를 비교 하거나 두 분류 모델을 비교 하면 보다 다릅니다. 그러나 전체 프레젠테이션에 동일합니다. 

## <a name="metrics"></a>메트릭

이 섹션에서는 설명 사용에 대 한 지원 되는 모델의 특정 형식에 대해 반환 되는 메트릭에 **모델 평가**:

+ [분류 모델](#bkmk_classification)
+ [회귀 모델](#bkmk_regression)

###  <a name="bkmk_classification"></a> 분류 모델에 대 한 메트릭

분류 모델을 평가할 때 다음 메트릭을 보고 됩니다. 모델을 비교 하는 평가 대해 선택한 메트릭을 기준으로 순위가 지정 됩니다.  
  
-   **정확도** 총 사례에 대 한 참 결과의 비율로 분류 모델의 적합성을 측정 합니다.  
  
-   **전체 자릿수** 모든 긍정적 결과 true 결과의 비율입니다.  
  
-   **회수** 부분 모델에서 반환 하는 모든 올바른 결과입니다.  
  
-   **F-점수** 정확도 재현 율 0과 1을 가장 이상적인 F-점수 값이 1 사이의 중된 평균으로 계산 됩니다.  
  
-   **AUC** 측정값을 사용 하 여 그린 곡선 아래의 영역 참 긍정에서 x 축에 y 축 및 거짓 긍정입니다. 이 메트릭은 서로 다른 유형의 모델을 비교할 수 있는 단일 숫자를 제공 하기 때문에 유용 합니다.  
  
- **평균 로그 손실** 단일 점수는 잘못 된 결과 대 한 페널티를 표현 하는 데 사용 됩니다. 두 확률 분포 – 실제 분포와 모델에서 간의 차이 따라 계산 됩니다.  
  
- **로그 손실 학습** 임의 예측과 비교한 분류자의 장점은 나타내는 단일 점수입니다. 로그 손실 레이블에 알려진된 값 (모드)를 출력 하는 확률을 비교 하 여 모델의 불확실성을 측정 합니다. 전체 모델에 대 한 로그 손실을 최소화 하려고 합니다.

##  <a name="bkmk_regression"></a> 회귀 모델에 대 한 메트릭
 
회귀 모델에 대해 반환 되는 메트릭은 오류의 크기를 추정 하려면 일반적으로 설계 되었습니다.  모델은 관찰 및 예측 값의 차이 작은 경우에 데이터에 맞게 간주 됩니다. 그러나 오차의 패턴을 살펴보면 (다른 예측된 지점 및 해당 실제 값 간의 차이) 알려 줄 수 있습니다 훨씬 모델의 잠재적인 편차에 대 한 합니다.  
  
 회귀 모델 평가 대 한 다음 메트릭을 보고 됩니다. 모델을 비교할 때 평가 대해 선택한 메트릭을 기준으로 순위가 지정 됩니다.  
  
- **절대 평균 오차 (MAE)** 따라서 낮은 점수는 더 나은; 얼마나 근접 예측이 실제 결과를 측정 합니다.  
  
- **제곱 평균 오차 (RMSE)** 모델에서 오류를 요약 하는 단일 값을 만듭니다. 차이 더하거나 제곱을 여는 메트릭을 과도 하 게 예측 및 예측 언더 간의 차이 무시 합니다.  
  
- **상대 절대 오차 (RAE)** 예상 및 실제 값 간의 상대 절대 차이 평균과 나눠 평균 차이 때문에 상대적입니다.  
  
- **상대 제곱 오차 (RSE)** 마찬가지로 예측된 값의 총 제곱된 오차를 실제 값의 총 제곱된 오차도 나누어 정규화 합니다.  
  
- **0 개 오류 (MZOE) 의미** 예측 올바른 인지 여부를 나타냅니다.  즉: `ZeroOneLoss(x,y) = 1` 때 `x!=y`이 고 그렇지 않으면 `0`합니다.
  
- **결정 계수**종종 라고 R<sup>2</sup>, 0과 1 사이의 값으로 모델의 예측 능력을 나타냅니다. 0은 모델을 임의 (아무 것도 설명 합니다). 1은 최적의 의미 합니다. 그러나 사용할 때 주의 해야 R 해석<sup>2</sup> 값, 값이 낮으면 완전히 정상 수와 값이 높으면 의심 스 러 울 수 있습니다.
  

## <a name="next-steps"></a>다음 단계

참조를 [사용할 수 있는 모듈 집합](module-reference.md) Azure Machine Learning 서비스입니다. 